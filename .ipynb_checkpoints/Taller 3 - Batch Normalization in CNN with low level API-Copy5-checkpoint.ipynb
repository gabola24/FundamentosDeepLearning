{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlutils\n",
    "reload(mlutils)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Convolutional network with TensorFlow low level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### RECOMMENDATION\n",
    "\n",
    "- close all applications\n",
    "- install Maxthon browser http://www.maxthon.com\n",
    "- open only VirtualBox and Maxthon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using a small dataset based on [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘mini_cifar.h5’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://s3.amazonaws.com/rlx/mini_cifar.h5\n",
    "import h5py\n",
    "with h5py.File('mini_cifar.h5','r') as h5f:\n",
    "    x_cifar = h5f[\"x\"][:]\n",
    "    y_cifar = h5f[\"y\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2253, 32, 32, 3) (2253,) (751, 32, 32, 3) (751,)\n",
      "\n",
      "distribution of train classes\n",
      "2    782\n",
      "0    745\n",
      "1    726\n",
      "dtype: int64\n",
      "\n",
      "distribution of test classes\n",
      "0    260\n",
      "1    248\n",
      "2    243\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_cifar, y_cifar, test_size=.25)\n",
    "print x_train.shape, y_train.shape, x_test.shape, y_test.shape\n",
    "print \"\\ndistribution of train classes\"\n",
    "print pd.Series(y_train).value_counts()\n",
    "print \"\\ndistribution of test classes\"\n",
    "print pd.Series(y_test).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1  2\n",
       "2248  0  0  1\n",
       "2249  0  0  1\n",
       "2250  1  0  0\n",
       "2251  0  1  0\n",
       "2252  1  0  0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y__=pd.Series(y_train)\n",
    "y_train_tidy=pd.get_dummies(y__)\n",
    "y_test_tidy=pd.Series(y_test)\n",
    "y_test_tidy=pd.get_dummies(y_test_tidy)\n",
    "y_train_tidy.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Build a CNN with TF Low Level API\n",
    "\n",
    "### Build the convolutional network model\n",
    "\n",
    "with the same architecture as in the corresponding notebook:\n",
    "\n",
    "    Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "    input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
    "    _________________________________________________________________\n",
    "    conv2d (Conv2D)              (None, 32, 32, 15)        735       \n",
    "    _________________________________________________________________\n",
    "    flatten (Flatten)            (None, 15360)             0         \n",
    "    _________________________________________________________________\n",
    "    dense (Dense)                (None, 16)                245776    \n",
    "    _________________________________________________________________\n",
    "    output_1 (Dense)             (None, 3)                 51        \n",
    "    =================================================================\n",
    "    Total params: 246,562\n",
    "    Trainable params: 246,562\n",
    "    Non-trainable params: 0\n",
    "    _________________________________________________________________\n",
    "\n",
    "#### understand carefully the example [here](http://www.jessicayung.com/explaining-tensorflow-code-for-a-convolutional-neural-network/). \n",
    "\n",
    "Complete the following function. You will have to:\n",
    "\n",
    "1. Declare tensor symbolic variables for inputs and model parameters:\n",
    "\n",
    "    - Define placefolders for X and y\n",
    "    - Define tf variables for W's and b's. You will have to think carefully about their shapes.\n",
    "\n",
    "\n",
    "2. Build the computational graph\n",
    "\n",
    "    - Use [tf.random_normal](https://www.tensorflow.org/api_docs/python/tf/random/normal) with mean 0 and std 1 as initialization distribuition for all W's and b's\n",
    "    - Use [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) for the convolutional layer (`h_conv1`)\n",
    "    - Use [tf.reshape](https://www.tensorflow.org/api_docs/python/tf/reshape) to transition from the convolutional layer to the dense layer (`h_conv1_flat`)\n",
    "    - Model the dense layer with TF matrix multiplication and relu activation (`h_dense`)\n",
    "    - Model the output with three output neurons and softmax activation (`y_proba`)\n",
    "\n",
    "the shapes of the weights your define must be equal to the ones printed out in the corresponding notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def multiply(n): \n",
    "    total = 1 \n",
    "    for i in range(1, len(n)):\n",
    "        total *= n[i]\n",
    "    return total \n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    #x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "output_units=len(np.unique(y_cifar))\n",
    "print(output_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape(None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ty1 = tf.placeholder(name=\"y\", dtype=tf.int32, shape = ( None))\n",
    "ty1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_filters, filter_size, dense_size, img_size, n_channels):\n",
    "\n",
    "    init_stddev = 0.01\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    tX = tf.placeholder(name=\"X\", dtype=tf.float64, shape = (None,img_size,img_size,n_channels))\n",
    "    ty = tf.placeholder(name=\"y\", dtype=tf.int32, shape = (None,len(np.unique(y_cifar))))\n",
    "    ty1 = tf.placeholder(name=\"y\", dtype=tf.int32, shape = ( None))\n",
    "    \n",
    "    w_conv1 = tf.Variable(initial_value=tf.random_normal([filter_size,filter_size,n_channels,n_filters],dtype=tf.float64,mean=0,stddev=1,),name='W1',dtype=tf.float64)\n",
    "    b_conv1 = tf.Variable(initial_value=tf.random_normal([n_filters],mean=0,stddev=1,dtype=tf.float64), name=\"b1\", dtype=tf.float64)\n",
    "\n",
    "    w_dense = tf.Variable(initial_value=tf.random_normal([n_filters*(img_size)*img_size,dense_size],mean=0,stddev=1,dtype=tf.float64),name='W2',dtype=tf.float64)\n",
    "\n",
    "    b_dense = tf.Variable(initial_value=tf.random_normal([dense_size],mean=0,stddev=1,dtype=tf.float64), name=\"b2\", dtype=tf.float64)\n",
    "\n",
    "\n",
    "    w_out   = tf.Variable(initial_value=tf.random_normal([dense_size,output_units],mean=0,stddev=1,dtype=tf.float64),name='W3',dtype=tf.float64)\n",
    "\n",
    "    b_out   = tf.Variable(initial_value=tf.random_normal([output_units],mean=0,stddev=1,dtype=tf.float64), name=\"b3\", dtype=tf.float64)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    with tf.name_scope(\"cnn\"):\n",
    "        h_conv1      = conv2d(tX,w_conv1,b_conv1) \n",
    "        h_conv1_flat = tf.reshape(h_conv1,(-1,multiply(h_conv1.get_shape().as_list())))\n",
    "        h_dense      = tf.add(tf.matmul(h_conv1_flat, w_dense), b_dense)\n",
    "        h_dense      = tf.nn.relu(h_dense)        \n",
    "        y_proba      = tf.add(tf.matmul(h_dense, w_out), b_out)\n",
    "        y_proba      = tf.nn.softmax(y_proba)\n",
    "        y_proba      = tf.cast(y_proba, tf.float32)\n",
    "        print(y_proba.shape)\n",
    "        print(ty.shape)\n",
    "\n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        y_hat        = tf.argmax(y_proba, axis=1)\n",
    "        xentropy     = tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_proba, labels=ty)\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        loss         = tf.reduce_mean(xentropy)\n",
    "        optimizer    = tf.train.AdamOptimizer()\n",
    "        training_op  = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        y_proba2=tf.reshape(y_proba,shape=(-1,1))\n",
    "        ty2=tf.reshape(ty,shape=(-1,1))\n",
    "        correct = tf.nn.in_top_k(y_proba,ty[0],1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    with tf.name_scope(\"init_and_save\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    print \"-- weights shape --\"\n",
    "    print w_conv1.shape\n",
    "    print b_conv1.shape\n",
    "    print w_dense.shape\n",
    "    print b_dense.shape\n",
    "    print w_out.shape\n",
    "    print b_out.shape \n",
    "\n",
    "    return tX, ty, init, accuracy, training_op, loss,ty1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 3 classes\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(y_cifar))\n",
    "print \"using\", n_classes, \"classes\"\n",
    "\n",
    "n_filters   = 15\n",
    "filter_size = 4\n",
    "dense_size  = 16\n",
    "n_channels  = 3 \n",
    "img_size    = 32\n",
    "\n",
    "n_epochs = 30\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 3)\n",
      "(?, 3)\n",
      "-- weights shape --\n",
      "(4, 4, 3, 15)\n",
      "(15,)\n",
      "(15360, 16)\n",
      "(16,)\n",
      "(16, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "tX, ty, init, accuracy, training_op, loss ,ty1= build_model(n_filters, filter_size, dense_size, img_size, n_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the optimization loop\n",
    "\n",
    "keep track of accuracy and loss in both train and test. Base your implementation on the notebook describing TF low level API.\n",
    "\n",
    "Observe that accuracy must keep one metric per epoch averaging the accuracy obtained in all batches. Likewise for loss.\n",
    "\n",
    "Plot the accuracy and loss curves for test and train separately, which should look like the following\n",
    "\n",
    "![](Images/lab_batch_01.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 1, 0, 2, 0, 0, 1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      "N/A% (0 of 29) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_hist = []\n",
    "n_epochs   = 30\n",
    "num_examples = len(x_train)\n",
    "acc_train, acc_test = [], []\n",
    "loss_train, loss_test = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    \n",
    "    for i in mlutils.pbar()(xrange(1,n_epochs)):\n",
    "        ixds = np.random.permutation(len(x_train))\n",
    "        ixds2 = np.random.permutation(len(x_test))\n",
    "        _eacc, _eloss= sess.run([training_op, loss], feed_dict={tX: x_train[ixds], ty: y_train_tidy.loc[ixds]})\n",
    "        _tacc, _tloss = sess.run([training_op, loss], feed_dict={tX: x_test[ixds2], ty: y_test_tidy.loc[ixds2]})\n",
    "        #_,tt=sess.run(accuracy,feed_dict={tX: x_train[ixds], ty: y_train_tidy.iloc[ixds], ty1: y_train[ixds]})\n",
    "\n",
    "        loss_train.append(_eloss)\n",
    "        loss_test.append(_tloss)\n",
    "        acc_train.append(_eacc)\n",
    "        acc_test.append(_tacc)\n",
    "        \n",
    "    #val_W1, val_W2, val_b1, val_b2 = sess.run([tW1, tW2, tb1, tb2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "plt.subplot(211)\n",
    "plt.plot(loss_train)\n",
    "plt.plot(loss_test)\n",
    "plt.subplot(212)\n",
    "plt.plot(acc_train)\n",
    "plt.plot(acc_test)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "Modify the optimization loop so that each batch is normalized before feeding it to the optimization step according to the following spec:\n",
    "\n",
    "- consider only $X^{i}$ in the current batch\n",
    "- $X^{i}$: image $i$\n",
    "- $X^{i}_{j|k}$: channel $k$ of pixel $j$ in image $i$\n",
    "- $S^i$: image $i$ standardized\n",
    "\n",
    "In pixel wise standardization, each pixel has zero mean and std=1 across the dataset:\n",
    "\n",
    "- $\\mu = \\frac{1}{N}\\sum_{i,j,k} X^{i}_{j|k}$\n",
    "- $\\sigma = \\frac{1}{N}\\sum_{i,j,k}^{N-1}(X^{i}_{j|k}-\\mu_{j|k})^2$\n",
    "\n",
    "So that:\n",
    "\n",
    "$$S^{i}_{j|k} = \\frac{1}{\\sigma + 10^{-6}}(X^{i}_{j|k} - \\mu)$$\n",
    "\n",
    "\n",
    "The $10^{-6}$ is to avoid the case of zero variance\n",
    "\n",
    "you must also plot:\n",
    "\n",
    "- accuracy and loss curves for train and test separately, which should look better than the previous\n",
    "\n",
    "![](Images/lab_batch_02.png)\n",
    "\n",
    "- for only train, the accuracy and loss curves of both experiments, looking like this\n",
    "\n",
    "![](Images/lab_batch_03.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX, ty, init, accuracy, training_op, loss = build_model(n_filters, filter_size, dense_size, img_size, n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx_test = np.r_[[(i-np.mean(i))/np.std(i) for i in x_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(x_train)\n",
    "bacc_train, bacc_test = [], []\n",
    "bloss_train, bloss_test = [], []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        ...\n",
    "        for iteration in range(num_examples // batch_size):\n",
    "            ...\n",
    "            \n",
    "        ...\n",
    "        print \"epoch: %3d\"%(epoch+1), \"  train accuracy: %.4f\"%bacc_train[-1], \"  test accuracy: %.4f\"%bacc_test[-1], \"  train loss: %.4f\"%bloss_train[-1], \"  test loss: %.4f\"%bloss_test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ... plot accuracy and loss for train in both experiments ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
