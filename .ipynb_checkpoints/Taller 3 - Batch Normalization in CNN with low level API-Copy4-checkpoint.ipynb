{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlutils\n",
    "reload(mlutils)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Convolutional network with TensorFlow low level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### RECOMMENDATION\n",
    "\n",
    "- close all applications\n",
    "- install Maxthon browser http://www.maxthon.com\n",
    "- open only VirtualBox and Maxthon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using a small dataset based on [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘mini_cifar.h5’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://s3.amazonaws.com/rlx/mini_cifar.h5\n",
    "import h5py\n",
    "with h5py.File('mini_cifar.h5','r') as h5f:\n",
    "    x_cifar = h5f[\"x\"][:]\n",
    "    y_cifar = h5f[\"y\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2253, 32, 32, 3) (2253,) (751, 32, 32, 3) (751,)\n",
      "\n",
      "distribution of train classes\n",
      "2    766\n",
      "0    759\n",
      "1    728\n",
      "dtype: int64\n",
      "\n",
      "distribution of test classes\n",
      "2    259\n",
      "1    246\n",
      "0    246\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_cifar, y_cifar, test_size=.25)\n",
    "print x_train.shape, y_train.shape, x_test.shape, y_test.shape\n",
    "print \"\\ndistribution of train classes\"\n",
    "print pd.Series(y_train).value_counts()\n",
    "print \"\\ndistribution of test classes\"\n",
    "print pd.Series(y_test).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y__=pd.Series(y_train)\n",
    "y_train_tidy=pd.get_dummies(y__)\n",
    "y_test_tidy=pd.Series(y_test)\n",
    "y_test_tidy=pd.get_dummies(y_test_tidy)\n",
    "#y_train_tidy.tail(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Build a CNN with TF Low Level API\n",
    "\n",
    "### Build the convolutional network model\n",
    "\n",
    "with the same architecture as in the corresponding notebook:\n",
    "\n",
    "    Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "    input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
    "    _________________________________________________________________\n",
    "    conv2d (Conv2D)              (None, 32, 32, 15)        735       \n",
    "    _________________________________________________________________\n",
    "    flatten (Flatten)            (None, 15360)             0         \n",
    "    _________________________________________________________________\n",
    "    dense (Dense)                (None, 16)                245776    \n",
    "    _________________________________________________________________\n",
    "    output_1 (Dense)             (None, 3)                 51        \n",
    "    =================================================================\n",
    "    Total params: 246,562\n",
    "    Trainable params: 246,562\n",
    "    Non-trainable params: 0\n",
    "    _________________________________________________________________\n",
    "\n",
    "#### understand carefully the example [here](http://www.jessicayung.com/explaining-tensorflow-code-for-a-convolutional-neural-network/). \n",
    "\n",
    "Complete the following function. You will have to:\n",
    "\n",
    "1. Declare tensor symbolic variables for inputs and model parameters:\n",
    "\n",
    "    - Define placefolders for X and y\n",
    "    - Define tf variables for W's and b's. You will have to think carefully about their shapes.\n",
    "\n",
    "\n",
    "2. Build the computational graph\n",
    "\n",
    "    - Use [tf.random_normal](https://www.tensorflow.org/api_docs/python/tf/random/normal) with mean 0 and std 1 as initialization distribuition for all W's and b's\n",
    "    - Use [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) for the convolutional layer (`h_conv1`)\n",
    "    - Use [tf.reshape](https://www.tensorflow.org/api_docs/python/tf/reshape) to transition from the convolutional layer to the dense layer (`h_conv1_flat`)\n",
    "    - Model the dense layer with TF matrix multiplication and relu activation (`h_dense`)\n",
    "    - Model the output with three output neurons and softmax activation (`y_proba`)\n",
    "\n",
    "the shapes of the weights your define must be equal to the ones printed out in the corresponding notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def multiply(n): \n",
    "    total = 1 \n",
    "    for i in range(1, len(n)):\n",
    "        total *= n[i]\n",
    "    return total \n",
    "\n",
    "output_units=3\n",
    "print(output_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape(None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ty1 = tf.placeholder(name=\"y\", dtype=tf.int32, shape = ( None))\n",
    "ty1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_filters, filter_size, dense_size, img_size, n_channels):\n",
    "\n",
    "    init_stddev = 0.01\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    tX = tf.placeholder(name=\"X\", dtype=tf.float64, shape = (None,img_size,img_size,n_channels))\n",
    "    ty = tf.placeholder(name=\"y\", dtype=tf.int32, shape = (None,1))\n",
    "    ty1 = tf.placeholder(name=\"y\", dtype=tf.int32, shape = (None))\n",
    "    \n",
    "    w_conv1 = tf.Variable(initial_value=tf.random_normal([filter_size,filter_size,n_channels,n_filters],dtype=tf.float64,mean=0,stddev=1,),name='W1',dtype=tf.float64)\n",
    "    b_conv1 = tf.Variable(initial_value=tf.random_normal([n_filters],mean=0,stddev=1,dtype=tf.float64), name=\"b1\", dtype=tf.float64)\n",
    "\n",
    "    w_dense = tf.Variable(initial_value=tf.random_normal([n_filters*(img_size)*img_size,dense_size],mean=0,stddev=1,dtype=tf.float64),name='W2',dtype=tf.float64)\n",
    "\n",
    "    b_dense = tf.Variable(initial_value=tf.random_normal([dense_size],mean=0,stddev=1,dtype=tf.float64), name=\"b2\", dtype=tf.float64)\n",
    "\n",
    "\n",
    "    w_out   = tf.Variable(initial_value=tf.random_normal([dense_size,output_units],mean=0,stddev=1,dtype=tf.float64),name='W3',dtype=tf.float64)\n",
    "\n",
    "    b_out   = tf.Variable(initial_value=tf.random_normal([output_units],mean=0,stddev=1,dtype=tf.float64), name=\"b3\", dtype=tf.float64)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    with tf.name_scope(\"cnn\"):\n",
    "        h_conv1      = tf.nn.conv2d(tX,w_conv1, strides=[1,1,1,1], padding='SAME')\n",
    "        h_conv1      = tf.add(h_conv1, b_conv1)\n",
    "        h_conv1      = tf.nn.relu(h_conv1)\n",
    "        h_conv1_flat = tf.reshape(h_conv1,(-1,multiply(h_conv1.get_shape().as_list())))\n",
    "        h_dense      = tf.add(tf.matmul(h_conv1_flat, w_dense), b_dense)\n",
    "        h_dense      = tf.nn.relu(h_dense)        \n",
    "        y_proba      = tf.add(tf.matmul(h_dense, w_out), b_out)\n",
    "        y_proba      = tf.nn.softmax(y_proba)\n",
    "        y_proba      = tf.cast(y_proba, tf.float32)\n",
    "        print(y_proba.shape)\n",
    "        print(ty.shape)\n",
    "\n",
    "    with tf.name_scope(\"cross_entropy\"):\n",
    "        y_hat        = tf.argmax(y_proba, axis=1)\n",
    "        xentropy     = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_proba, labels=ty[1])\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        loss         = tf.reduce_mean(xentropy)\n",
    "        optimizer    = tf.train.AdamOptimizer()\n",
    "        training_op  = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        y_proba2=tf.reshape(y_proba,shape=(-1,1))\n",
    "        ty2=tf.reshape(ty,shape=(-1,1))\n",
    "        correct = tf.nn.in_top_k(y_proba,ty[0],1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    with tf.name_scope(\"init_and_save\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    print \"-- weights shape --\"\n",
    "    print w_conv1.shape\n",
    "    print b_conv1.shape\n",
    "    print w_dense.shape\n",
    "    print b_dense.shape\n",
    "    print w_out.shape\n",
    "    print b_out.shape \n",
    "\n",
    "    return tX, ty, init, accuracy, training_op, loss,ty1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 3 classes\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(y_cifar))\n",
    "print \"using\", n_classes, \"classes\"\n",
    "\n",
    "n_filters   = 15\n",
    "filter_size = 4\n",
    "dense_size  = 16\n",
    "n_channels  = 3 \n",
    "img_size    = 32\n",
    "\n",
    "n_epochs = 30\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 3)\n",
      "(?, 1)\n",
      "-- weights shape --\n",
      "(4, 4, 3, 15)\n",
      "(15,)\n",
      "(15360, 16)\n",
      "(16,)\n",
      "(16, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "tX, ty, init, accuracy, training_op, loss ,ty1= build_model(n_filters, filter_size, dense_size, img_size, n_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the optimization loop\n",
    "\n",
    "keep track of accuracy and loss in both train and test. Base your implementation on the notebook describing TF low level API.\n",
    "\n",
    "Observe that accuracy must keep one metric per epoch averaging the accuracy obtained in all batches. Likewise for loss.\n",
    "\n",
    "Plot the accuracy and loss curves for test and train separately, which should look like the following\n",
    "\n",
    "![](Images/lab_batch_01.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 2, 2, 2, 2, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      "N/A% (0 of 29) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must have the same first dimension, got logits shape [2253,3] and labels shape [1]\n\t [[Node: cross_entropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](cnn/Cast, cross_entropy/strided_slice)]]\n\nCaused by op u'cross_entropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"/opt/miniconda/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/opt/miniconda/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/opt/miniconda/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/miniconda/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/opt/miniconda/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/opt/miniconda/lib/python2.7/site-packages/tornado/ioloop.py\", line 1065, in start\n    handler_func(fd_obj, events)\n  File \"/opt/miniconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/miniconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/opt/miniconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/opt/miniconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/opt/miniconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/miniconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/opt/miniconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/opt/miniconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/opt/miniconda/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/miniconda/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/miniconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/miniconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/opt/miniconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-6c1651bd8e4b>\", line 1, in <module>\n    tX, ty, init, accuracy, training_op, loss ,ty1= build_model(n_filters, filter_size, dense_size, img_size, n_channels)\n  File \"<ipython-input-8-35fd7460c1d3>\", line 42, in build_model\n    xentropy     = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_proba, labels=ty[1])\n  File \"/opt/miniconda/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 2062, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/opt/miniconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7515, in sparse_softmax_cross_entropy_with_logits\n    labels=labels, name=name)\n  File \"/opt/miniconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/opt/miniconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/opt/miniconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [2253,3] and labels shape [1]\n\t [[Node: cross_entropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](cnn/Cast, cross_entropy/strided_slice)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-09894d605178>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mixds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mixds2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0m_eacc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_eloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mixds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mixds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0m_tacc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_tloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mixds2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mixds2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#_,tt=sess.run(accuracy,feed_dict={tX: x_train[ixds], ty: y_train_tidy.iloc[ixds], ty1: y_train[ixds]})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must have the same first dimension, got logits shape [2253,3] and labels shape [1]\n\t [[Node: cross_entropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](cnn/Cast, cross_entropy/strided_slice)]]\n\nCaused by op u'cross_entropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"/opt/miniconda/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/opt/miniconda/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/opt/miniconda/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/miniconda/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/opt/miniconda/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/opt/miniconda/lib/python2.7/site-packages/tornado/ioloop.py\", line 1065, in start\n    handler_func(fd_obj, events)\n  File \"/opt/miniconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/miniconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/opt/miniconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/opt/miniconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/opt/miniconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/miniconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/opt/miniconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/opt/miniconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/opt/miniconda/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/miniconda/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/miniconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/miniconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/opt/miniconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-6c1651bd8e4b>\", line 1, in <module>\n    tX, ty, init, accuracy, training_op, loss ,ty1= build_model(n_filters, filter_size, dense_size, img_size, n_channels)\n  File \"<ipython-input-8-35fd7460c1d3>\", line 42, in build_model\n    xentropy     = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_proba, labels=ty[1])\n  File \"/opt/miniconda/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 2062, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/opt/miniconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7515, in sparse_softmax_cross_entropy_with_logits\n    labels=labels, name=name)\n  File \"/opt/miniconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/opt/miniconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/opt/miniconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [2253,3] and labels shape [1]\n\t [[Node: cross_entropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](cnn/Cast, cross_entropy/strided_slice)]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_hist = []\n",
    "n_epochs   = 30\n",
    "num_examples = len(x_train)\n",
    "acc_train, acc_test = [], []\n",
    "loss_train, loss_test = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    \n",
    "    for i in mlutils.pbar()(xrange(1,n_epochs)):\n",
    "        ixds = np.random.permutation(len(x_train))\n",
    "        ixds2 = np.random.permutation(len(x_test))\n",
    "        _eacc, _eloss= sess.run([training_op, loss], feed_dict={tX: x_train[ixds], ty: y_train[ixds].reshape(-1,1)})\n",
    "        _tacc, _tloss = sess.run([training_op, loss], feed_dict={tX: x_test[ixds2], ty: y_test[ixds2].reshape(-1,1)})\n",
    "        #_,tt=sess.run(accuracy,feed_dict={tX: x_train[ixds], ty: y_train_tidy.iloc[ixds], ty1: y_train[ixds]})\n",
    "\n",
    "        loss_train.append(_eloss)\n",
    "        loss_test.append(_tloss)\n",
    "        acc_train.append(_eacc)\n",
    "        acc_test.append(_tacc)\n",
    "        \n",
    "    #val_W1, val_W2, val_b1, val_b2 = sess.run([tW1, tW2, tb1, tb2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "plt.subplot(211)\n",
    "plt.plot(loss_train)\n",
    "plt.plot(loss_test)\n",
    "plt.subplot(212)\n",
    "plt.plot(acc_train)\n",
    "plt.plot(acc_test)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "Modify the optimization loop so that each batch is normalized before feeding it to the optimization step according to the following spec:\n",
    "\n",
    "- consider only $X^{i}$ in the current batch\n",
    "- $X^{i}$: image $i$\n",
    "- $X^{i}_{j|k}$: channel $k$ of pixel $j$ in image $i$\n",
    "- $S^i$: image $i$ standardized\n",
    "\n",
    "In pixel wise standardization, each pixel has zero mean and std=1 across the dataset:\n",
    "\n",
    "- $\\mu = \\frac{1}{N}\\sum_{i,j,k} X^{i}_{j|k}$\n",
    "- $\\sigma = \\frac{1}{N}\\sum_{i,j,k}^{N-1}(X^{i}_{j|k}-\\mu_{j|k})^2$\n",
    "\n",
    "So that:\n",
    "\n",
    "$$S^{i}_{j|k} = \\frac{1}{\\sigma + 10^{-6}}(X^{i}_{j|k} - \\mu)$$\n",
    "\n",
    "\n",
    "The $10^{-6}$ is to avoid the case of zero variance\n",
    "\n",
    "you must also plot:\n",
    "\n",
    "- accuracy and loss curves for train and test separately, which should look better than the previous\n",
    "\n",
    "![](Images/lab_batch_02.png)\n",
    "\n",
    "- for only train, the accuracy and loss curves of both experiments, looking like this\n",
    "\n",
    "![](Images/lab_batch_03.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX, ty, init, accuracy, training_op, loss = build_model(n_filters, filter_size, dense_size, img_size, n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx_test = np.r_[[(i-np.mean(i))/np.std(i) for i in x_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(x_train)\n",
    "bacc_train, bacc_test = [], []\n",
    "bloss_train, bloss_test = [], []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        ...\n",
    "        for iteration in range(num_examples // batch_size):\n",
    "            ...\n",
    "            \n",
    "        ...\n",
    "        print \"epoch: %3d\"%(epoch+1), \"  train accuracy: %.4f\"%bacc_train[-1], \"  test accuracy: %.4f\"%bacc_test[-1], \"  train loss: %.4f\"%bloss_train[-1], \"  test loss: %.4f\"%bloss_test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ... plot accuracy and loss for train in both experiments ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
