{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab vanishing gradient\n",
    "\n",
    "\n",
    "In this lab you will impement a complete exploration and results visualization for the following experiment configurations  with dense neural networks:\n",
    "\n",
    "- with the following numbers of layers [2,3,4,5,6,7,8,9,10]\n",
    "- with the following neurons per layer [3,5,10,30]\n",
    "- with activations ReLU, Sigmoid and LeakyReLU\n",
    "\n",
    "In total, you must train and test 36 network architectures for each activation function.\n",
    "\n",
    "The experimentation must be done with the MNIST Digits datasets (1500 items) using a random partition of 50/50 for train and test.\n",
    "\n",
    "For each configuration you must record the **accuracy in test** and the **time used to train** the model.\n",
    "\n",
    "Then, you will need to build the following visualizations\n",
    "\n",
    "- one heat map for each activation function illustrating the **accuracy in test** for each configuration\n",
    "- one heat map for each activation function illustrating the **time used to train** for each configuration\n",
    "- a scatter plot of all your experiments, showing:\n",
    "    - the time used to train in the x-axis\n",
    "    - the accuracy in test in the y-axis\n",
    "    - the number of layers as the size of the dots\n",
    "    - expetiments for each activation function in different colors   \n",
    "- a bar plot showing the average performance of each activation function for each number of layers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your visualizations must look similar to these ones:\n",
    "\n",
    "![alt text](./Images/lab_vanishing_01.png)\n",
    "![alt text](./Images/lab_vanishing_02.png)\n",
    "![alt text](./Images/lab_vanishing_03.png)\n",
    "![alt text](./Images/lab_vanishing_04.png)\n",
    "![alt text](./Images/lab_vanishing_05.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension de las imagenes y las clases (1500, 784) (1500,)\n"
     ]
    }
   ],
   "source": [
    "mnist = pd.read_csv(\"data/mnist1.5k.csv.gz\", compression=\"gzip\", header=None).values\n",
    "X=mnist[:,1:785]/255.\n",
    "y=mnist[:,0]\n",
    "print \"dimension de las imagenes y las clases\", X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 784) (1200, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "X_train = X_train\n",
    "X_test  = X_test\n",
    "y_train_oh = np.eye(10)[y_train]\n",
    "y_test_oh  = np.eye(10)[y_test]\n",
    "print X_train.shape, y_train_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, concatenate, Input\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_dim=784, output_dim=10, num_hidden_layers=6, hidden_size=10, activation=\"relu\"):\n",
    "\n",
    "    clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_size, activation=activation, input_dim=input_dim, name=\"Layer_%02d_Input\"%(0)))\n",
    "    \n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add(Dense(hidden_size, activation=activation, name=\"Layer_%02d_Hidden\"%(i+1)))\n",
    "   \n",
    "    model.add(Dense(output_dim, activation=\"softmax\", name=\"Layer_%02d_Output\"%(num_hidden_layers+1)))\n",
    "        \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.reset_states()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[2,3,4,5,6,7,8,9,10]\n",
    "activations=[\"relu\", \"sigmoid\", \"leakyRelu\"]\n",
    "#cuando sea leakyrelu toca poner la fn de activacion directamente =tf.nn.leaky_relu\n",
    "neuro=[3,5,10,30]\n",
    "\n",
    "r_testA1 = pd.DataFrame(np.zeros((len(neuro), len(layers)))*np.nan, index=[str(i) for i in neuro],\n",
    "                      columns=[str(i) for i in layers])\n",
    "                     \n",
    "r_testA1\n",
    "\n",
    "r_testA2 = pd.DataFrame(np.zeros((len(neuro), len(layers)))*np.nan, index=[str(i) for i in neuro],\n",
    "                      columns=[str(i) for i in layers])\n",
    "\n",
    "r_testA3 = pd.DataFrame(np.zeros((len(neuro), len(layers)))*np.nan, index=[str(i) for i in neuro],\n",
    "                      columns=[str(i) for i in layers])\n",
    "\n",
    "r_testT1 = pd.DataFrame(np.zeros((len(neuro), len(layers)))*np.nan, index=[str(i) for i in neuro],\n",
    "                      columns=[str(i) for i in layers])\n",
    "\n",
    "r_testT2 = pd.DataFrame(np.zeros((len(neuro), len(layers)))*np.nan, index=[str(i) for i in neuro],\n",
    "                      columns=[str(i) for i in layers])\n",
    "\n",
    "r_testT3 = pd.DataFrame(np.zeros((len(neuro), len(layers)))*np.nan, index=[str(i) for i in neuro],\n",
    "                      columns=[str(i) for i in layers])\n",
    "\n",
    "\n",
    "#model = get_model(num_hidden_layers=10, activation=\"sigmoid\")\n",
    "#!rm -rf log/sigmoid\n",
    "#tb_callback = keras.callbacks.TensorBoard(log_dir='./log/sigmoid', histogram_freq=1,  write_grads=True, write_graph=True, write_images=True)\n",
    "#model.fit(X_train, y_train_oh, epochs=30, batch_size=32, validation_data=(X_test, y_test_oh), callbacks=[tb_callback])\n",
    "\n",
    "def get_model(input_dim=784, output_dim=10, num_hidden_layers=6, hidden_size=10, activation=\"relu\"):\n",
    "\n",
    "    clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_size, activation=activation, input_dim=input_dim, name=\"Layer_%02d_Input\"%(0)))\n",
    "    \n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add(Dense(hidden_size, activation=activation, name=\"Layer_%02d_Hidden\"%(i+1)))\n",
    "   \n",
    "    model.add(Dense(output_dim, activation=\"softmax\", name=\"Layer_%02d_Output\"%(num_hidden_layers+1)))\n",
    "        \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.reset_states()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 858us/step - loss: 2.2715 - acc: 0.1083 - val_loss: 2.2394 - val_acc: 0.0700\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.1908 - acc: 0.1533 - val_loss: 2.1681 - val_acc: 0.1267\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 2.1153 - acc: 0.1758 - val_loss: 2.1093 - val_acc: 0.1700\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 2.0597 - acc: 0.2008 - val_loss: 2.0714 - val_acc: 0.1633\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 2.0252 - acc: 0.2092 - val_loss: 2.0434 - val_acc: 0.1633\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 2.0015 - acc: 0.2167 - val_loss: 2.0212 - val_acc: 0.1667\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.9813 - acc: 0.2150 - val_loss: 2.0074 - val_acc: 0.1733\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.9665 - acc: 0.2258 - val_loss: 1.9928 - val_acc: 0.1967\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 1.9495 - acc: 0.2283 - val_loss: 1.9818 - val_acc: 0.1900\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.9353 - acc: 0.2267 - val_loss: 1.9677 - val_acc: 0.1900\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.9207 - acc: 0.2325 - val_loss: 1.9549 - val_acc: 0.1967\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.9063 - acc: 0.2350 - val_loss: 1.9453 - val_acc: 0.2067\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 94us/step - loss: 1.8934 - acc: 0.2375 - val_loss: 1.9354 - val_acc: 0.2167\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.8789 - acc: 0.2433 - val_loss: 1.9238 - val_acc: 0.2333\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.8657 - acc: 0.2517 - val_loss: 1.9186 - val_acc: 0.2400\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.8534 - acc: 0.2567 - val_loss: 1.9070 - val_acc: 0.2300\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.8398 - acc: 0.2533 - val_loss: 1.9002 - val_acc: 0.2567\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.8269 - acc: 0.2650 - val_loss: 1.8917 - val_acc: 0.2600\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 1.8155 - acc: 0.2658 - val_loss: 1.8862 - val_acc: 0.2600\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 1.8043 - acc: 0.2642 - val_loss: 1.8728 - val_acc: 0.2700\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 1.7925 - acc: 0.2742 - val_loss: 1.8671 - val_acc: 0.2700\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.7811 - acc: 0.2767 - val_loss: 1.8628 - val_acc: 0.2667\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.7704 - acc: 0.2783 - val_loss: 1.8523 - val_acc: 0.2733\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.7584 - acc: 0.2842 - val_loss: 1.8442 - val_acc: 0.3033\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.7522 - acc: 0.2958 - val_loss: 1.8416 - val_acc: 0.2967\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.7364 - acc: 0.2942 - val_loss: 1.8293 - val_acc: 0.3067\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.7253 - acc: 0.2992 - val_loss: 1.8254 - val_acc: 0.3333\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.7151 - acc: 0.3025 - val_loss: 1.8168 - val_acc: 0.3067\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 1.7031 - acc: 0.3108 - val_loss: 1.8081 - val_acc: 0.3400\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.6933 - acc: 0.3150 - val_loss: 1.8029 - val_acc: 0.3400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcee1e837d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time \n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "    \n",
    "time_callback = TimeHistory()\n",
    "model = get_model(input_dim=784, output_dim=10, num_hidden_layers=3, hidden_size=3, activation=\"relu\")\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir='./log/relu', histogram_freq=1,  write_grads=True, write_graph=True, write_images=True)\n",
    "\n",
    "model.fit(X_train, y_train_oh, epochs=30, batch_size=32, validation_data=(X_test, y_test_oh), callbacks=[tb_callback, time_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = time_callback.times\n",
    "t=0\n",
    "for i in range(0,len(times)):\n",
    "    t = t+times[i]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.352856636047363"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 0s 401us/step - loss: 2.2624 - acc: 0.1233 - val_loss: 2.2277 - val_acc: 0.1567\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 2.1908 - acc: 0.1600 - val_loss: 2.1732 - val_acc: 0.1700\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 2.1443 - acc: 0.1633 - val_loss: 2.1297 - val_acc: 0.1833\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 2.1031 - acc: 0.1683 - val_loss: 2.0996 - val_acc: 0.1767\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 2.0719 - acc: 0.1675 - val_loss: 2.0718 - val_acc: 0.1833\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 2.0443 - acc: 0.1800 - val_loss: 2.0464 - val_acc: 0.1800\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 2.0205 - acc: 0.1833 - val_loss: 2.0316 - val_acc: 0.1867\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.0021 - acc: 0.1925 - val_loss: 2.0196 - val_acc: 0.1967\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 1.9865 - acc: 0.1933 - val_loss: 2.0150 - val_acc: 0.1833\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 1.9713 - acc: 0.2000 - val_loss: 2.0016 - val_acc: 0.2000\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 1.9587 - acc: 0.2050 - val_loss: 1.9909 - val_acc: 0.2067\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 1.9465 - acc: 0.2075 - val_loss: 2.0049 - val_acc: 0.1933\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.9399 - acc: 0.2200 - val_loss: 1.9759 - val_acc: 0.2100\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 1.9239 - acc: 0.2383 - val_loss: 1.9802 - val_acc: 0.2100\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.9140 - acc: 0.2383 - val_loss: 1.9804 - val_acc: 0.2167\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.9044 - acc: 0.2392 - val_loss: 1.9655 - val_acc: 0.2200\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 1.8962 - acc: 0.2400 - val_loss: 1.9687 - val_acc: 0.2367\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 1.8872 - acc: 0.2450 - val_loss: 1.9691 - val_acc: 0.2367\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 1.8786 - acc: 0.2467 - val_loss: 1.9583 - val_acc: 0.2400\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.8697 - acc: 0.2408 - val_loss: 1.9522 - val_acc: 0.2333\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.8621 - acc: 0.2517 - val_loss: 1.9650 - val_acc: 0.2367\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.8550 - acc: 0.2550 - val_loss: 1.9468 - val_acc: 0.2433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.8453 - acc: 0.2500 - val_loss: 1.9595 - val_acc: 0.2433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.8372 - acc: 0.2550 - val_loss: 1.9572 - val_acc: 0.2333\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.8302 - acc: 0.2550 - val_loss: 1.9397 - val_acc: 0.2433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.8222 - acc: 0.2575 - val_loss: 1.9599 - val_acc: 0.2467\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.8140 - acc: 0.2733 - val_loss: 1.9519 - val_acc: 0.2400\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 1.8093 - acc: 0.2783 - val_loss: 1.9483 - val_acc: 0.2467\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 1.7980 - acc: 0.2867 - val_loss: 1.9390 - val_acc: 0.2467\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.7906 - acc: 0.2900 - val_loss: 1.9307 - val_acc: 0.2500\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 0s 386us/step - loss: 2.2238 - acc: 0.1225 - val_loss: 2.1454 - val_acc: 0.1533\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.1000 - acc: 0.2142 - val_loss: 2.0588 - val_acc: 0.2667\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.0045 - acc: 0.2733 - val_loss: 1.9665 - val_acc: 0.2600\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 1.9001 - acc: 0.2942 - val_loss: 1.8634 - val_acc: 0.3067\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.7914 - acc: 0.3567 - val_loss: 1.7568 - val_acc: 0.3300\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 1.6881 - acc: 0.3800 - val_loss: 1.6620 - val_acc: 0.3900\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.5909 - acc: 0.4283 - val_loss: 1.5891 - val_acc: 0.4100\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.5106 - acc: 0.4575 - val_loss: 1.5342 - val_acc: 0.4167\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.4449 - acc: 0.4700 - val_loss: 1.4807 - val_acc: 0.4467\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.3931 - acc: 0.5217 - val_loss: 1.4376 - val_acc: 0.4767\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.3280 - acc: 0.5583 - val_loss: 1.3917 - val_acc: 0.5033\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.2688 - acc: 0.6092 - val_loss: 1.3469 - val_acc: 0.5400\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.2102 - acc: 0.6417 - val_loss: 1.2943 - val_acc: 0.5900\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.1538 - acc: 0.6592 - val_loss: 1.2608 - val_acc: 0.6167\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 1.1094 - acc: 0.6758 - val_loss: 1.2096 - val_acc: 0.6333\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.0518 - acc: 0.7017 - val_loss: 1.1806 - val_acc: 0.6500\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.0078 - acc: 0.7125 - val_loss: 1.1417 - val_acc: 0.6200\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.9603 - acc: 0.7175 - val_loss: 1.1416 - val_acc: 0.6633\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.9208 - acc: 0.7392 - val_loss: 1.0854 - val_acc: 0.6700\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 0.8909 - acc: 0.7425 - val_loss: 1.0725 - val_acc: 0.6767\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.8491 - acc: 0.7683 - val_loss: 1.0786 - val_acc: 0.6900\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 0.8190 - acc: 0.7758 - val_loss: 1.0569 - val_acc: 0.6933\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 0.7900 - acc: 0.7892 - val_loss: 1.0364 - val_acc: 0.6667\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 0.7641 - acc: 0.7875 - val_loss: 1.0287 - val_acc: 0.6933\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.7355 - acc: 0.7908 - val_loss: 1.0106 - val_acc: 0.7000\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 0.7043 - acc: 0.8125 - val_loss: 1.0147 - val_acc: 0.7000\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.6848 - acc: 0.8050 - val_loss: 1.0045 - val_acc: 0.6800\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 0.6640 - acc: 0.8200 - val_loss: 1.0068 - val_acc: 0.6933\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 0.6445 - acc: 0.8350 - val_loss: 1.0012 - val_acc: 0.6967\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 68us/step - loss: 0.6209 - acc: 0.8300 - val_loss: 0.9917 - val_acc: 0.6967\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 436us/step - loss: 2.2300 - acc: 0.1617 - val_loss: 2.1488 - val_acc: 0.1600\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.0186 - acc: 0.3058 - val_loss: 1.9455 - val_acc: 0.3133\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 1.7969 - acc: 0.3900 - val_loss: 1.7647 - val_acc: 0.3733\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 1.5902 - acc: 0.4608 - val_loss: 1.5626 - val_acc: 0.4733\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 1.3689 - acc: 0.5425 - val_loss: 1.3629 - val_acc: 0.5333\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 1.1584 - acc: 0.6242 - val_loss: 1.1668 - val_acc: 0.6267\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 0.9942 - acc: 0.7017 - val_loss: 1.0458 - val_acc: 0.6833\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.8705 - acc: 0.7492 - val_loss: 0.9856 - val_acc: 0.7167\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.7632 - acc: 0.7875 - val_loss: 0.8707 - val_acc: 0.7700\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.6643 - acc: 0.8133 - val_loss: 0.7936 - val_acc: 0.7667\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.5989 - acc: 0.8375 - val_loss: 0.7506 - val_acc: 0.7767\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.5447 - acc: 0.8567 - val_loss: 0.7365 - val_acc: 0.7833\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 0.4989 - acc: 0.8650 - val_loss: 0.7010 - val_acc: 0.7867\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 0.4637 - acc: 0.8767 - val_loss: 0.6985 - val_acc: 0.7833\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.4405 - acc: 0.8858 - val_loss: 0.6815 - val_acc: 0.7900\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.4033 - acc: 0.8942 - val_loss: 0.6653 - val_acc: 0.7833\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.3761 - acc: 0.9017 - val_loss: 0.6714 - val_acc: 0.7967\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.3474 - acc: 0.9033 - val_loss: 0.6799 - val_acc: 0.7867\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.3324 - acc: 0.9150 - val_loss: 0.6539 - val_acc: 0.7967\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.3038 - acc: 0.9200 - val_loss: 0.6679 - val_acc: 0.8033\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 0.2849 - acc: 0.9233 - val_loss: 0.6482 - val_acc: 0.7900\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 0.2652 - acc: 0.9325 - val_loss: 0.6615 - val_acc: 0.8000\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.2494 - acc: 0.9367 - val_loss: 0.6685 - val_acc: 0.7967\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.2333 - acc: 0.9392 - val_loss: 0.6553 - val_acc: 0.7933\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.2207 - acc: 0.9425 - val_loss: 0.6729 - val_acc: 0.7767\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 0.2031 - acc: 0.9483 - val_loss: 0.6797 - val_acc: 0.7867\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 0.1877 - acc: 0.9517 - val_loss: 0.6605 - val_acc: 0.8033\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.1740 - acc: 0.9533 - val_loss: 0.6628 - val_acc: 0.8100\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 120us/step - loss: 0.1667 - acc: 0.9575 - val_loss: 0.6664 - val_acc: 0.8100\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 0.1573 - acc: 0.9633 - val_loss: 0.6842 - val_acc: 0.8033\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 435us/step - loss: 2.1113 - acc: 0.2292 - val_loss: 1.8076 - val_acc: 0.4233\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 1.4795 - acc: 0.5342 - val_loss: 1.1765 - val_acc: 0.6700\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.8940 - acc: 0.7475 - val_loss: 0.7447 - val_acc: 0.7833\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.5787 - acc: 0.8325 - val_loss: 0.6070 - val_acc: 0.8200\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 0.4191 - acc: 0.8908 - val_loss: 0.5566 - val_acc: 0.8267\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.3297 - acc: 0.9050 - val_loss: 0.5000 - val_acc: 0.8433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 132us/step - loss: 0.2664 - acc: 0.9300 - val_loss: 0.5055 - val_acc: 0.8300\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.2255 - acc: 0.9450 - val_loss: 0.4808 - val_acc: 0.8533\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.1785 - acc: 0.9600 - val_loss: 0.4695 - val_acc: 0.8600\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.1502 - acc: 0.9658 - val_loss: 0.4601 - val_acc: 0.8700\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 0.1361 - acc: 0.9700 - val_loss: 0.4948 - val_acc: 0.8700\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.1098 - acc: 0.9825 - val_loss: 0.4864 - val_acc: 0.8567\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 122us/step - loss: 0.0951 - acc: 0.9858 - val_loss: 0.4747 - val_acc: 0.8633\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 0.0771 - acc: 0.9900 - val_loss: 0.5180 - val_acc: 0.8600\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 0.0682 - acc: 0.9925 - val_loss: 0.4834 - val_acc: 0.8567\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.0568 - acc: 0.9933 - val_loss: 0.5059 - val_acc: 0.8700\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.0470 - acc: 0.9967 - val_loss: 0.5267 - val_acc: 0.8667\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 0.0422 - acc: 0.9958 - val_loss: 0.4793 - val_acc: 0.8733\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 0.0347 - acc: 0.9967 - val_loss: 0.4944 - val_acc: 0.8667\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 0.0311 - acc: 0.9992 - val_loss: 0.5043 - val_acc: 0.8667\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 0.0273 - acc: 0.9983 - val_loss: 0.5215 - val_acc: 0.8633\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.0240 - acc: 0.9992 - val_loss: 0.5366 - val_acc: 0.8667\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.0198 - acc: 0.9992 - val_loss: 0.5492 - val_acc: 0.8700\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.0174 - acc: 0.9992 - val_loss: 0.5387 - val_acc: 0.8700\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.0152 - acc: 1.0000 - val_loss: 0.5590 - val_acc: 0.8700\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.5639 - val_acc: 0.8667\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 181us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.5706 - val_acc: 0.8633\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.6011 - val_acc: 0.8667\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 0.6057 - val_acc: 0.8667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 123us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.6004 - val_acc: 0.8667\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 0s 406us/step - loss: 2.3601 - acc: 0.1075 - val_loss: 2.3842 - val_acc: 0.0833\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 2.3387 - acc: 0.1075 - val_loss: 2.3621 - val_acc: 0.0833\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 2.3252 - acc: 0.1075 - val_loss: 2.3460 - val_acc: 0.0833\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 2.3153 - acc: 0.1075 - val_loss: 2.3354 - val_acc: 0.0833\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 2.3081 - acc: 0.1075 - val_loss: 2.3252 - val_acc: 0.0833\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 2.3021 - acc: 0.1075 - val_loss: 2.3174 - val_acc: 0.0833\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2976 - acc: 0.1075 - val_loss: 2.3111 - val_acc: 0.0833\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 2.2936 - acc: 0.1075 - val_loss: 2.3051 - val_acc: 0.0833\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 50us/step - loss: 2.2900 - acc: 0.1075 - val_loss: 2.3008 - val_acc: 0.0833\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.2864 - acc: 0.1075 - val_loss: 2.2966 - val_acc: 0.0833\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2829 - acc: 0.1075 - val_loss: 2.2919 - val_acc: 0.0833\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 49us/step - loss: 2.2792 - acc: 0.1025 - val_loss: 2.2882 - val_acc: 0.1467\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.2755 - acc: 0.1183 - val_loss: 2.2839 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2715 - acc: 0.1192 - val_loss: 2.2799 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.2672 - acc: 0.1200 - val_loss: 2.2756 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 2.2628 - acc: 0.1192 - val_loss: 2.2710 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 2.2581 - acc: 0.1192 - val_loss: 2.2664 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.2526 - acc: 0.1217 - val_loss: 2.2615 - val_acc: 0.1500\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 2.2469 - acc: 0.1408 - val_loss: 2.2567 - val_acc: 0.1633\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.2414 - acc: 0.1742 - val_loss: 2.2513 - val_acc: 0.2033\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2348 - acc: 0.1967 - val_loss: 2.2456 - val_acc: 0.2100\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.2284 - acc: 0.2042 - val_loss: 2.2413 - val_acc: 0.2067\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2216 - acc: 0.2050 - val_loss: 2.2346 - val_acc: 0.2067\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2140 - acc: 0.2133 - val_loss: 2.2294 - val_acc: 0.2033\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2066 - acc: 0.2125 - val_loss: 2.2222 - val_acc: 0.2067\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.1983 - acc: 0.2167 - val_loss: 2.2170 - val_acc: 0.2033\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 44us/step - loss: 2.1902 - acc: 0.2158 - val_loss: 2.2119 - val_acc: 0.2033\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 2.1820 - acc: 0.2133 - val_loss: 2.2032 - val_acc: 0.2100\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.1730 - acc: 0.2158 - val_loss: 2.2012 - val_acc: 0.2033\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.1647 - acc: 0.2150 - val_loss: 2.1914 - val_acc: 0.2067\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 0s 391us/step - loss: 2.3525 - acc: 0.0817 - val_loss: 2.3403 - val_acc: 0.0900\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.3282 - acc: 0.0817 - val_loss: 2.3223 - val_acc: 0.0900\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.3120 - acc: 0.0817 - val_loss: 2.3080 - val_acc: 0.0900\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.3002 - acc: 0.1025 - val_loss: 2.2973 - val_acc: 0.0733\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.2910 - acc: 0.1075 - val_loss: 2.2891 - val_acc: 0.0733\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 2.2830 - acc: 0.1075 - val_loss: 2.2816 - val_acc: 0.0733\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 45us/step - loss: 2.2751 - acc: 0.1075 - val_loss: 2.2729 - val_acc: 0.0733\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 2.2667 - acc: 0.1292 - val_loss: 2.2637 - val_acc: 0.1533\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 2.2573 - acc: 0.1883 - val_loss: 2.2527 - val_acc: 0.1667\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 2.2466 - acc: 0.1875 - val_loss: 2.2406 - val_acc: 0.1667\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 2.2340 - acc: 0.1908 - val_loss: 2.2262 - val_acc: 0.2667\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 2.2189 - acc: 0.2733 - val_loss: 2.2093 - val_acc: 0.2833\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 2.2019 - acc: 0.2642 - val_loss: 2.1909 - val_acc: 0.2767\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.1822 - acc: 0.2558 - val_loss: 2.1698 - val_acc: 0.2633\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.1607 - acc: 0.2317 - val_loss: 2.1465 - val_acc: 0.2500\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 2.1376 - acc: 0.2333 - val_loss: 2.1235 - val_acc: 0.2467\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 2.1127 - acc: 0.2358 - val_loss: 2.0996 - val_acc: 0.2567\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 2.0869 - acc: 0.2308 - val_loss: 2.0729 - val_acc: 0.2400\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 2.0611 - acc: 0.2267 - val_loss: 2.0499 - val_acc: 0.2500\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.0354 - acc: 0.2358 - val_loss: 2.0271 - val_acc: 0.2500\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 2.0115 - acc: 0.2350 - val_loss: 2.0065 - val_acc: 0.2500\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 1.9869 - acc: 0.2408 - val_loss: 1.9867 - val_acc: 0.2533\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 1.9649 - acc: 0.2383 - val_loss: 1.9694 - val_acc: 0.2533\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 1.9433 - acc: 0.2425 - val_loss: 1.9526 - val_acc: 0.2533\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 45us/step - loss: 1.9236 - acc: 0.2392 - val_loss: 1.9382 - val_acc: 0.2500\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 1.9055 - acc: 0.2575 - val_loss: 1.9245 - val_acc: 0.2667\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.8876 - acc: 0.2558 - val_loss: 1.9128 - val_acc: 0.2600\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 1.8724 - acc: 0.2650 - val_loss: 1.9002 - val_acc: 0.2600\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 85us/step - loss: 1.8569 - acc: 0.2500 - val_loss: 1.8935 - val_acc: 0.2600\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.8437 - acc: 0.2558 - val_loss: 1.8830 - val_acc: 0.2733\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 0s 398us/step - loss: 2.4346 - acc: 0.1017 - val_loss: 2.4102 - val_acc: 0.1167\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 45us/step - loss: 2.3591 - acc: 0.1017 - val_loss: 2.3545 - val_acc: 0.1167\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 44us/step - loss: 2.3186 - acc: 0.1017 - val_loss: 2.3194 - val_acc: 0.1167\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 44us/step - loss: 2.2910 - acc: 0.1017 - val_loss: 2.2943 - val_acc: 0.1167\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 42us/step - loss: 2.2712 - acc: 0.1017 - val_loss: 2.2741 - val_acc: 0.1167\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 2.2517 - acc: 0.1250 - val_loss: 2.2541 - val_acc: 0.1967\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 44us/step - loss: 2.2311 - acc: 0.2608 - val_loss: 2.2318 - val_acc: 0.3100\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 44us/step - loss: 2.2061 - acc: 0.3017 - val_loss: 2.2046 - val_acc: 0.3500\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 43us/step - loss: 2.1750 - acc: 0.3533 - val_loss: 2.1742 - val_acc: 0.3600\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 43us/step - loss: 2.1380 - acc: 0.3442 - val_loss: 2.1378 - val_acc: 0.3333\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 42us/step - loss: 2.0952 - acc: 0.3225 - val_loss: 2.1002 - val_acc: 0.2733\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 43us/step - loss: 2.0503 - acc: 0.2833 - val_loss: 2.0597 - val_acc: 0.2267\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 43us/step - loss: 2.0038 - acc: 0.2417 - val_loss: 2.0204 - val_acc: 0.2133\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 43us/step - loss: 1.9592 - acc: 0.2383 - val_loss: 1.9854 - val_acc: 0.2233\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 43us/step - loss: 1.9188 - acc: 0.2442 - val_loss: 1.9498 - val_acc: 0.2133\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 43us/step - loss: 1.8816 - acc: 0.2350 - val_loss: 1.9229 - val_acc: 0.2133\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 43us/step - loss: 1.8509 - acc: 0.2458 - val_loss: 1.8975 - val_acc: 0.2267\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 1.8228 - acc: 0.2592 - val_loss: 1.8769 - val_acc: 0.2367\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 1.7989 - acc: 0.2517 - val_loss: 1.8584 - val_acc: 0.2333\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 1.7777 - acc: 0.2767 - val_loss: 1.8428 - val_acc: 0.2633\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.7588 - acc: 0.2825 - val_loss: 1.8297 - val_acc: 0.2700\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 1.7432 - acc: 0.2992 - val_loss: 1.8163 - val_acc: 0.2667\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 1.7270 - acc: 0.3150 - val_loss: 1.8038 - val_acc: 0.2767\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 1.7127 - acc: 0.3200 - val_loss: 1.7921 - val_acc: 0.2767\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.6979 - acc: 0.3408 - val_loss: 1.7793 - val_acc: 0.2900\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 1.6822 - acc: 0.3667 - val_loss: 1.7698 - val_acc: 0.3233\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 43us/step - loss: 1.6665 - acc: 0.4092 - val_loss: 1.7540 - val_acc: 0.3900\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 44us/step - loss: 1.6508 - acc: 0.4358 - val_loss: 1.7409 - val_acc: 0.4200\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 42us/step - loss: 1.6334 - acc: 0.4608 - val_loss: 1.7252 - val_acc: 0.4200\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 43us/step - loss: 1.6146 - acc: 0.4825 - val_loss: 1.7085 - val_acc: 0.4667\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 0s 404us/step - loss: 2.3666 - acc: 0.1075 - val_loss: 2.3168 - val_acc: 0.0900\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2651 - acc: 0.2133 - val_loss: 2.2479 - val_acc: 0.2300\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 2.2151 - acc: 0.3067 - val_loss: 2.1899 - val_acc: 0.3300\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.1428 - acc: 0.3267 - val_loss: 2.1110 - val_acc: 0.3133\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 2.0461 - acc: 0.3475 - val_loss: 1.9985 - val_acc: 0.3700\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 1.9291 - acc: 0.4108 - val_loss: 1.8747 - val_acc: 0.5200\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 1.8007 - acc: 0.5258 - val_loss: 1.7491 - val_acc: 0.5867\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 1.6695 - acc: 0.6400 - val_loss: 1.6253 - val_acc: 0.6267\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 1.5413 - acc: 0.6608 - val_loss: 1.5122 - val_acc: 0.6633\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 1.4212 - acc: 0.6908 - val_loss: 1.4055 - val_acc: 0.7033\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 1.3106 - acc: 0.7525 - val_loss: 1.3064 - val_acc: 0.7400\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 1.2113 - acc: 0.7817 - val_loss: 1.2267 - val_acc: 0.7267\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 1.1249 - acc: 0.7808 - val_loss: 1.1511 - val_acc: 0.7467\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 1.0435 - acc: 0.8158 - val_loss: 1.0828 - val_acc: 0.7400\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.9737 - acc: 0.8158 - val_loss: 1.0253 - val_acc: 0.7467\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.9102 - acc: 0.8475 - val_loss: 0.9797 - val_acc: 0.7800\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 0.8511 - acc: 0.8508 - val_loss: 0.9360 - val_acc: 0.7867\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 0.7984 - acc: 0.8558 - val_loss: 0.8868 - val_acc: 0.7967\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 0.7488 - acc: 0.8725 - val_loss: 0.8587 - val_acc: 0.7767\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 0.7051 - acc: 0.8750 - val_loss: 0.8274 - val_acc: 0.7800\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 0.6667 - acc: 0.8900 - val_loss: 0.8047 - val_acc: 0.7867\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 0.6252 - acc: 0.9000 - val_loss: 0.7735 - val_acc: 0.8033\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 0.5899 - acc: 0.9000 - val_loss: 0.7560 - val_acc: 0.8033\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 0.5585 - acc: 0.9033 - val_loss: 0.7335 - val_acc: 0.8133\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 0.5266 - acc: 0.9150 - val_loss: 0.7165 - val_acc: 0.8100\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 0.4979 - acc: 0.9233 - val_loss: 0.6998 - val_acc: 0.8133\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 0.4705 - acc: 0.9333 - val_loss: 0.6838 - val_acc: 0.8233\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 0.4467 - acc: 0.9358 - val_loss: 0.6708 - val_acc: 0.8167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.4210 - acc: 0.9417 - val_loss: 0.6604 - val_acc: 0.8167\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 0.4007 - acc: 0.9500 - val_loss: 0.6517 - val_acc: 0.8200\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 0s 408us/step - loss: 2.2569 - acc: 0.1575 - val_loss: 2.2036 - val_acc: 0.1767\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.1377 - acc: 0.1808 - val_loss: 2.0859 - val_acc: 0.1800\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 2.0157 - acc: 0.2108 - val_loss: 1.9911 - val_acc: 0.1833\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 1.9287 - acc: 0.2075 - val_loss: 1.9253 - val_acc: 0.1767\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 1.8691 - acc: 0.2083 - val_loss: 1.8811 - val_acc: 0.1800\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 1.8273 - acc: 0.2133 - val_loss: 1.8453 - val_acc: 0.1900\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 128us/step - loss: 1.7914 - acc: 0.2200 - val_loss: 1.8202 - val_acc: 0.1967\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.7617 - acc: 0.2350 - val_loss: 1.7912 - val_acc: 0.2233\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 1.7380 - acc: 0.2425 - val_loss: 1.7801 - val_acc: 0.2300\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 1.7144 - acc: 0.2475 - val_loss: 1.7603 - val_acc: 0.2267\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 1.6962 - acc: 0.2633 - val_loss: 1.7392 - val_acc: 0.2467\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 1.6757 - acc: 0.2583 - val_loss: 1.7341 - val_acc: 0.2400\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 1.6600 - acc: 0.2767 - val_loss: 1.7246 - val_acc: 0.2533\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 1.6397 - acc: 0.2842 - val_loss: 1.7045 - val_acc: 0.2500\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 1.6222 - acc: 0.2933 - val_loss: 1.6942 - val_acc: 0.2700\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 1.6036 - acc: 0.3067 - val_loss: 1.6839 - val_acc: 0.2633\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 1.5773 - acc: 0.3167 - val_loss: 1.6778 - val_acc: 0.2667\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 1.5412 - acc: 0.3408 - val_loss: 1.6534 - val_acc: 0.2867\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 1.5030 - acc: 0.3592 - val_loss: 1.6056 - val_acc: 0.3133\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 1.4648 - acc: 0.3842 - val_loss: 1.5916 - val_acc: 0.3267\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 1.4408 - acc: 0.4050 - val_loss: 1.5603 - val_acc: 0.3467\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 1.4155 - acc: 0.4067 - val_loss: 1.5386 - val_acc: 0.3833\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 1.3957 - acc: 0.4250 - val_loss: 1.5144 - val_acc: 0.3967\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 1.3740 - acc: 0.4408 - val_loss: 1.5072 - val_acc: 0.4033\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 1.3654 - acc: 0.4583 - val_loss: 1.5022 - val_acc: 0.4267\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 1.3455 - acc: 0.4600 - val_loss: 1.4995 - val_acc: 0.4267\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 1.3288 - acc: 0.4708 - val_loss: 1.4812 - val_acc: 0.4367\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 1.3152 - acc: 0.4725 - val_loss: 1.4691 - val_acc: 0.4367\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 1.3050 - acc: 0.4900 - val_loss: 1.4681 - val_acc: 0.4467\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 1.2922 - acc: 0.4883 - val_loss: 1.4647 - val_acc: 0.4433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 514us/step - loss: 2.2507 - acc: 0.1458 - val_loss: 2.1843 - val_acc: 0.1600\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.1283 - acc: 0.2017 - val_loss: 2.0862 - val_acc: 0.1833\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 2.0068 - acc: 0.2100 - val_loss: 1.9689 - val_acc: 0.1867\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.8783 - acc: 0.2942 - val_loss: 1.8440 - val_acc: 0.2833\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.7528 - acc: 0.3125 - val_loss: 1.7242 - val_acc: 0.3500\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 1.6331 - acc: 0.3992 - val_loss: 1.6207 - val_acc: 0.4100\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 1.5267 - acc: 0.4292 - val_loss: 1.5076 - val_acc: 0.4267\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 1.4187 - acc: 0.4683 - val_loss: 1.4130 - val_acc: 0.4500\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.3045 - acc: 0.5350 - val_loss: 1.3073 - val_acc: 0.5600\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.1839 - acc: 0.6100 - val_loss: 1.2000 - val_acc: 0.5967\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 1.0828 - acc: 0.6575 - val_loss: 1.1077 - val_acc: 0.6367\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 1.0002 - acc: 0.6808 - val_loss: 1.0555 - val_acc: 0.6433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 0.9330 - acc: 0.6967 - val_loss: 1.0133 - val_acc: 0.6400\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.8766 - acc: 0.6983 - val_loss: 0.9964 - val_acc: 0.6533\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 0.8321 - acc: 0.7125 - val_loss: 0.9517 - val_acc: 0.6867\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 0.7893 - acc: 0.7400 - val_loss: 0.9434 - val_acc: 0.7233\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 0.7608 - acc: 0.7517 - val_loss: 0.9105 - val_acc: 0.7133\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 0.7291 - acc: 0.7500 - val_loss: 0.9024 - val_acc: 0.7267\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.7024 - acc: 0.7717 - val_loss: 0.8902 - val_acc: 0.7200\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 0.6759 - acc: 0.7733 - val_loss: 0.8643 - val_acc: 0.7233\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 0.6569 - acc: 0.7875 - val_loss: 0.8589 - val_acc: 0.7267\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 0.6479 - acc: 0.7750 - val_loss: 0.8462 - val_acc: 0.7400\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 0.6196 - acc: 0.8108 - val_loss: 0.8446 - val_acc: 0.7500\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 0.6017 - acc: 0.8233 - val_loss: 0.8642 - val_acc: 0.7500\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 0.5862 - acc: 0.8200 - val_loss: 0.8334 - val_acc: 0.7533\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 0.5715 - acc: 0.8425 - val_loss: 0.8244 - val_acc: 0.7600\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 0.5581 - acc: 0.8433 - val_loss: 0.8227 - val_acc: 0.7667\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 60us/step - loss: 0.5462 - acc: 0.8408 - val_loss: 0.8192 - val_acc: 0.7600\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 0.5360 - acc: 0.8500 - val_loss: 0.8262 - val_acc: 0.7433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 0.5275 - acc: 0.8450 - val_loss: 0.8355 - val_acc: 0.7567\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 0s 413us/step - loss: 2.2089 - acc: 0.1958 - val_loss: 2.0555 - val_acc: 0.2700\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 1.9332 - acc: 0.3092 - val_loss: 1.7782 - val_acc: 0.3967\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 1.6704 - acc: 0.4650 - val_loss: 1.5098 - val_acc: 0.5100\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 1.4168 - acc: 0.5425 - val_loss: 1.2778 - val_acc: 0.5700\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 1.1916 - acc: 0.6333 - val_loss: 1.0780 - val_acc: 0.6267\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.0065 - acc: 0.7083 - val_loss: 0.9538 - val_acc: 0.6967\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 0.8475 - acc: 0.7642 - val_loss: 0.8617 - val_acc: 0.7200\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 0.7232 - acc: 0.8042 - val_loss: 0.7546 - val_acc: 0.7533\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 0.6424 - acc: 0.8183 - val_loss: 0.7028 - val_acc: 0.7733\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.5688 - acc: 0.8433 - val_loss: 0.6534 - val_acc: 0.8000\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 0.5121 - acc: 0.8642 - val_loss: 0.6241 - val_acc: 0.8067\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 0.4753 - acc: 0.8725 - val_loss: 0.6184 - val_acc: 0.8100\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 0.4303 - acc: 0.8900 - val_loss: 0.5946 - val_acc: 0.8267\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 0.3952 - acc: 0.9000 - val_loss: 0.5825 - val_acc: 0.8267\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 0.3744 - acc: 0.8967 - val_loss: 0.5698 - val_acc: 0.8133\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 0.3378 - acc: 0.9192 - val_loss: 0.5760 - val_acc: 0.8200\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 0.3192 - acc: 0.9200 - val_loss: 0.5579 - val_acc: 0.8233\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 0.3008 - acc: 0.9225 - val_loss: 0.5576 - val_acc: 0.8333\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 0.2818 - acc: 0.9325 - val_loss: 0.5666 - val_acc: 0.8367\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 0.2631 - acc: 0.9350 - val_loss: 0.5542 - val_acc: 0.8300\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 0.2485 - acc: 0.9433 - val_loss: 0.5634 - val_acc: 0.8233\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 0.2391 - acc: 0.9442 - val_loss: 0.5673 - val_acc: 0.8367\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 0.2227 - acc: 0.9492 - val_loss: 0.5575 - val_acc: 0.8367\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 0.2140 - acc: 0.9525 - val_loss: 0.5707 - val_acc: 0.8367\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 0.2002 - acc: 0.9542 - val_loss: 0.5525 - val_acc: 0.8467\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 0.1878 - acc: 0.9567 - val_loss: 0.5549 - val_acc: 0.8433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 0.1734 - acc: 0.9617 - val_loss: 0.5573 - val_acc: 0.8467\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 0.1687 - acc: 0.9667 - val_loss: 0.5625 - val_acc: 0.8467\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 0.1603 - acc: 0.9742 - val_loss: 0.5773 - val_acc: 0.8400\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 0.1527 - acc: 0.9708 - val_loss: 0.5707 - val_acc: 0.8400\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 448us/step - loss: 2.0117 - acc: 0.3725 - val_loss: 1.5760 - val_acc: 0.6033\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.0746 - acc: 0.7400 - val_loss: 0.8604 - val_acc: 0.7300\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 0.6000 - acc: 0.8292 - val_loss: 0.6983 - val_acc: 0.7833\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 0.4666 - acc: 0.8667 - val_loss: 0.6139 - val_acc: 0.8100\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 0.3720 - acc: 0.8867 - val_loss: 0.5929 - val_acc: 0.8033\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.3058 - acc: 0.9125 - val_loss: 0.5608 - val_acc: 0.8167\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.2547 - acc: 0.9358 - val_loss: 0.5565 - val_acc: 0.8133\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 0.2119 - acc: 0.9483 - val_loss: 0.5240 - val_acc: 0.8200\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 0.1804 - acc: 0.9592 - val_loss: 0.5632 - val_acc: 0.8167\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.1483 - acc: 0.9683 - val_loss: 0.5728 - val_acc: 0.8367\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 0.1268 - acc: 0.9775 - val_loss: 0.5634 - val_acc: 0.8267\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.1094 - acc: 0.9867 - val_loss: 0.5730 - val_acc: 0.8333\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 0.0966 - acc: 0.9850 - val_loss: 0.5936 - val_acc: 0.8233\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 0.0798 - acc: 0.9908 - val_loss: 0.6279 - val_acc: 0.8400\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 0.0711 - acc: 0.9892 - val_loss: 0.6121 - val_acc: 0.8400\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 0.0590 - acc: 0.9942 - val_loss: 0.5953 - val_acc: 0.8500\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.0484 - acc: 0.9958 - val_loss: 0.6291 - val_acc: 0.8433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.0421 - acc: 0.9975 - val_loss: 0.6080 - val_acc: 0.8467\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 0.0372 - acc: 0.9975 - val_loss: 0.6006 - val_acc: 0.8533\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 0.0326 - acc: 0.9967 - val_loss: 0.6250 - val_acc: 0.8567\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 0.0267 - acc: 0.9983 - val_loss: 0.6402 - val_acc: 0.8467\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.0232 - acc: 0.9983 - val_loss: 0.6385 - val_acc: 0.8500\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 0.0195 - acc: 0.9983 - val_loss: 0.6744 - val_acc: 0.8500\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 0.0173 - acc: 0.9983 - val_loss: 0.6716 - val_acc: 0.8500\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.0142 - acc: 1.0000 - val_loss: 0.6815 - val_acc: 0.8533\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.6931 - val_acc: 0.8600\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.0106 - acc: 1.0000 - val_loss: 0.6909 - val_acc: 0.8567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 0.7110 - val_acc: 0.8533\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.7027 - val_acc: 0.8600\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.7204 - val_acc: 0.8600\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 435us/step - loss: 2.2904 - acc: 0.1158 - val_loss: 2.2741 - val_acc: 0.1667\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 2.2453 - acc: 0.1617 - val_loss: 2.2133 - val_acc: 0.1867\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 2.1795 - acc: 0.1708 - val_loss: 2.1529 - val_acc: 0.2067\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 2.1265 - acc: 0.1833 - val_loss: 2.1163 - val_acc: 0.2133\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 2.0876 - acc: 0.1933 - val_loss: 2.0870 - val_acc: 0.2233\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 2.0576 - acc: 0.1983 - val_loss: 2.0586 - val_acc: 0.2367\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 2.0334 - acc: 0.2058 - val_loss: 2.0395 - val_acc: 0.2500\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 2.0118 - acc: 0.2075 - val_loss: 2.0285 - val_acc: 0.2167\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 1.9919 - acc: 0.2033 - val_loss: 2.0312 - val_acc: 0.2000\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 1.9788 - acc: 0.2142 - val_loss: 2.0075 - val_acc: 0.2300\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 1.9625 - acc: 0.2167 - val_loss: 1.9977 - val_acc: 0.2267\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 1.9472 - acc: 0.2167 - val_loss: 2.0016 - val_acc: 0.2067\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 1.9358 - acc: 0.2167 - val_loss: 1.9866 - val_acc: 0.2300\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 1.9227 - acc: 0.2208 - val_loss: 1.9795 - val_acc: 0.2300\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 1.9119 - acc: 0.2192 - val_loss: 1.9722 - val_acc: 0.2333\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 1.9007 - acc: 0.2258 - val_loss: 1.9650 - val_acc: 0.2400\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 1.8907 - acc: 0.2233 - val_loss: 1.9655 - val_acc: 0.2300\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 1.8810 - acc: 0.2258 - val_loss: 1.9669 - val_acc: 0.2367\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.8715 - acc: 0.2300 - val_loss: 1.9666 - val_acc: 0.2367\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.8628 - acc: 0.2292 - val_loss: 1.9614 - val_acc: 0.2400\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 1.8541 - acc: 0.2325 - val_loss: 1.9591 - val_acc: 0.2367\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 1.8455 - acc: 0.2325 - val_loss: 1.9646 - val_acc: 0.2333\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 1.8372 - acc: 0.2442 - val_loss: 1.9494 - val_acc: 0.2433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 1.8290 - acc: 0.2450 - val_loss: 1.9431 - val_acc: 0.2433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 1.8200 - acc: 0.2450 - val_loss: 1.9477 - val_acc: 0.2367\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 1.8120 - acc: 0.2483 - val_loss: 1.9571 - val_acc: 0.2400\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 1.8035 - acc: 0.2492 - val_loss: 1.9514 - val_acc: 0.2433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 1.7975 - acc: 0.2458 - val_loss: 1.9365 - val_acc: 0.2433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 1.7885 - acc: 0.2517 - val_loss: 1.9296 - val_acc: 0.2500\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 1.7827 - acc: 0.2575 - val_loss: 1.9159 - val_acc: 0.2500\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 524us/step - loss: 2.2937 - acc: 0.1067 - val_loss: 2.2591 - val_acc: 0.1433\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 2.2215 - acc: 0.1700 - val_loss: 2.1432 - val_acc: 0.1933\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.1178 - acc: 0.1925 - val_loss: 2.0424 - val_acc: 0.2233\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 2.0350 - acc: 0.2350 - val_loss: 1.9696 - val_acc: 0.2233\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 1.9654 - acc: 0.2308 - val_loss: 1.9069 - val_acc: 0.2500\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 1.9106 - acc: 0.2567 - val_loss: 1.8620 - val_acc: 0.2633\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.8611 - acc: 0.2642 - val_loss: 1.8194 - val_acc: 0.2967\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.8147 - acc: 0.3242 - val_loss: 1.7850 - val_acc: 0.3567\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.7653 - acc: 0.3408 - val_loss: 1.7352 - val_acc: 0.3200\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.7093 - acc: 0.3408 - val_loss: 1.6952 - val_acc: 0.3467\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.6472 - acc: 0.3642 - val_loss: 1.6547 - val_acc: 0.3633\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.5948 - acc: 0.3783 - val_loss: 1.6088 - val_acc: 0.3567\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.5487 - acc: 0.3942 - val_loss: 1.5799 - val_acc: 0.3733\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 1.5128 - acc: 0.4308 - val_loss: 1.5586 - val_acc: 0.3833\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.4732 - acc: 0.4750 - val_loss: 1.5310 - val_acc: 0.3900\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.4392 - acc: 0.4808 - val_loss: 1.5297 - val_acc: 0.4100\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.4024 - acc: 0.4833 - val_loss: 1.4845 - val_acc: 0.3867\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.3582 - acc: 0.4883 - val_loss: 1.4710 - val_acc: 0.4233\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.2957 - acc: 0.4967 - val_loss: 1.3845 - val_acc: 0.4200\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.2235 - acc: 0.5067 - val_loss: 1.3466 - val_acc: 0.4267\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.1637 - acc: 0.5275 - val_loss: 1.3160 - val_acc: 0.4300\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.1220 - acc: 0.5300 - val_loss: 1.2831 - val_acc: 0.4100\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.0728 - acc: 0.5683 - val_loss: 1.2637 - val_acc: 0.4333\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.0427 - acc: 0.5875 - val_loss: 1.2532 - val_acc: 0.4433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.0111 - acc: 0.5858 - val_loss: 1.2412 - val_acc: 0.4400\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.9908 - acc: 0.6025 - val_loss: 1.2240 - val_acc: 0.4833\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.9592 - acc: 0.6250 - val_loss: 1.2275 - val_acc: 0.5033\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.9312 - acc: 0.6533 - val_loss: 1.2074 - val_acc: 0.5067\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 0.9095 - acc: 0.6825 - val_loss: 1.2398 - val_acc: 0.4967\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 0.8849 - acc: 0.7017 - val_loss: 1.2064 - val_acc: 0.5200\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 435us/step - loss: 2.2430 - acc: 0.1050 - val_loss: 2.1929 - val_acc: 0.0967\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 2.0953 - acc: 0.1450 - val_loss: 2.0766 - val_acc: 0.1567\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 1.9614 - acc: 0.1900 - val_loss: 1.9468 - val_acc: 0.2000\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.7985 - acc: 0.3067 - val_loss: 1.7613 - val_acc: 0.3767\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.5971 - acc: 0.4383 - val_loss: 1.5619 - val_acc: 0.4467\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.4154 - acc: 0.5158 - val_loss: 1.3866 - val_acc: 0.4767\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.2613 - acc: 0.5525 - val_loss: 1.2798 - val_acc: 0.5600\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.1199 - acc: 0.5983 - val_loss: 1.1658 - val_acc: 0.6300\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.0071 - acc: 0.6567 - val_loss: 1.1045 - val_acc: 0.6467\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.9149 - acc: 0.6875 - val_loss: 1.0477 - val_acc: 0.7000\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 0.8195 - acc: 0.7475 - val_loss: 0.9863 - val_acc: 0.7167\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.7507 - acc: 0.7950 - val_loss: 0.9476 - val_acc: 0.7367\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 0.6920 - acc: 0.8133 - val_loss: 0.9502 - val_acc: 0.7467\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 0.6224 - acc: 0.8550 - val_loss: 0.9172 - val_acc: 0.7533\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.5670 - acc: 0.8642 - val_loss: 0.8773 - val_acc: 0.7667\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.5333 - acc: 0.8758 - val_loss: 0.8616 - val_acc: 0.7600\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.4828 - acc: 0.8783 - val_loss: 0.8681 - val_acc: 0.7733\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.4421 - acc: 0.8875 - val_loss: 0.8862 - val_acc: 0.7733\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.4008 - acc: 0.9083 - val_loss: 0.9122 - val_acc: 0.7600\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.3740 - acc: 0.9142 - val_loss: 0.8915 - val_acc: 0.7667\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 0.3387 - acc: 0.9100 - val_loss: 0.8765 - val_acc: 0.7733\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.3090 - acc: 0.9233 - val_loss: 0.8642 - val_acc: 0.7800\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.2808 - acc: 0.9317 - val_loss: 0.8570 - val_acc: 0.7800\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.2579 - acc: 0.9392 - val_loss: 0.8860 - val_acc: 0.7800\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.2344 - acc: 0.9408 - val_loss: 0.9521 - val_acc: 0.7667\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.2168 - acc: 0.9492 - val_loss: 0.9126 - val_acc: 0.7800\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.1973 - acc: 0.9583 - val_loss: 0.9277 - val_acc: 0.7700\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.1842 - acc: 0.9608 - val_loss: 0.9412 - val_acc: 0.7833\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.1697 - acc: 0.9650 - val_loss: 0.9759 - val_acc: 0.7700\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 0.1590 - acc: 0.9608 - val_loss: 0.9509 - val_acc: 0.7867\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 460us/step - loss: 2.0929 - acc: 0.3367 - val_loss: 1.7620 - val_acc: 0.5100\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 1.2830 - acc: 0.6650 - val_loss: 0.9735 - val_acc: 0.7567\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 0.7184 - acc: 0.8017 - val_loss: 0.6669 - val_acc: 0.8067\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.5255 - acc: 0.8525 - val_loss: 0.5764 - val_acc: 0.8233\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 0.3953 - acc: 0.8917 - val_loss: 0.5121 - val_acc: 0.8333\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 0.3222 - acc: 0.9117 - val_loss: 0.4761 - val_acc: 0.8500\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 0.2596 - acc: 0.9308 - val_loss: 0.4580 - val_acc: 0.8600\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 0.2199 - acc: 0.9367 - val_loss: 0.4573 - val_acc: 0.8533\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 94us/step - loss: 0.1776 - acc: 0.9592 - val_loss: 0.4975 - val_acc: 0.8633\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 164us/step - loss: 0.1600 - acc: 0.9592 - val_loss: 0.4331 - val_acc: 0.8767\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 0.1265 - acc: 0.9750 - val_loss: 0.4324 - val_acc: 0.8767\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.1029 - acc: 0.9767 - val_loss: 0.4206 - val_acc: 0.8700\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 162us/step - loss: 0.0848 - acc: 0.9858 - val_loss: 0.4659 - val_acc: 0.8667\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.0694 - acc: 0.9900 - val_loss: 0.5144 - val_acc: 0.8800\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 0.0598 - acc: 0.9908 - val_loss: 0.5061 - val_acc: 0.8733\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 0.0455 - acc: 0.9958 - val_loss: 0.4458 - val_acc: 0.8667\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.0374 - acc: 0.9967 - val_loss: 0.4619 - val_acc: 0.8767\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 0.0293 - acc: 0.9992 - val_loss: 0.4979 - val_acc: 0.8767\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 0.0271 - acc: 0.9958 - val_loss: 0.4995 - val_acc: 0.8700\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 120us/step - loss: 0.0239 - acc: 0.9992 - val_loss: 0.4756 - val_acc: 0.8733\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 0.0157 - acc: 1.0000 - val_loss: 0.5140 - val_acc: 0.8700\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.5173 - val_acc: 0.8767\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.5139 - val_acc: 0.8800\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.5238 - val_acc: 0.8733\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.5470 - val_acc: 0.8733\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.5473 - val_acc: 0.8767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.5532 - val_acc: 0.8800\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.5731 - val_acc: 0.8733\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.5748 - val_acc: 0.8733\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 185us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.5747 - val_acc: 0.8767\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 486us/step - loss: 2.3175 - acc: 0.0950 - val_loss: 2.3351 - val_acc: 0.0900\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 2.3101 - acc: 0.1000 - val_loss: 2.3264 - val_acc: 0.1400\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.3051 - acc: 0.1733 - val_loss: 2.3194 - val_acc: 0.1300\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 44us/step - loss: 2.3010 - acc: 0.1592 - val_loss: 2.3147 - val_acc: 0.1000\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 2.2979 - acc: 0.1450 - val_loss: 2.3099 - val_acc: 0.0833\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.2952 - acc: 0.1075 - val_loss: 2.3062 - val_acc: 0.0833\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 2.2926 - acc: 0.1500 - val_loss: 2.3024 - val_acc: 0.1533\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2901 - acc: 0.1875 - val_loss: 2.2997 - val_acc: 0.1500\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 2.2878 - acc: 0.1917 - val_loss: 2.2966 - val_acc: 0.1600\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.2854 - acc: 0.2058 - val_loss: 2.2934 - val_acc: 0.1867\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 49us/step - loss: 2.2828 - acc: 0.2067 - val_loss: 2.2906 - val_acc: 0.1933\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 2.2798 - acc: 0.2100 - val_loss: 2.2874 - val_acc: 0.2133\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 2.2766 - acc: 0.2150 - val_loss: 2.2837 - val_acc: 0.2133\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 2.2728 - acc: 0.2142 - val_loss: 2.2805 - val_acc: 0.2100\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2685 - acc: 0.2125 - val_loss: 2.2762 - val_acc: 0.2100\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2635 - acc: 0.2125 - val_loss: 2.2714 - val_acc: 0.2100\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 2.2580 - acc: 0.2133 - val_loss: 2.2663 - val_acc: 0.2100\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2515 - acc: 0.2158 - val_loss: 2.2597 - val_acc: 0.2100\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.2442 - acc: 0.2150 - val_loss: 2.2535 - val_acc: 0.2100\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2358 - acc: 0.2142 - val_loss: 2.2453 - val_acc: 0.2100\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.2261 - acc: 0.2217 - val_loss: 2.2370 - val_acc: 0.2100\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 2.2153 - acc: 0.2142 - val_loss: 2.2275 - val_acc: 0.2100\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.2034 - acc: 0.2275 - val_loss: 2.2157 - val_acc: 0.1933\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.1901 - acc: 0.2183 - val_loss: 2.2040 - val_acc: 0.1867\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.1758 - acc: 0.2175 - val_loss: 2.1912 - val_acc: 0.1867\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 2.1606 - acc: 0.2150 - val_loss: 2.1764 - val_acc: 0.1967\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 2.1441 - acc: 0.2158 - val_loss: 2.1626 - val_acc: 0.1867\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.1273 - acc: 0.2167 - val_loss: 2.1490 - val_acc: 0.1767\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.1107 - acc: 0.2175 - val_loss: 2.1317 - val_acc: 0.1900\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.0918 - acc: 0.2175 - val_loss: 2.1153 - val_acc: 0.2000\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 421us/step - loss: 2.4008 - acc: 0.0908 - val_loss: 2.3664 - val_acc: 0.1233\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.3628 - acc: 0.0908 - val_loss: 2.3403 - val_acc: 0.1233\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.3402 - acc: 0.0908 - val_loss: 2.3252 - val_acc: 0.1233\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.3260 - acc: 0.0908 - val_loss: 2.3156 - val_acc: 0.1233\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.3165 - acc: 0.0908 - val_loss: 2.3093 - val_acc: 0.1233\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 2.3099 - acc: 0.0908 - val_loss: 2.3044 - val_acc: 0.1233\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.3055 - acc: 0.0908 - val_loss: 2.3018 - val_acc: 0.1233\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 2.3024 - acc: 0.0908 - val_loss: 2.2991 - val_acc: 0.1233\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.2998 - acc: 0.0908 - val_loss: 2.2980 - val_acc: 0.1233\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 2.2977 - acc: 0.1000 - val_loss: 2.2968 - val_acc: 0.2567\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 2.2957 - acc: 0.1333 - val_loss: 2.2957 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2944 - acc: 0.1192 - val_loss: 2.2944 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2929 - acc: 0.1192 - val_loss: 2.2924 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.2906 - acc: 0.1192 - val_loss: 2.2914 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.2889 - acc: 0.1192 - val_loss: 2.2903 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.2862 - acc: 0.1192 - val_loss: 2.2888 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 2.2834 - acc: 0.1192 - val_loss: 2.2866 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.2800 - acc: 0.1192 - val_loss: 2.2838 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2756 - acc: 0.1192 - val_loss: 2.2802 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 2.2707 - acc: 0.1192 - val_loss: 2.2756 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.2647 - acc: 0.1192 - val_loss: 2.2701 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2577 - acc: 0.1875 - val_loss: 2.2637 - val_acc: 0.2000\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.2494 - acc: 0.2067 - val_loss: 2.2565 - val_acc: 0.1967\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2395 - acc: 0.1933 - val_loss: 2.2494 - val_acc: 0.1933\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 2.2283 - acc: 0.1958 - val_loss: 2.2397 - val_acc: 0.1933\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.2155 - acc: 0.1942 - val_loss: 2.2293 - val_acc: 0.1933\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 2.2014 - acc: 0.2383 - val_loss: 2.2182 - val_acc: 0.2600\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.1861 - acc: 0.2408 - val_loss: 2.2043 - val_acc: 0.2033\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.1686 - acc: 0.2358 - val_loss: 2.1911 - val_acc: 0.2033\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 2.1502 - acc: 0.2308 - val_loss: 2.1775 - val_acc: 0.2067\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 467us/step - loss: 2.4341 - acc: 0.0950 - val_loss: 2.3895 - val_acc: 0.0900\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.3596 - acc: 0.0950 - val_loss: 2.3384 - val_acc: 0.0900\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 2.3213 - acc: 0.0950 - val_loss: 2.3073 - val_acc: 0.0900\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2979 - acc: 0.0950 - val_loss: 2.2889 - val_acc: 0.0900\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 2.2805 - acc: 0.1117 - val_loss: 2.2743 - val_acc: 0.1700\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2646 - acc: 0.2008 - val_loss: 2.2570 - val_acc: 0.2200\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2457 - acc: 0.1833 - val_loss: 2.2337 - val_acc: 0.2033\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 2.2214 - acc: 0.1808 - val_loss: 2.2049 - val_acc: 0.2233\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 2.1919 - acc: 0.1900 - val_loss: 2.1751 - val_acc: 0.1700\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 2.1571 - acc: 0.2083 - val_loss: 2.1397 - val_acc: 0.2233\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 2.1203 - acc: 0.2217 - val_loss: 2.1032 - val_acc: 0.2233\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 2.0817 - acc: 0.2167 - val_loss: 2.0677 - val_acc: 0.2300\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 2.0451 - acc: 0.2208 - val_loss: 2.0329 - val_acc: 0.2300\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 2.0102 - acc: 0.2050 - val_loss: 2.0082 - val_acc: 0.2467\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 1.9791 - acc: 0.2300 - val_loss: 1.9845 - val_acc: 0.2267\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 1.9493 - acc: 0.2192 - val_loss: 1.9595 - val_acc: 0.2600\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 1.9224 - acc: 0.2150 - val_loss: 1.9384 - val_acc: 0.2267\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 1.9002 - acc: 0.2267 - val_loss: 1.9236 - val_acc: 0.2567\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 1.8779 - acc: 0.2183 - val_loss: 1.9113 - val_acc: 0.2167\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 116us/step - loss: 1.8569 - acc: 0.2283 - val_loss: 1.9004 - val_acc: 0.2200\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 1.8394 - acc: 0.2317 - val_loss: 1.8834 - val_acc: 0.2300\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 1.8225 - acc: 0.2333 - val_loss: 1.8752 - val_acc: 0.2300\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 94us/step - loss: 1.8056 - acc: 0.2375 - val_loss: 1.8628 - val_acc: 0.2267\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.7922 - acc: 0.2392 - val_loss: 1.8525 - val_acc: 0.2300\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.7762 - acc: 0.2483 - val_loss: 1.8442 - val_acc: 0.2333\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 1.7633 - acc: 0.2500 - val_loss: 1.8380 - val_acc: 0.2467\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.7506 - acc: 0.2517 - val_loss: 1.8253 - val_acc: 0.2367\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 1.7375 - acc: 0.2650 - val_loss: 1.8162 - val_acc: 0.2500\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 1.7249 - acc: 0.2792 - val_loss: 1.8109 - val_acc: 0.2533\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 1.7102 - acc: 0.2967 - val_loss: 1.8033 - val_acc: 0.2600\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 513us/step - loss: 2.3292 - acc: 0.1125 - val_loss: 2.2973 - val_acc: 0.1433\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.2882 - acc: 0.1225 - val_loss: 2.2793 - val_acc: 0.1433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.2705 - acc: 0.1450 - val_loss: 2.2563 - val_acc: 0.2600\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2403 - acc: 0.2533 - val_loss: 2.2158 - val_acc: 0.2967\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.1887 - acc: 0.3142 - val_loss: 2.1427 - val_acc: 0.3033\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.1068 - acc: 0.3342 - val_loss: 2.0452 - val_acc: 0.3267\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 1.9977 - acc: 0.3517 - val_loss: 1.9320 - val_acc: 0.3267\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 1.8802 - acc: 0.3292 - val_loss: 1.8156 - val_acc: 0.3533\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 1.7657 - acc: 0.3833 - val_loss: 1.7203 - val_acc: 0.3333\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 1.6674 - acc: 0.3750 - val_loss: 1.6347 - val_acc: 0.3600\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 1.5804 - acc: 0.3925 - val_loss: 1.5726 - val_acc: 0.3800\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.5051 - acc: 0.4442 - val_loss: 1.5092 - val_acc: 0.4633\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 1.4378 - acc: 0.4958 - val_loss: 1.4701 - val_acc: 0.4467\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 1.3764 - acc: 0.5092 - val_loss: 1.4175 - val_acc: 0.4833\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 1.3154 - acc: 0.5667 - val_loss: 1.3894 - val_acc: 0.5033\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 1.2592 - acc: 0.6058 - val_loss: 1.3495 - val_acc: 0.5167\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.2087 - acc: 0.6283 - val_loss: 1.3133 - val_acc: 0.5467\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 1.1561 - acc: 0.6458 - val_loss: 1.2777 - val_acc: 0.5833\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 1.1073 - acc: 0.6967 - val_loss: 1.2484 - val_acc: 0.5933\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 1.0635 - acc: 0.7075 - val_loss: 1.2177 - val_acc: 0.6067\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 1.0195 - acc: 0.7242 - val_loss: 1.1906 - val_acc: 0.6300\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 0.9775 - acc: 0.7375 - val_loss: 1.1652 - val_acc: 0.6400\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.9355 - acc: 0.7717 - val_loss: 1.1421 - val_acc: 0.6400\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 0.8964 - acc: 0.7742 - val_loss: 1.1205 - val_acc: 0.6467\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 0.8572 - acc: 0.7908 - val_loss: 1.0837 - val_acc: 0.6833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 0.8186 - acc: 0.8158 - val_loss: 1.0610 - val_acc: 0.7033\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 0.7818 - acc: 0.8392 - val_loss: 1.0355 - val_acc: 0.6933\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 0.7434 - acc: 0.8525 - val_loss: 1.0133 - val_acc: 0.7333\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 0.7044 - acc: 0.8717 - val_loss: 0.9860 - val_acc: 0.7367\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 0.6683 - acc: 0.8800 - val_loss: 0.9478 - val_acc: 0.7433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 549us/step - loss: 2.2782 - acc: 0.0908 - val_loss: 2.2486 - val_acc: 0.0933\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 2.2044 - acc: 0.1458 - val_loss: 2.1771 - val_acc: 0.1267\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 2.1169 - acc: 0.2025 - val_loss: 2.1111 - val_acc: 0.1733\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.0532 - acc: 0.2158 - val_loss: 2.0711 - val_acc: 0.1767\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.0030 - acc: 0.2175 - val_loss: 2.0304 - val_acc: 0.1933\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.9574 - acc: 0.2167 - val_loss: 1.9932 - val_acc: 0.1900\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 1.9035 - acc: 0.2300 - val_loss: 1.9469 - val_acc: 0.2033\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 1.8494 - acc: 0.2333 - val_loss: 1.9084 - val_acc: 0.2000\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.7991 - acc: 0.2442 - val_loss: 1.8726 - val_acc: 0.2167\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.7549 - acc: 0.2558 - val_loss: 1.8605 - val_acc: 0.2133\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 1.7179 - acc: 0.2750 - val_loss: 1.8242 - val_acc: 0.2300\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 1.6853 - acc: 0.2892 - val_loss: 1.8069 - val_acc: 0.2433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 1.6582 - acc: 0.3075 - val_loss: 1.7946 - val_acc: 0.2533\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.6311 - acc: 0.3283 - val_loss: 1.7654 - val_acc: 0.2700\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.6096 - acc: 0.3333 - val_loss: 1.7450 - val_acc: 0.3000\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 1.5918 - acc: 0.3342 - val_loss: 1.7366 - val_acc: 0.2933\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 1.5719 - acc: 0.3542 - val_loss: 1.7295 - val_acc: 0.3033\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 1.5553 - acc: 0.3667 - val_loss: 1.7293 - val_acc: 0.3167\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 1.5371 - acc: 0.3558 - val_loss: 1.6782 - val_acc: 0.3233\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 1.5226 - acc: 0.3842 - val_loss: 1.7140 - val_acc: 0.3333\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.5010 - acc: 0.3758 - val_loss: 1.6869 - val_acc: 0.3133\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.4817 - acc: 0.3908 - val_loss: 1.6999 - val_acc: 0.3400\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 1.4601 - acc: 0.3908 - val_loss: 1.6747 - val_acc: 0.3500\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.4366 - acc: 0.4092 - val_loss: 1.6629 - val_acc: 0.3800\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.4125 - acc: 0.4367 - val_loss: 1.6345 - val_acc: 0.3633\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.3933 - acc: 0.4275 - val_loss: 1.6195 - val_acc: 0.4033\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.3694 - acc: 0.4567 - val_loss: 1.6159 - val_acc: 0.3900\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 1.3487 - acc: 0.4533 - val_loss: 1.5837 - val_acc: 0.4233\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.3238 - acc: 0.4650 - val_loss: 1.5775 - val_acc: 0.3967\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 1.3010 - acc: 0.4908 - val_loss: 1.5582 - val_acc: 0.3967\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 597us/step - loss: 2.2618 - acc: 0.1292 - val_loss: 2.2482 - val_acc: 0.1700\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 2.1766 - acc: 0.2050 - val_loss: 2.2094 - val_acc: 0.2067\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.1069 - acc: 0.2450 - val_loss: 2.1689 - val_acc: 0.2567\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.0558 - acc: 0.2608 - val_loss: 2.1205 - val_acc: 0.2400\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 1.9975 - acc: 0.2658 - val_loss: 2.0864 - val_acc: 0.2367\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.9361 - acc: 0.2717 - val_loss: 2.0037 - val_acc: 0.2400\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.8672 - acc: 0.2792 - val_loss: 1.9061 - val_acc: 0.2600\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 1.7533 - acc: 0.2992 - val_loss: 1.8019 - val_acc: 0.2833\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.6417 - acc: 0.3217 - val_loss: 1.6744 - val_acc: 0.2667\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.5625 - acc: 0.3392 - val_loss: 1.6198 - val_acc: 0.2933\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 1.4936 - acc: 0.3658 - val_loss: 1.5478 - val_acc: 0.3267\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 1.4323 - acc: 0.3908 - val_loss: 1.4952 - val_acc: 0.3733\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.3685 - acc: 0.4533 - val_loss: 1.4310 - val_acc: 0.4767\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.2887 - acc: 0.5050 - val_loss: 1.3692 - val_acc: 0.4667\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 1.2133 - acc: 0.5367 - val_loss: 1.3260 - val_acc: 0.5033\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.1581 - acc: 0.5567 - val_loss: 1.2800 - val_acc: 0.5600\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.1152 - acc: 0.5767 - val_loss: 1.2407 - val_acc: 0.5633\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 1.0643 - acc: 0.6142 - val_loss: 1.1853 - val_acc: 0.5900\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.0203 - acc: 0.6317 - val_loss: 1.1916 - val_acc: 0.5867\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 0.9800 - acc: 0.6458 - val_loss: 1.1374 - val_acc: 0.5933\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 0.9457 - acc: 0.6675 - val_loss: 1.1568 - val_acc: 0.6067\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 0.9061 - acc: 0.6792 - val_loss: 1.1306 - val_acc: 0.6200\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 0.8758 - acc: 0.6883 - val_loss: 1.2064 - val_acc: 0.6133\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 0.8448 - acc: 0.6983 - val_loss: 1.0670 - val_acc: 0.6067\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 60us/step - loss: 0.8215 - acc: 0.7025 - val_loss: 1.0787 - val_acc: 0.6267\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 0.8050 - acc: 0.7033 - val_loss: 1.0527 - val_acc: 0.6333\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.7826 - acc: 0.7250 - val_loss: 1.0309 - val_acc: 0.6300\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 0.7626 - acc: 0.7300 - val_loss: 1.1191 - val_acc: 0.6133\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.7473 - acc: 0.7233 - val_loss: 1.1379 - val_acc: 0.6333\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 0.7385 - acc: 0.7300 - val_loss: 1.0534 - val_acc: 0.6467\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 570us/step - loss: 2.2088 - acc: 0.1583 - val_loss: 2.0744 - val_acc: 0.2967\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 1.9264 - acc: 0.2975 - val_loss: 1.7326 - val_acc: 0.3900\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.5739 - acc: 0.4633 - val_loss: 1.3657 - val_acc: 0.5967\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.2225 - acc: 0.6017 - val_loss: 1.0935 - val_acc: 0.6967\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 0.9667 - acc: 0.6942 - val_loss: 0.9651 - val_acc: 0.7133\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.8243 - acc: 0.7258 - val_loss: 0.8953 - val_acc: 0.7167\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.7234 - acc: 0.7558 - val_loss: 0.7970 - val_acc: 0.7567\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 0.6420 - acc: 0.8008 - val_loss: 0.7665 - val_acc: 0.7667\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 0.5845 - acc: 0.8100 - val_loss: 0.7625 - val_acc: 0.7533\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 0.5360 - acc: 0.8325 - val_loss: 0.7461 - val_acc: 0.7733\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 0.4901 - acc: 0.8667 - val_loss: 0.7579 - val_acc: 0.7800\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 0.4485 - acc: 0.8758 - val_loss: 0.7135 - val_acc: 0.8100\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.4190 - acc: 0.8767 - val_loss: 0.6992 - val_acc: 0.7933\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 0.3917 - acc: 0.8917 - val_loss: 0.6863 - val_acc: 0.8133\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 0.3611 - acc: 0.9033 - val_loss: 0.6869 - val_acc: 0.8200\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.3373 - acc: 0.9083 - val_loss: 0.6956 - val_acc: 0.7933\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.3059 - acc: 0.9283 - val_loss: 0.7130 - val_acc: 0.8033\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 0.2825 - acc: 0.9308 - val_loss: 0.6993 - val_acc: 0.8233\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.2641 - acc: 0.9350 - val_loss: 0.7159 - val_acc: 0.8200\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 0.2565 - acc: 0.9308 - val_loss: 0.7016 - val_acc: 0.8233\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 0.2336 - acc: 0.9400 - val_loss: 0.7587 - val_acc: 0.8033\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.2219 - acc: 0.9467 - val_loss: 0.7215 - val_acc: 0.8267\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.2090 - acc: 0.9467 - val_loss: 0.8031 - val_acc: 0.8067\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 0.1954 - acc: 0.9525 - val_loss: 0.7488 - val_acc: 0.8167\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 0.1786 - acc: 0.9600 - val_loss: 0.7619 - val_acc: 0.8167\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 0.1636 - acc: 0.9592 - val_loss: 0.7819 - val_acc: 0.8067\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 0.1549 - acc: 0.9642 - val_loss: 0.7583 - val_acc: 0.8133\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 0.1449 - acc: 0.9642 - val_loss: 0.7979 - val_acc: 0.8233\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 0.1343 - acc: 0.9717 - val_loss: 0.8567 - val_acc: 0.8100\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 0.1277 - acc: 0.9750 - val_loss: 0.8206 - val_acc: 0.8200\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 523us/step - loss: 2.0918 - acc: 0.2667 - val_loss: 1.6969 - val_acc: 0.4200\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 1.2749 - acc: 0.5817 - val_loss: 0.9120 - val_acc: 0.7467\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 0.6433 - acc: 0.8133 - val_loss: 0.6030 - val_acc: 0.8433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.4317 - acc: 0.8783 - val_loss: 0.6251 - val_acc: 0.8333\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 0.3543 - acc: 0.8942 - val_loss: 0.6147 - val_acc: 0.8367\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 0.2779 - acc: 0.9200 - val_loss: 0.5291 - val_acc: 0.8367\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.2254 - acc: 0.9367 - val_loss: 0.5945 - val_acc: 0.8367\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.1898 - acc: 0.9475 - val_loss: 0.5331 - val_acc: 0.8533\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 0.1502 - acc: 0.9658 - val_loss: 0.5439 - val_acc: 0.8433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.1223 - acc: 0.9700 - val_loss: 0.5438 - val_acc: 0.8633\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 0.1015 - acc: 0.9758 - val_loss: 0.5302 - val_acc: 0.8767\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.0837 - acc: 0.9833 - val_loss: 0.5490 - val_acc: 0.8600\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.0662 - acc: 0.9875 - val_loss: 0.5954 - val_acc: 0.8700\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.0538 - acc: 0.9925 - val_loss: 0.5870 - val_acc: 0.8667\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 0.0397 - acc: 0.9958 - val_loss: 0.5938 - val_acc: 0.8667\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.0326 - acc: 0.9992 - val_loss: 0.6204 - val_acc: 0.8700\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.0258 - acc: 1.0000 - val_loss: 0.6159 - val_acc: 0.8600\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.0197 - acc: 1.0000 - val_loss: 0.6463 - val_acc: 0.8633\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.0155 - acc: 1.0000 - val_loss: 0.6508 - val_acc: 0.8600\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.0124 - acc: 1.0000 - val_loss: 0.6552 - val_acc: 0.8533\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.6621 - val_acc: 0.8633\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.6954 - val_acc: 0.8633\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 0.6870 - val_acc: 0.8567\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.6888 - val_acc: 0.8633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.7041 - val_acc: 0.8667\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.7142 - val_acc: 0.8633\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.7132 - val_acc: 0.8633\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.7285 - val_acc: 0.8667\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.7347 - val_acc: 0.8667\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.7390 - val_acc: 0.8633\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 599us/step - loss: 2.2708 - acc: 0.0942 - val_loss: 2.2225 - val_acc: 0.1333\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 2.1911 - acc: 0.1442 - val_loss: 2.1480 - val_acc: 0.1667\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 2.1340 - acc: 0.1717 - val_loss: 2.0931 - val_acc: 0.1667\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 2.0935 - acc: 0.1925 - val_loss: 2.0543 - val_acc: 0.2300\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.0582 - acc: 0.2008 - val_loss: 2.0235 - val_acc: 0.2233\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 2.0218 - acc: 0.2008 - val_loss: 1.9830 - val_acc: 0.2400\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 1.9870 - acc: 0.2075 - val_loss: 1.9475 - val_acc: 0.2467\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 1.9499 - acc: 0.1992 - val_loss: 1.9098 - val_acc: 0.2400\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.9090 - acc: 0.2025 - val_loss: 1.8640 - val_acc: 0.2467\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 231us/step - loss: 1.8559 - acc: 0.2058 - val_loss: 1.8007 - val_acc: 0.2567\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 1.8031 - acc: 0.2025 - val_loss: 1.7422 - val_acc: 0.2567\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 1.7433 - acc: 0.2108 - val_loss: 1.7009 - val_acc: 0.2667\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 1.6933 - acc: 0.2267 - val_loss: 1.6608 - val_acc: 0.2933\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.6507 - acc: 0.2758 - val_loss: 1.6449 - val_acc: 0.2967\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.6084 - acc: 0.2842 - val_loss: 1.6122 - val_acc: 0.2833\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 1.5723 - acc: 0.3058 - val_loss: 1.5957 - val_acc: 0.2700\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 1.5404 - acc: 0.3342 - val_loss: 1.5860 - val_acc: 0.3200\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 1.5132 - acc: 0.3692 - val_loss: 1.5744 - val_acc: 0.3333\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.4854 - acc: 0.3817 - val_loss: 1.5376 - val_acc: 0.3433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.4588 - acc: 0.4083 - val_loss: 1.5337 - val_acc: 0.3800\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.4383 - acc: 0.4167 - val_loss: 1.5231 - val_acc: 0.4100\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.4099 - acc: 0.4158 - val_loss: 1.4922 - val_acc: 0.4000\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 1.3836 - acc: 0.4417 - val_loss: 1.5023 - val_acc: 0.4400\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 1.3717 - acc: 0.4400 - val_loss: 1.4687 - val_acc: 0.4267\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 1.3500 - acc: 0.4625 - val_loss: 1.4480 - val_acc: 0.4333\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 1.3197 - acc: 0.4742 - val_loss: 1.4504 - val_acc: 0.4433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.2959 - acc: 0.4725 - val_loss: 1.4513 - val_acc: 0.4600\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 1.2799 - acc: 0.4800 - val_loss: 1.4286 - val_acc: 0.4633\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 1.2573 - acc: 0.5050 - val_loss: 1.5361 - val_acc: 0.4900\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 1.2426 - acc: 0.5008 - val_loss: 1.4804 - val_acc: 0.4867\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 553us/step - loss: 2.2832 - acc: 0.1108 - val_loss: 2.2645 - val_acc: 0.1133\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 2.2387 - acc: 0.1300 - val_loss: 2.2139 - val_acc: 0.1400\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 2.1913 - acc: 0.1808 - val_loss: 2.1718 - val_acc: 0.1667\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.1476 - acc: 0.2158 - val_loss: 2.1320 - val_acc: 0.1867\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 2.1081 - acc: 0.2133 - val_loss: 2.0978 - val_acc: 0.1900\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 2.0714 - acc: 0.2200 - val_loss: 2.0773 - val_acc: 0.1367\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 2.0410 - acc: 0.2167 - val_loss: 2.0469 - val_acc: 0.1933\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 2.0161 - acc: 0.2142 - val_loss: 2.0286 - val_acc: 0.1933\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.9928 - acc: 0.2275 - val_loss: 2.0102 - val_acc: 0.2167\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 105us/step - loss: 1.9721 - acc: 0.2242 - val_loss: 1.9979 - val_acc: 0.2133\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 1.9545 - acc: 0.2283 - val_loss: 1.9856 - val_acc: 0.2167\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 95us/step - loss: 1.9348 - acc: 0.2092 - val_loss: 1.9704 - val_acc: 0.2233\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 1.9102 - acc: 0.2083 - val_loss: 1.9362 - val_acc: 0.2400\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.8503 - acc: 0.2117 - val_loss: 1.8398 - val_acc: 0.2533\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.7481 - acc: 0.2192 - val_loss: 1.7265 - val_acc: 0.2600\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.6650 - acc: 0.2525 - val_loss: 1.6845 - val_acc: 0.2933\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.6188 - acc: 0.2833 - val_loss: 1.6841 - val_acc: 0.3033\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.5797 - acc: 0.3083 - val_loss: 1.6173 - val_acc: 0.3233\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 1.5517 - acc: 0.3183 - val_loss: 1.5994 - val_acc: 0.3300\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.5201 - acc: 0.3292 - val_loss: 1.5991 - val_acc: 0.3233\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.4947 - acc: 0.3400 - val_loss: 1.5877 - val_acc: 0.3367\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 1.4718 - acc: 0.3583 - val_loss: 1.5815 - val_acc: 0.3600\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.4555 - acc: 0.3583 - val_loss: 1.5491 - val_acc: 0.3633\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 85us/step - loss: 1.4307 - acc: 0.3950 - val_loss: 1.5427 - val_acc: 0.3667\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 1.4061 - acc: 0.4008 - val_loss: 1.5332 - val_acc: 0.4067\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.3840 - acc: 0.4283 - val_loss: 1.5154 - val_acc: 0.4300\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.3747 - acc: 0.4400 - val_loss: 1.5252 - val_acc: 0.4300\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 1.3423 - acc: 0.4658 - val_loss: 1.5000 - val_acc: 0.4267\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.3133 - acc: 0.4850 - val_loss: 1.5058 - val_acc: 0.4433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.2903 - acc: 0.4967 - val_loss: 1.4996 - val_acc: 0.4300\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 551us/step - loss: 2.2842 - acc: 0.1708 - val_loss: 2.2404 - val_acc: 0.2500\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 2.1567 - acc: 0.3083 - val_loss: 2.0302 - val_acc: 0.3533\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.9579 - acc: 0.3733 - val_loss: 1.8104 - val_acc: 0.3633\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 1.7385 - acc: 0.3792 - val_loss: 1.5941 - val_acc: 0.3933\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.5319 - acc: 0.4133 - val_loss: 1.4333 - val_acc: 0.4567\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.3849 - acc: 0.4967 - val_loss: 1.2811 - val_acc: 0.5800\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 1.2404 - acc: 0.5333 - val_loss: 1.1864 - val_acc: 0.5767\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 1.1348 - acc: 0.5442 - val_loss: 1.1768 - val_acc: 0.5833\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.0313 - acc: 0.5633 - val_loss: 1.0619 - val_acc: 0.6033\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 0.9415 - acc: 0.6100 - val_loss: 1.0116 - val_acc: 0.6300\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 0.8668 - acc: 0.6425 - val_loss: 1.0250 - val_acc: 0.6100\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.7937 - acc: 0.6958 - val_loss: 0.9666 - val_acc: 0.6733\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 0.7163 - acc: 0.7392 - val_loss: 0.9206 - val_acc: 0.6667\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 0.6592 - acc: 0.7800 - val_loss: 0.8836 - val_acc: 0.6767\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.5873 - acc: 0.8117 - val_loss: 0.8868 - val_acc: 0.6833\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 0.5340 - acc: 0.8183 - val_loss: 0.9164 - val_acc: 0.7000\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 0.4849 - acc: 0.8417 - val_loss: 0.8708 - val_acc: 0.6900\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.4658 - acc: 0.8475 - val_loss: 0.8922 - val_acc: 0.7033\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.4257 - acc: 0.8650 - val_loss: 0.8591 - val_acc: 0.7200\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.3909 - acc: 0.8733 - val_loss: 0.8771 - val_acc: 0.7300\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 0.3763 - acc: 0.8833 - val_loss: 0.9299 - val_acc: 0.7100\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.3496 - acc: 0.8933 - val_loss: 0.8670 - val_acc: 0.7167\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.3286 - acc: 0.9067 - val_loss: 0.8752 - val_acc: 0.7300\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 0.2990 - acc: 0.9175 - val_loss: 0.9209 - val_acc: 0.7267\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 0.2909 - acc: 0.9133 - val_loss: 0.9015 - val_acc: 0.7433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.2725 - acc: 0.9200 - val_loss: 0.9361 - val_acc: 0.7400\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 0.2515 - acc: 0.9333 - val_loss: 0.9142 - val_acc: 0.7533\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 0.2343 - acc: 0.9342 - val_loss: 0.9534 - val_acc: 0.7500\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 0.2230 - acc: 0.9375 - val_loss: 0.9723 - val_acc: 0.7533\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.2059 - acc: 0.9400 - val_loss: 0.9932 - val_acc: 0.7400\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 507us/step - loss: 2.1628 - acc: 0.2567 - val_loss: 1.8776 - val_acc: 0.3833\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 1.5533 - acc: 0.4800 - val_loss: 1.1976 - val_acc: 0.6433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 0.9133 - acc: 0.7167 - val_loss: 0.8575 - val_acc: 0.7333\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.5773 - acc: 0.8392 - val_loss: 0.6680 - val_acc: 0.8133\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 0.4187 - acc: 0.8858 - val_loss: 0.6221 - val_acc: 0.8233\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 94us/step - loss: 0.3014 - acc: 0.9250 - val_loss: 0.5592 - val_acc: 0.8500\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 0.2311 - acc: 0.9375 - val_loss: 0.5747 - val_acc: 0.8433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 0.1915 - acc: 0.9525 - val_loss: 0.5840 - val_acc: 0.8600\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 0.1537 - acc: 0.9608 - val_loss: 0.5832 - val_acc: 0.8600\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 0.1140 - acc: 0.9742 - val_loss: 0.6089 - val_acc: 0.8700\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 0.0919 - acc: 0.9825 - val_loss: 0.6315 - val_acc: 0.8533\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.0708 - acc: 0.9842 - val_loss: 0.6102 - val_acc: 0.8667\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 0.0526 - acc: 0.9925 - val_loss: 0.6210 - val_acc: 0.8567\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 0.0418 - acc: 0.9942 - val_loss: 0.6683 - val_acc: 0.8533\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.0327 - acc: 0.9958 - val_loss: 0.7082 - val_acc: 0.8600\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 0.0229 - acc: 0.9975 - val_loss: 0.6709 - val_acc: 0.8633\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 0.0185 - acc: 0.9983 - val_loss: 0.7226 - val_acc: 0.8633\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 0.0142 - acc: 0.9992 - val_loss: 0.7183 - val_acc: 0.8700\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 0.0119 - acc: 0.9992 - val_loss: 0.7318 - val_acc: 0.8667\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 0.7509 - val_acc: 0.8667\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.0083 - acc: 0.9992 - val_loss: 0.7779 - val_acc: 0.8633\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.8010 - val_acc: 0.8633\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.8055 - val_acc: 0.8633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.8325 - val_acc: 0.8633\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.8012 - val_acc: 0.8633\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.8203 - val_acc: 0.8600\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.8254 - val_acc: 0.8633\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 94us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.8386 - val_acc: 0.8633\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.8643 - val_acc: 0.8633\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.8414 - val_acc: 0.8633\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 529us/step - loss: 2.3312 - acc: 0.1075 - val_loss: 2.3086 - val_acc: 0.0833\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 2.3212 - acc: 0.1075 - val_loss: 2.3043 - val_acc: 0.0833\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.3145 - acc: 0.1075 - val_loss: 2.3007 - val_acc: 0.0833\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 2.3094 - acc: 0.1075 - val_loss: 2.2988 - val_acc: 0.0833\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.3060 - acc: 0.1075 - val_loss: 2.2976 - val_acc: 0.0833\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.3033 - acc: 0.1075 - val_loss: 2.2972 - val_acc: 0.0833\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 2.3014 - acc: 0.1083 - val_loss: 2.2967 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 2.2999 - acc: 0.1192 - val_loss: 2.2969 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.2966 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 2.2978 - acc: 0.1192 - val_loss: 2.2966 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 49us/step - loss: 2.2970 - acc: 0.1192 - val_loss: 2.2967 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 2.2963 - acc: 0.1192 - val_loss: 2.2967 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.2955 - acc: 0.1192 - val_loss: 2.2963 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 2.2947 - acc: 0.1192 - val_loss: 2.2956 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 2.2937 - acc: 0.1192 - val_loss: 2.2947 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.2928 - acc: 0.1192 - val_loss: 2.2945 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.2917 - acc: 0.1192 - val_loss: 2.2935 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 2.2901 - acc: 0.1192 - val_loss: 2.2921 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 2.2885 - acc: 0.1192 - val_loss: 2.2905 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.2863 - acc: 0.1192 - val_loss: 2.2882 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.2836 - acc: 0.1192 - val_loss: 2.2857 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.2803 - acc: 0.1192 - val_loss: 2.2825 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.2764 - acc: 0.1192 - val_loss: 2.2783 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 2.2715 - acc: 0.1192 - val_loss: 2.2741 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.2656 - acc: 0.1192 - val_loss: 2.2680 - val_acc: 0.1433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 2.2585 - acc: 0.1192 - val_loss: 2.2612 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.2500 - acc: 0.1192 - val_loss: 2.2530 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 2.2399 - acc: 0.1192 - val_loss: 2.2425 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 2.2282 - acc: 0.1350 - val_loss: 2.2323 - val_acc: 0.2200\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 2.2146 - acc: 0.1942 - val_loss: 2.2201 - val_acc: 0.2233\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 462us/step - loss: 2.3534 - acc: 0.1017 - val_loss: 2.3258 - val_acc: 0.1167\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.3311 - acc: 0.1017 - val_loss: 2.3146 - val_acc: 0.1167\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 2.3179 - acc: 0.1017 - val_loss: 2.3070 - val_acc: 0.1167\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.3101 - acc: 0.1017 - val_loss: 2.3029 - val_acc: 0.1167\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.3048 - acc: 0.1017 - val_loss: 2.3004 - val_acc: 0.1167\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.3018 - acc: 0.1042 - val_loss: 2.2994 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.3001 - acc: 0.1192 - val_loss: 2.3000 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.2984 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.2973 - acc: 0.1192 - val_loss: 2.2984 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.2965 - acc: 0.1192 - val_loss: 2.2988 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.2956 - acc: 0.1192 - val_loss: 2.2979 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 2.2946 - acc: 0.1192 - val_loss: 2.2969 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 2.2935 - acc: 0.1192 - val_loss: 2.2961 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 2.2921 - acc: 0.1192 - val_loss: 2.2958 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 2.2903 - acc: 0.1192 - val_loss: 2.2934 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.2881 - acc: 0.1192 - val_loss: 2.2913 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.2850 - acc: 0.1192 - val_loss: 2.2888 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 2.2812 - acc: 0.1192 - val_loss: 2.2842 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.2759 - acc: 0.1192 - val_loss: 2.2786 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 2.2689 - acc: 0.1192 - val_loss: 2.2715 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.2607 - acc: 0.1192 - val_loss: 2.2650 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 2.2488 - acc: 0.1192 - val_loss: 2.2546 - val_acc: 0.1433\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 46us/step - loss: 2.2355 - acc: 0.1192 - val_loss: 2.2427 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 2.2199 - acc: 0.1192 - val_loss: 2.2303 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 2.2016 - acc: 0.1367 - val_loss: 2.2154 - val_acc: 0.1900\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.1829 - acc: 0.1892 - val_loss: 2.2001 - val_acc: 0.1133\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 2.1616 - acc: 0.1842 - val_loss: 2.1840 - val_acc: 0.1167\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 2.1402 - acc: 0.1850 - val_loss: 2.1676 - val_acc: 0.1167\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 2.1179 - acc: 0.1950 - val_loss: 2.1520 - val_acc: 0.1100\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 47us/step - loss: 2.0967 - acc: 0.1950 - val_loss: 2.1362 - val_acc: 0.1200\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 527us/step - loss: 2.4407 - acc: 0.1192 - val_loss: 2.3267 - val_acc: 0.1433\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 2.3743 - acc: 0.1192 - val_loss: 2.3030 - val_acc: 0.1433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 50us/step - loss: 2.3393 - acc: 0.1192 - val_loss: 2.2917 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 2.3194 - acc: 0.1192 - val_loss: 2.2885 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.3074 - acc: 0.1192 - val_loss: 2.2866 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.2989 - acc: 0.1192 - val_loss: 2.2855 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2920 - acc: 0.1192 - val_loss: 2.2833 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2846 - acc: 0.1192 - val_loss: 2.2791 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2753 - acc: 0.1192 - val_loss: 2.2714 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2622 - acc: 0.1192 - val_loss: 2.2597 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 2.2445 - acc: 0.1242 - val_loss: 2.2423 - val_acc: 0.1733\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 2.2201 - acc: 0.1883 - val_loss: 2.2188 - val_acc: 0.1567\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 2.1879 - acc: 0.1775 - val_loss: 2.1864 - val_acc: 0.1567\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 50us/step - loss: 2.1481 - acc: 0.1950 - val_loss: 2.1519 - val_acc: 0.1467\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.1030 - acc: 0.2033 - val_loss: 2.1124 - val_acc: 0.1867\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.0550 - acc: 0.2142 - val_loss: 2.0723 - val_acc: 0.1900\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 2.0125 - acc: 0.1975 - val_loss: 2.0450 - val_acc: 0.1967\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 1.9739 - acc: 0.2133 - val_loss: 2.0124 - val_acc: 0.1933\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 1.9457 - acc: 0.2142 - val_loss: 1.9902 - val_acc: 0.1933\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 48us/step - loss: 1.9188 - acc: 0.2108 - val_loss: 1.9671 - val_acc: 0.2033\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 50us/step - loss: 1.8918 - acc: 0.2250 - val_loss: 1.9425 - val_acc: 0.2100\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 46us/step - loss: 1.8689 - acc: 0.2308 - val_loss: 1.9322 - val_acc: 0.2200\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 1.8482 - acc: 0.2375 - val_loss: 1.9091 - val_acc: 0.2200\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 50us/step - loss: 1.8320 - acc: 0.2475 - val_loss: 1.8946 - val_acc: 0.2367\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 1.8120 - acc: 0.2658 - val_loss: 1.8755 - val_acc: 0.2567\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 1.7984 - acc: 0.2817 - val_loss: 1.8664 - val_acc: 0.2500\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 1.7833 - acc: 0.2892 - val_loss: 1.8497 - val_acc: 0.2633\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 52us/step - loss: 1.7690 - acc: 0.3000 - val_loss: 1.8422 - val_acc: 0.2767\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 1.7566 - acc: 0.2950 - val_loss: 1.8321 - val_acc: 0.2767\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 51us/step - loss: 1.7465 - acc: 0.2933 - val_loss: 1.8245 - val_acc: 0.2800\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 480us/step - loss: 2.3078 - acc: 0.1183 - val_loss: 2.3004 - val_acc: 0.1433\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.2983 - val_acc: 0.1433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2979 - acc: 0.1208 - val_loss: 2.2961 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2892 - acc: 0.1192 - val_loss: 2.2817 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 2.2747 - acc: 0.1242 - val_loss: 2.2627 - val_acc: 0.2200\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 2.2423 - acc: 0.2175 - val_loss: 2.2157 - val_acc: 0.2433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.1768 - acc: 0.2617 - val_loss: 2.1346 - val_acc: 0.2567\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.0755 - acc: 0.2933 - val_loss: 2.0228 - val_acc: 0.2600\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.9493 - acc: 0.3083 - val_loss: 1.9042 - val_acc: 0.2700\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.8073 - acc: 0.3367 - val_loss: 1.7866 - val_acc: 0.3500\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 1.6711 - acc: 0.3958 - val_loss: 1.6793 - val_acc: 0.3933\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 1.5455 - acc: 0.4592 - val_loss: 1.5851 - val_acc: 0.4333\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 1.4378 - acc: 0.4892 - val_loss: 1.5018 - val_acc: 0.4667\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 1.3520 - acc: 0.5117 - val_loss: 1.4387 - val_acc: 0.4800\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 1.2735 - acc: 0.5892 - val_loss: 1.3826 - val_acc: 0.5033\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 1.2108 - acc: 0.6275 - val_loss: 1.3374 - val_acc: 0.5300\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.1553 - acc: 0.6583 - val_loss: 1.3059 - val_acc: 0.5333\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.1041 - acc: 0.6792 - val_loss: 1.2625 - val_acc: 0.5500\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 1.0591 - acc: 0.7067 - val_loss: 1.2364 - val_acc: 0.5767\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.0178 - acc: 0.6967 - val_loss: 1.2047 - val_acc: 0.6000\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 0.9748 - acc: 0.7508 - val_loss: 1.1765 - val_acc: 0.6433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.9382 - acc: 0.7392 - val_loss: 1.1556 - val_acc: 0.6500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 0.9015 - acc: 0.7458 - val_loss: 1.1351 - val_acc: 0.6333\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.8738 - acc: 0.7550 - val_loss: 1.1176 - val_acc: 0.6700\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 0.8392 - acc: 0.7983 - val_loss: 1.0973 - val_acc: 0.6867\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 0.8061 - acc: 0.8058 - val_loss: 1.0961 - val_acc: 0.6633\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 0.7748 - acc: 0.8125 - val_loss: 1.0672 - val_acc: 0.6767\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 0.7449 - acc: 0.8192 - val_loss: 1.0649 - val_acc: 0.6900\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.7183 - acc: 0.8575 - val_loss: 1.0619 - val_acc: 0.6900\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 0.6915 - acc: 0.8333 - val_loss: 1.0380 - val_acc: 0.6800\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 593us/step - loss: 2.3002 - acc: 0.1233 - val_loss: 2.2942 - val_acc: 0.1433\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2783 - acc: 0.1925 - val_loss: 2.2613 - val_acc: 0.1800\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.2280 - acc: 0.1917 - val_loss: 2.1977 - val_acc: 0.1933\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.1439 - acc: 0.1967 - val_loss: 2.1058 - val_acc: 0.1967\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.0284 - acc: 0.2050 - val_loss: 1.9889 - val_acc: 0.2000\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.8981 - acc: 0.2317 - val_loss: 1.8731 - val_acc: 0.2633\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.7727 - acc: 0.2775 - val_loss: 1.7665 - val_acc: 0.2700\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.6784 - acc: 0.2900 - val_loss: 1.6754 - val_acc: 0.3000\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.6084 - acc: 0.2967 - val_loss: 1.6535 - val_acc: 0.2867\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.5550 - acc: 0.3025 - val_loss: 1.5874 - val_acc: 0.3267\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.5131 - acc: 0.3175 - val_loss: 1.5600 - val_acc: 0.3267\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.4701 - acc: 0.3358 - val_loss: 1.5307 - val_acc: 0.3267\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.4349 - acc: 0.3483 - val_loss: 1.4976 - val_acc: 0.3367\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.3981 - acc: 0.3675 - val_loss: 1.4732 - val_acc: 0.3600\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.3697 - acc: 0.3775 - val_loss: 1.4569 - val_acc: 0.3600\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.3464 - acc: 0.3800 - val_loss: 1.4346 - val_acc: 0.3567\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.3174 - acc: 0.3967 - val_loss: 1.4210 - val_acc: 0.3833\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.2921 - acc: 0.4217 - val_loss: 1.4209 - val_acc: 0.4200\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.2763 - acc: 0.4558 - val_loss: 1.4131 - val_acc: 0.4300\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 1.2535 - acc: 0.4650 - val_loss: 1.3799 - val_acc: 0.4200\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.2391 - acc: 0.4850 - val_loss: 1.3760 - val_acc: 0.4667\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.2205 - acc: 0.4933 - val_loss: 1.3517 - val_acc: 0.4567\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.2076 - acc: 0.5067 - val_loss: 1.3406 - val_acc: 0.4767\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 1.1884 - acc: 0.4942 - val_loss: 1.3377 - val_acc: 0.4567\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 1.1709 - acc: 0.5042 - val_loss: 1.3328 - val_acc: 0.4733\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.1570 - acc: 0.5258 - val_loss: 1.3233 - val_acc: 0.4833\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.1445 - acc: 0.5225 - val_loss: 1.3255 - val_acc: 0.4933\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 1.1350 - acc: 0.5142 - val_loss: 1.3170 - val_acc: 0.4933\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.1276 - acc: 0.5367 - val_loss: 1.3026 - val_acc: 0.5000\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.1122 - acc: 0.5442 - val_loss: 1.2976 - val_acc: 0.5333\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 560us/step - loss: 2.2765 - acc: 0.1533 - val_loss: 2.2505 - val_acc: 0.1533\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.1818 - acc: 0.1750 - val_loss: 2.1449 - val_acc: 0.1633\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.0776 - acc: 0.1992 - val_loss: 2.0575 - val_acc: 0.2033\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.9976 - acc: 0.2508 - val_loss: 1.9771 - val_acc: 0.2933\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.9194 - acc: 0.2942 - val_loss: 1.9051 - val_acc: 0.3167\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.8559 - acc: 0.3192 - val_loss: 1.8507 - val_acc: 0.3300\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.7984 - acc: 0.3367 - val_loss: 1.8074 - val_acc: 0.3567\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.7511 - acc: 0.3400 - val_loss: 1.7725 - val_acc: 0.3467\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.6978 - acc: 0.3525 - val_loss: 1.7561 - val_acc: 0.3533\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.6467 - acc: 0.3633 - val_loss: 1.6808 - val_acc: 0.3833\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.6003 - acc: 0.3692 - val_loss: 1.6334 - val_acc: 0.3933\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.5509 - acc: 0.3842 - val_loss: 1.5820 - val_acc: 0.4033\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.5022 - acc: 0.3883 - val_loss: 1.5487 - val_acc: 0.4000\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.4679 - acc: 0.3967 - val_loss: 1.5261 - val_acc: 0.4133\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 1.4336 - acc: 0.4158 - val_loss: 1.4885 - val_acc: 0.4233\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 1.3889 - acc: 0.4383 - val_loss: 1.4675 - val_acc: 0.4333\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.3589 - acc: 0.4625 - val_loss: 1.4335 - val_acc: 0.4600\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 1.3151 - acc: 0.4983 - val_loss: 1.4228 - val_acc: 0.4633\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.2836 - acc: 0.5033 - val_loss: 1.4116 - val_acc: 0.4767\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 1.2468 - acc: 0.5392 - val_loss: 1.3305 - val_acc: 0.5433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.1842 - acc: 0.5767 - val_loss: 1.3171 - val_acc: 0.5433\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.1406 - acc: 0.5967 - val_loss: 1.2948 - val_acc: 0.5433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 1.1002 - acc: 0.5933 - val_loss: 1.2600 - val_acc: 0.5767\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.0568 - acc: 0.6192 - val_loss: 1.2173 - val_acc: 0.5733\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 1.0176 - acc: 0.6250 - val_loss: 1.1884 - val_acc: 0.5833\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.9745 - acc: 0.6375 - val_loss: 1.1803 - val_acc: 0.5933\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 0.9295 - acc: 0.6425 - val_loss: 1.1543 - val_acc: 0.5867\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 0.8903 - acc: 0.6633 - val_loss: 1.1777 - val_acc: 0.6000\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.8577 - acc: 0.6767 - val_loss: 1.1277 - val_acc: 0.6067\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 0.8223 - acc: 0.6933 - val_loss: 1.1250 - val_acc: 0.6233\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 594us/step - loss: 2.2293 - acc: 0.1400 - val_loss: 2.1258 - val_acc: 0.1867\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.9738 - acc: 0.2325 - val_loss: 1.9072 - val_acc: 0.3167\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 1.7080 - acc: 0.4150 - val_loss: 1.6639 - val_acc: 0.4700\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 1.3955 - acc: 0.5575 - val_loss: 1.3799 - val_acc: 0.5333\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 1.1590 - acc: 0.6258 - val_loss: 1.2108 - val_acc: 0.5933\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 0.9761 - acc: 0.6867 - val_loss: 1.0740 - val_acc: 0.6033\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 0.8460 - acc: 0.7308 - val_loss: 1.0082 - val_acc: 0.6633\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 0.7630 - acc: 0.7567 - val_loss: 0.9272 - val_acc: 0.6667\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.6903 - acc: 0.7817 - val_loss: 0.9326 - val_acc: 0.6933\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 0.6466 - acc: 0.7842 - val_loss: 0.9945 - val_acc: 0.6800\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 0.6163 - acc: 0.8092 - val_loss: 0.8950 - val_acc: 0.7033\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 0.5659 - acc: 0.8283 - val_loss: 0.9493 - val_acc: 0.7133\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 0.5408 - acc: 0.8408 - val_loss: 0.8901 - val_acc: 0.7367\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 0.5020 - acc: 0.8467 - val_loss: 0.9058 - val_acc: 0.7500\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 0.4767 - acc: 0.8583 - val_loss: 0.9280 - val_acc: 0.7433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 0.4404 - acc: 0.8658 - val_loss: 0.9072 - val_acc: 0.7433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 0.4239 - acc: 0.8767 - val_loss: 0.8943 - val_acc: 0.7667\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.3960 - acc: 0.8783 - val_loss: 0.9850 - val_acc: 0.7567\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 0.3652 - acc: 0.8858 - val_loss: 0.9315 - val_acc: 0.7833\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 0.3450 - acc: 0.8992 - val_loss: 0.9146 - val_acc: 0.7867\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 0.3134 - acc: 0.9083 - val_loss: 0.9197 - val_acc: 0.7933\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 0.2864 - acc: 0.9192 - val_loss: 0.9569 - val_acc: 0.7667\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 0.2785 - acc: 0.9167 - val_loss: 0.9892 - val_acc: 0.7700\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 0.2547 - acc: 0.9258 - val_loss: 0.9534 - val_acc: 0.7967\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 0.2361 - acc: 0.9333 - val_loss: 0.9724 - val_acc: 0.8100\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 0.2123 - acc: 0.9450 - val_loss: 1.0188 - val_acc: 0.7833\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.1953 - acc: 0.9517 - val_loss: 1.0052 - val_acc: 0.7967\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.1816 - acc: 0.9542 - val_loss: 1.0138 - val_acc: 0.8000\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.1839 - acc: 0.9425 - val_loss: 1.0805 - val_acc: 0.7967\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 0.1588 - acc: 0.9558 - val_loss: 1.0263 - val_acc: 0.8167\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 602us/step - loss: 2.0440 - acc: 0.2650 - val_loss: 1.5495 - val_acc: 0.4167\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.1707 - acc: 0.6125 - val_loss: 0.8898 - val_acc: 0.7233\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 0.7176 - acc: 0.7817 - val_loss: 0.6605 - val_acc: 0.7733\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 0.5191 - acc: 0.8358 - val_loss: 0.6624 - val_acc: 0.7933\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.4460 - acc: 0.8533 - val_loss: 0.6134 - val_acc: 0.8067\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 0.3518 - acc: 0.8825 - val_loss: 0.5443 - val_acc: 0.8367\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 0.2916 - acc: 0.9167 - val_loss: 0.5847 - val_acc: 0.8267\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 0.2398 - acc: 0.9258 - val_loss: 0.5554 - val_acc: 0.8500\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.2058 - acc: 0.9342 - val_loss: 0.5520 - val_acc: 0.8467\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.1765 - acc: 0.9575 - val_loss: 0.5852 - val_acc: 0.8500\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.1445 - acc: 0.9642 - val_loss: 0.5589 - val_acc: 0.8400\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 0.1120 - acc: 0.9742 - val_loss: 0.5661 - val_acc: 0.8567\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.0948 - acc: 0.9717 - val_loss: 0.5815 - val_acc: 0.8633\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.0772 - acc: 0.9850 - val_loss: 0.6439 - val_acc: 0.8400\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 0.0593 - acc: 0.9900 - val_loss: 0.6109 - val_acc: 0.8567\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 0.0480 - acc: 0.9883 - val_loss: 0.6561 - val_acc: 0.8333\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 0.0596 - acc: 0.9850 - val_loss: 0.6663 - val_acc: 0.8567\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.0331 - acc: 0.9950 - val_loss: 0.6937 - val_acc: 0.8533\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.0245 - acc: 0.9983 - val_loss: 0.7042 - val_acc: 0.8567\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.0168 - acc: 0.9983 - val_loss: 0.7430 - val_acc: 0.8467\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 0.7763 - val_acc: 0.8433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.0113 - acc: 1.0000 - val_loss: 0.7579 - val_acc: 0.8567\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 0.7759 - val_acc: 0.8467\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.8161 - val_acc: 0.8533\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.7962 - val_acc: 0.8500\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.8305 - val_acc: 0.8467\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.8562 - val_acc: 0.8500\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.8561 - val_acc: 0.8500\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.8568 - val_acc: 0.8500\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.8642 - val_acc: 0.8567\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 549us/step - loss: 2.3026 - acc: 0.1042 - val_loss: 2.3023 - val_acc: 0.1433\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.3018 - acc: 0.1192 - val_loss: 2.3019 - val_acc: 0.1433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 2.3012 - acc: 0.1192 - val_loss: 2.3018 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.3008 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.3004 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.3001 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2998 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 2.2995 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 2.2994 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 2.2991 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 2.2990 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2989 - acc: 0.1192 - val_loss: 2.3009 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 2.2988 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3008 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3009 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2979 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 592us/step - loss: 2.2968 - acc: 0.1117 - val_loss: 2.2879 - val_acc: 0.1433\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 120us/step - loss: 2.2684 - acc: 0.1267 - val_loss: 2.2420 - val_acc: 0.1133\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 2.2084 - acc: 0.1450 - val_loss: 2.1753 - val_acc: 0.1367\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 2.1464 - acc: 0.1600 - val_loss: 2.1151 - val_acc: 0.1800\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 2.0864 - acc: 0.1758 - val_loss: 2.0609 - val_acc: 0.1967\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 2.0371 - acc: 0.1808 - val_loss: 2.0204 - val_acc: 0.1900\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.9939 - acc: 0.1850 - val_loss: 1.9848 - val_acc: 0.1900\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.9527 - acc: 0.1925 - val_loss: 1.9534 - val_acc: 0.2700\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 1.9138 - acc: 0.2125 - val_loss: 1.9200 - val_acc: 0.2467\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.8805 - acc: 0.2158 - val_loss: 1.8897 - val_acc: 0.2567\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 95us/step - loss: 1.8414 - acc: 0.2083 - val_loss: 1.8665 - val_acc: 0.2600\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 1.8071 - acc: 0.2308 - val_loss: 1.8433 - val_acc: 0.2533\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.7778 - acc: 0.2400 - val_loss: 1.8282 - val_acc: 0.2533\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.7443 - acc: 0.2800 - val_loss: 1.8024 - val_acc: 0.3133\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.7105 - acc: 0.3008 - val_loss: 1.7937 - val_acc: 0.2867\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.6795 - acc: 0.2825 - val_loss: 1.7768 - val_acc: 0.2900\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 1.6471 - acc: 0.3192 - val_loss: 1.7520 - val_acc: 0.2733\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.6099 - acc: 0.3333 - val_loss: 1.7272 - val_acc: 0.3333\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.5760 - acc: 0.3600 - val_loss: 1.6834 - val_acc: 0.3133\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.5312 - acc: 0.3550 - val_loss: 1.6512 - val_acc: 0.3567\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 96us/step - loss: 1.4887 - acc: 0.3758 - val_loss: 1.6179 - val_acc: 0.3467\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.4353 - acc: 0.3742 - val_loss: 1.5887 - val_acc: 0.4200\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.3932 - acc: 0.4633 - val_loss: 1.5663 - val_acc: 0.4200\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.3531 - acc: 0.4833 - val_loss: 1.5486 - val_acc: 0.4200\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 1.3125 - acc: 0.4817 - val_loss: 1.5383 - val_acc: 0.3833\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 1.2820 - acc: 0.4858 - val_loss: 1.5184 - val_acc: 0.4467\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 1.2437 - acc: 0.5158 - val_loss: 1.5160 - val_acc: 0.4367\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 1.2253 - acc: 0.5083 - val_loss: 1.5170 - val_acc: 0.4533\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.1907 - acc: 0.5358 - val_loss: 1.5098 - val_acc: 0.4133\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 1.1561 - acc: 0.5358 - val_loss: 1.5084 - val_acc: 0.4500\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 593us/step - loss: 2.2540 - acc: 0.1917 - val_loss: 2.1823 - val_acc: 0.2200\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 2.1018 - acc: 0.2042 - val_loss: 2.0309 - val_acc: 0.2500\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 1.9305 - acc: 0.2533 - val_loss: 1.8163 - val_acc: 0.3500\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.7082 - acc: 0.3792 - val_loss: 1.6748 - val_acc: 0.3967\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.4901 - acc: 0.4658 - val_loss: 1.4980 - val_acc: 0.4567\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.3179 - acc: 0.5483 - val_loss: 1.4436 - val_acc: 0.5367\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 95us/step - loss: 1.1573 - acc: 0.5992 - val_loss: 1.2915 - val_acc: 0.5800\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.0428 - acc: 0.6450 - val_loss: 1.1883 - val_acc: 0.5800\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 0.9553 - acc: 0.6650 - val_loss: 1.0766 - val_acc: 0.5967\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 0.8675 - acc: 0.6942 - val_loss: 1.0747 - val_acc: 0.6367\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.8011 - acc: 0.7408 - val_loss: 1.0241 - val_acc: 0.6633\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 0.7500 - acc: 0.7500 - val_loss: 0.9517 - val_acc: 0.6800\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 0.6958 - acc: 0.7700 - val_loss: 0.9606 - val_acc: 0.6800\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 0.6461 - acc: 0.7975 - val_loss: 0.9570 - val_acc: 0.6667\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 0.6136 - acc: 0.8075 - val_loss: 0.9100 - val_acc: 0.7000\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 0.5710 - acc: 0.8142 - val_loss: 0.9181 - val_acc: 0.6800\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 0.5546 - acc: 0.8175 - val_loss: 0.8857 - val_acc: 0.7267\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 0.5072 - acc: 0.8358 - val_loss: 0.9073 - val_acc: 0.7267\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 0.4898 - acc: 0.8425 - val_loss: 0.8806 - val_acc: 0.7400\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 0.4876 - acc: 0.8375 - val_loss: 0.8818 - val_acc: 0.7267\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 0.4463 - acc: 0.8558 - val_loss: 0.8870 - val_acc: 0.7433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 0.4257 - acc: 0.8558 - val_loss: 0.8759 - val_acc: 0.7300\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.4285 - acc: 0.8683 - val_loss: 0.8645 - val_acc: 0.7500\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 0.3929 - acc: 0.8833 - val_loss: 0.8460 - val_acc: 0.7433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.3660 - acc: 0.8867 - val_loss: 0.8859 - val_acc: 0.7400\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.3551 - acc: 0.8917 - val_loss: 0.8603 - val_acc: 0.7567\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 0.3273 - acc: 0.9083 - val_loss: 0.9803 - val_acc: 0.7267\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 95us/step - loss: 0.3200 - acc: 0.9017 - val_loss: 0.9004 - val_acc: 0.7567\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 0.2943 - acc: 0.9158 - val_loss: 0.8809 - val_acc: 0.7800\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 106us/step - loss: 0.2788 - acc: 0.9175 - val_loss: 0.9008 - val_acc: 0.7767\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 565us/step - loss: 2.1482 - acc: 0.1867 - val_loss: 1.8180 - val_acc: 0.3767\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 116us/step - loss: 1.4902 - acc: 0.4692 - val_loss: 1.1557 - val_acc: 0.6100\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 136us/step - loss: 0.9241 - acc: 0.6733 - val_loss: 0.8526 - val_acc: 0.6800\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 106us/step - loss: 0.6342 - acc: 0.7942 - val_loss: 0.7116 - val_acc: 0.7667\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 0.4781 - acc: 0.8450 - val_loss: 0.7126 - val_acc: 0.7967\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.4000 - acc: 0.8758 - val_loss: 0.6349 - val_acc: 0.8100\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 142us/step - loss: 0.3130 - acc: 0.9117 - val_loss: 0.7404 - val_acc: 0.7833\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 123us/step - loss: 0.2672 - acc: 0.9250 - val_loss: 0.6438 - val_acc: 0.8133\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 0.2088 - acc: 0.9467 - val_loss: 0.6487 - val_acc: 0.8333\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 0.1811 - acc: 0.9483 - val_loss: 0.6969 - val_acc: 0.8500\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 0.1457 - acc: 0.9608 - val_loss: 0.6780 - val_acc: 0.8433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 0.1140 - acc: 0.9650 - val_loss: 0.7381 - val_acc: 0.8367\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 0.0922 - acc: 0.9758 - val_loss: 0.7581 - val_acc: 0.8367\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 121us/step - loss: 0.0996 - acc: 0.9717 - val_loss: 0.7289 - val_acc: 0.8333\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.0703 - acc: 0.9833 - val_loss: 0.7658 - val_acc: 0.8367\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.0585 - acc: 0.9833 - val_loss: 0.8319 - val_acc: 0.8333\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 0.0392 - acc: 0.9942 - val_loss: 0.8130 - val_acc: 0.8400\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 162us/step - loss: 0.0346 - acc: 0.9950 - val_loss: 0.7545 - val_acc: 0.8533\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.0226 - acc: 0.9975 - val_loss: 0.8300 - val_acc: 0.8533\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 101us/step - loss: 0.0201 - acc: 0.9958 - val_loss: 0.8464 - val_acc: 0.8500\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.8614 - val_acc: 0.8600\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 153us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.8450 - val_acc: 0.8533\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.8652 - val_acc: 0.8567\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 128us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.9137 - val_acc: 0.8567\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 121us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.9089 - val_acc: 0.8533\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.9368 - val_acc: 0.8533\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 148us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.9384 - val_acc: 0.8533\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 123us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.9439 - val_acc: 0.8533\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 151us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.9605 - val_acc: 0.8533\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.9818 - val_acc: 0.8533\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 628us/step - loss: 2.3553 - acc: 0.1017 - val_loss: 2.3163 - val_acc: 0.1167\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.3384 - acc: 0.1100 - val_loss: 2.3087 - val_acc: 0.1433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.3271 - acc: 0.1192 - val_loss: 2.3043 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.3190 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.3133 - acc: 0.1192 - val_loss: 2.2995 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.3094 - acc: 0.1192 - val_loss: 2.2985 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.3062 - acc: 0.1192 - val_loss: 2.2976 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.3040 - acc: 0.1192 - val_loss: 2.2979 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 2.3024 - acc: 0.1192 - val_loss: 2.2981 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.3013 - acc: 0.1192 - val_loss: 2.2981 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.3003 - acc: 0.1192 - val_loss: 2.2983 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2995 - acc: 0.1192 - val_loss: 2.2985 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2990 - acc: 0.1192 - val_loss: 2.2987 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.2987 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.2992 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.2994 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2978 - acc: 0.1192 - val_loss: 2.2994 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2976 - acc: 0.1192 - val_loss: 2.2996 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2976 - acc: 0.1192 - val_loss: 2.2996 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2974 - acc: 0.1192 - val_loss: 2.2997 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2971 - acc: 0.1192 - val_loss: 2.3001 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2967 - acc: 0.1192 - val_loss: 2.2995 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2966 - acc: 0.1192 - val_loss: 2.2994 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.2964 - acc: 0.1192 - val_loss: 2.2986 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2959 - acc: 0.1192 - val_loss: 2.2986 - val_acc: 0.1433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2954 - acc: 0.1192 - val_loss: 2.2983 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2947 - acc: 0.1192 - val_loss: 2.2972 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2940 - acc: 0.1192 - val_loss: 2.2966 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2932 - acc: 0.1192 - val_loss: 2.2952 - val_acc: 0.1433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2916 - acc: 0.1192 - val_loss: 2.2935 - val_acc: 0.1433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 506us/step - loss: 2.3356 - acc: 0.1017 - val_loss: 2.3308 - val_acc: 0.1167\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.3203 - acc: 0.1017 - val_loss: 2.3198 - val_acc: 0.1167\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 2.3112 - acc: 0.1017 - val_loss: 2.3126 - val_acc: 0.1167\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.3057 - acc: 0.1017 - val_loss: 2.3083 - val_acc: 0.1167\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.3025 - acc: 0.1017 - val_loss: 2.3058 - val_acc: 0.1167\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.3008 - acc: 0.1017 - val_loss: 2.3037 - val_acc: 0.1167\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2999 - acc: 0.0967 - val_loss: 2.3032 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.2989 - acc: 0.1192 - val_loss: 2.3022 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3017 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2979 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 2.2977 - acc: 0.1192 - val_loss: 2.3006 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.2973 - acc: 0.1192 - val_loss: 2.3009 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.2970 - acc: 0.1192 - val_loss: 2.3003 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.2969 - acc: 0.1192 - val_loss: 2.3002 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2965 - acc: 0.1192 - val_loss: 2.3000 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 2.2965 - acc: 0.1192 - val_loss: 2.2987 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2957 - acc: 0.1192 - val_loss: 2.2987 - val_acc: 0.1433\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2948 - acc: 0.1192 - val_loss: 2.2979 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2938 - acc: 0.1192 - val_loss: 2.2970 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2929 - acc: 0.1192 - val_loss: 2.2954 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2911 - acc: 0.1192 - val_loss: 2.2937 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.2889 - acc: 0.1192 - val_loss: 2.2915 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.2860 - acc: 0.1192 - val_loss: 2.2879 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2817 - acc: 0.1192 - val_loss: 2.2841 - val_acc: 0.1433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2764 - acc: 0.1192 - val_loss: 2.2780 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2695 - acc: 0.1192 - val_loss: 2.2711 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2616 - acc: 0.1875 - val_loss: 2.2633 - val_acc: 0.2200\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2511 - acc: 0.2158 - val_loss: 2.2533 - val_acc: 0.2200\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2404 - acc: 0.2183 - val_loss: 2.2415 - val_acc: 0.2200\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 572us/step - loss: 2.3284 - acc: 0.1033 - val_loss: 2.3281 - val_acc: 0.0933\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.3116 - acc: 0.1033 - val_loss: 2.3151 - val_acc: 0.1433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.3043 - acc: 0.1192 - val_loss: 2.3091 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.3009 - acc: 0.1192 - val_loss: 2.3049 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 2.2990 - acc: 0.1192 - val_loss: 2.3027 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3027 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.2972 - acc: 0.1192 - val_loss: 2.2997 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2964 - acc: 0.1192 - val_loss: 2.2985 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2955 - acc: 0.1192 - val_loss: 2.2968 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 2.2935 - acc: 0.1192 - val_loss: 2.2940 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.2890 - acc: 0.1192 - val_loss: 2.2898 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.2832 - acc: 0.1192 - val_loss: 2.2828 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.2734 - acc: 0.1192 - val_loss: 2.2711 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 2.2593 - acc: 0.1192 - val_loss: 2.2548 - val_acc: 0.1633\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.2376 - acc: 0.1667 - val_loss: 2.2305 - val_acc: 0.2133\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 2.2101 - acc: 0.1925 - val_loss: 2.2011 - val_acc: 0.1933\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 2.1740 - acc: 0.1900 - val_loss: 2.1646 - val_acc: 0.1967\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.1332 - acc: 0.1925 - val_loss: 2.1253 - val_acc: 0.2000\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 2.0922 - acc: 0.1908 - val_loss: 2.0862 - val_acc: 0.2233\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 2.0483 - acc: 0.2008 - val_loss: 2.0504 - val_acc: 0.2267\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 2.0118 - acc: 0.2008 - val_loss: 2.0267 - val_acc: 0.2167\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 1.9741 - acc: 0.1867 - val_loss: 1.9959 - val_acc: 0.1933\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 1.9455 - acc: 0.1975 - val_loss: 1.9813 - val_acc: 0.2133\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 1.9186 - acc: 0.2083 - val_loss: 1.9753 - val_acc: 0.1967\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 55us/step - loss: 1.8966 - acc: 0.2042 - val_loss: 1.9501 - val_acc: 0.1933\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 1.8776 - acc: 0.1950 - val_loss: 1.9406 - val_acc: 0.2033\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 1.8671 - acc: 0.2100 - val_loss: 1.9310 - val_acc: 0.1933\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 54us/step - loss: 1.8513 - acc: 0.2008 - val_loss: 1.9038 - val_acc: 0.2033\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 1.8377 - acc: 0.1975 - val_loss: 1.9139 - val_acc: 0.2033\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 53us/step - loss: 1.8204 - acc: 0.2017 - val_loss: 1.9084 - val_acc: 0.2067\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 574us/step - loss: 2.4322 - acc: 0.1192 - val_loss: 2.3414 - val_acc: 0.1433\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 2.3214 - acc: 0.1192 - val_loss: 2.3062 - val_acc: 0.1433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.3026 - acc: 0.1192 - val_loss: 2.3016 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.3001 - acc: 0.1192 - val_loss: 2.3017 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.2994 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2940 - acc: 0.1250 - val_loss: 2.2906 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2829 - acc: 0.1642 - val_loss: 2.2683 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.2548 - acc: 0.1192 - val_loss: 2.2274 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 2.1912 - acc: 0.2092 - val_loss: 2.1438 - val_acc: 0.2233\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.0856 - acc: 0.2475 - val_loss: 2.0385 - val_acc: 0.2433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.9743 - acc: 0.2058 - val_loss: 1.9530 - val_acc: 0.2500\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.8881 - acc: 0.2225 - val_loss: 1.9124 - val_acc: 0.2033\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 1.8320 - acc: 0.2458 - val_loss: 1.8812 - val_acc: 0.2633\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 1.7898 - acc: 0.2225 - val_loss: 1.8638 - val_acc: 0.2233\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 1.7628 - acc: 0.2400 - val_loss: 1.8481 - val_acc: 0.2400\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 1.7355 - acc: 0.3008 - val_loss: 1.8381 - val_acc: 0.3333\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 1.7081 - acc: 0.3192 - val_loss: 1.7993 - val_acc: 0.2933\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.6718 - acc: 0.3467 - val_loss: 1.7586 - val_acc: 0.3067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 1.6169 - acc: 0.3342 - val_loss: 1.6793 - val_acc: 0.3533\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 1.5433 - acc: 0.4300 - val_loss: 1.5971 - val_acc: 0.4333\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.4581 - acc: 0.4608 - val_loss: 1.5270 - val_acc: 0.4700\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 1.3787 - acc: 0.4825 - val_loss: 1.4456 - val_acc: 0.4733\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.3075 - acc: 0.5300 - val_loss: 1.4089 - val_acc: 0.5200\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.2469 - acc: 0.5767 - val_loss: 1.3372 - val_acc: 0.5433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 1.1903 - acc: 0.5850 - val_loss: 1.2921 - val_acc: 0.5433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.1456 - acc: 0.5900 - val_loss: 1.2587 - val_acc: 0.5500\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.0989 - acc: 0.5983 - val_loss: 1.2747 - val_acc: 0.5133\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.0608 - acc: 0.5842 - val_loss: 1.2046 - val_acc: 0.5533\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 1.0264 - acc: 0.6108 - val_loss: 1.1769 - val_acc: 0.5567\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 0.9898 - acc: 0.5983 - val_loss: 1.2025 - val_acc: 0.5467\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 691us/step - loss: 2.3022 - acc: 0.1042 - val_loss: 2.2992 - val_acc: 0.1433\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.2911 - acc: 0.1192 - val_loss: 2.2675 - val_acc: 0.1433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 2.2253 - acc: 0.1342 - val_loss: 2.1626 - val_acc: 0.1933\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.1166 - acc: 0.2083 - val_loss: 2.0561 - val_acc: 0.2367\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 2.0317 - acc: 0.2483 - val_loss: 1.9916 - val_acc: 0.2333\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.9734 - acc: 0.2500 - val_loss: 1.9388 - val_acc: 0.2200\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 1.9240 - acc: 0.2550 - val_loss: 1.9035 - val_acc: 0.2633\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.8716 - acc: 0.2767 - val_loss: 1.8657 - val_acc: 0.3000\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.8240 - acc: 0.2867 - val_loss: 1.8425 - val_acc: 0.2967\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.7664 - acc: 0.2983 - val_loss: 1.7990 - val_acc: 0.3067\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.7106 - acc: 0.3000 - val_loss: 1.7635 - val_acc: 0.3033\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.6493 - acc: 0.3167 - val_loss: 1.7374 - val_acc: 0.3067\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.5886 - acc: 0.3258 - val_loss: 1.7039 - val_acc: 0.3200\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 1.5419 - acc: 0.3325 - val_loss: 1.6687 - val_acc: 0.3267\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 1.5002 - acc: 0.3450 - val_loss: 1.6663 - val_acc: 0.3300\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.4865 - acc: 0.3558 - val_loss: 1.6389 - val_acc: 0.3467\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.4341 - acc: 0.3700 - val_loss: 1.6161 - val_acc: 0.3600\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.4034 - acc: 0.3958 - val_loss: 1.6034 - val_acc: 0.3733\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 1.3752 - acc: 0.4067 - val_loss: 1.6039 - val_acc: 0.3900\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.3589 - acc: 0.4317 - val_loss: 1.5903 - val_acc: 0.4067\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.3260 - acc: 0.4508 - val_loss: 1.5815 - val_acc: 0.4067\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.3128 - acc: 0.4575 - val_loss: 1.5890 - val_acc: 0.4233\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.2944 - acc: 0.4658 - val_loss: 1.5827 - val_acc: 0.4667\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.2716 - acc: 0.4767 - val_loss: 1.5883 - val_acc: 0.4467\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 1.2545 - acc: 0.5058 - val_loss: 1.5760 - val_acc: 0.4700\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.2329 - acc: 0.5150 - val_loss: 1.5656 - val_acc: 0.4967\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.2158 - acc: 0.5400 - val_loss: 1.5643 - val_acc: 0.5200\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.1962 - acc: 0.5458 - val_loss: 1.5401 - val_acc: 0.5167\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.1814 - acc: 0.5600 - val_loss: 1.5277 - val_acc: 0.5333\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.1503 - acc: 0.5875 - val_loss: 1.5392 - val_acc: 0.5300\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 642us/step - loss: 2.2926 - acc: 0.1425 - val_loss: 2.2614 - val_acc: 0.2100\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.2126 - acc: 0.1992 - val_loss: 2.1140 - val_acc: 0.2167\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.0533 - acc: 0.2100 - val_loss: 1.9656 - val_acc: 0.2567\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.9353 - acc: 0.2442 - val_loss: 1.8713 - val_acc: 0.2767\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.8648 - acc: 0.2650 - val_loss: 1.8229 - val_acc: 0.3033\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.8098 - acc: 0.2950 - val_loss: 1.7665 - val_acc: 0.2833\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.7515 - acc: 0.3033 - val_loss: 1.7073 - val_acc: 0.3100\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.6850 - acc: 0.3317 - val_loss: 1.6446 - val_acc: 0.3500\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 1.6125 - acc: 0.3742 - val_loss: 1.5726 - val_acc: 0.3633\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.5295 - acc: 0.3967 - val_loss: 1.4995 - val_acc: 0.3900\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 1.4553 - acc: 0.4292 - val_loss: 1.4480 - val_acc: 0.4067\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.3818 - acc: 0.4683 - val_loss: 1.4039 - val_acc: 0.5267\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.3204 - acc: 0.4925 - val_loss: 1.4625 - val_acc: 0.4967\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.2879 - acc: 0.5067 - val_loss: 1.3713 - val_acc: 0.5000\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 1.2372 - acc: 0.5217 - val_loss: 1.3473 - val_acc: 0.5333\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.2083 - acc: 0.5350 - val_loss: 1.3428 - val_acc: 0.5933\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.1758 - acc: 0.5708 - val_loss: 1.3411 - val_acc: 0.5433\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.1424 - acc: 0.5833 - val_loss: 1.3117 - val_acc: 0.5867\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 94us/step - loss: 1.1071 - acc: 0.6008 - val_loss: 1.3072 - val_acc: 0.5867\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.0725 - acc: 0.6050 - val_loss: 1.2902 - val_acc: 0.5967\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 1.0411 - acc: 0.6200 - val_loss: 1.2974 - val_acc: 0.6100\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.0212 - acc: 0.6283 - val_loss: 1.2904 - val_acc: 0.6200\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.9745 - acc: 0.6375 - val_loss: 1.2491 - val_acc: 0.6233\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 0.9526 - acc: 0.6492 - val_loss: 1.2341 - val_acc: 0.6167\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.9316 - acc: 0.6550 - val_loss: 1.1969 - val_acc: 0.6500\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.9077 - acc: 0.6675 - val_loss: 1.2329 - val_acc: 0.6200\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 0.8744 - acc: 0.6700 - val_loss: 1.1956 - val_acc: 0.6367\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 0.8461 - acc: 0.6800 - val_loss: 1.1778 - val_acc: 0.6533\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 0.8343 - acc: 0.6842 - val_loss: 1.2233 - val_acc: 0.6200\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 0.8241 - acc: 0.6958 - val_loss: 1.1788 - val_acc: 0.6400\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 674us/step - loss: 2.2695 - acc: 0.1525 - val_loss: 2.2177 - val_acc: 0.2767\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.0599 - acc: 0.2917 - val_loss: 1.9719 - val_acc: 0.2733\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 1.8215 - acc: 0.3258 - val_loss: 1.8052 - val_acc: 0.3600\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 1.6561 - acc: 0.3567 - val_loss: 1.6786 - val_acc: 0.3700\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 1.4842 - acc: 0.4467 - val_loss: 1.5037 - val_acc: 0.4167\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.3156 - acc: 0.5142 - val_loss: 1.3788 - val_acc: 0.5233\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 1.1427 - acc: 0.6000 - val_loss: 1.2194 - val_acc: 0.5700\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 0.9889 - acc: 0.6542 - val_loss: 1.0935 - val_acc: 0.6267\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 0.8547 - acc: 0.7100 - val_loss: 1.0268 - val_acc: 0.6267\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.7735 - acc: 0.7267 - val_loss: 1.0035 - val_acc: 0.6633\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 0.7113 - acc: 0.7542 - val_loss: 0.9444 - val_acc: 0.6900\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 0.6707 - acc: 0.7733 - val_loss: 0.9267 - val_acc: 0.6833\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 0.6030 - acc: 0.8083 - val_loss: 0.9416 - val_acc: 0.6800\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.5548 - acc: 0.8167 - val_loss: 0.9439 - val_acc: 0.6800\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.5271 - acc: 0.8358 - val_loss: 0.9315 - val_acc: 0.7200\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 0.4933 - acc: 0.8467 - val_loss: 0.9313 - val_acc: 0.7100\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.4688 - acc: 0.8558 - val_loss: 0.9802 - val_acc: 0.7133\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 0.4438 - acc: 0.8625 - val_loss: 0.9365 - val_acc: 0.7100\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.4207 - acc: 0.8767 - val_loss: 0.9510 - val_acc: 0.7367\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 0.3901 - acc: 0.8742 - val_loss: 0.9553 - val_acc: 0.7267\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.3904 - acc: 0.8758 - val_loss: 0.9894 - val_acc: 0.7300\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 0.3684 - acc: 0.8875 - val_loss: 1.0024 - val_acc: 0.7133\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 0.3516 - acc: 0.8917 - val_loss: 0.9730 - val_acc: 0.7533\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.3151 - acc: 0.9017 - val_loss: 1.0004 - val_acc: 0.7633\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 0.2949 - acc: 0.9150 - val_loss: 1.0195 - val_acc: 0.7367\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.2894 - acc: 0.9092 - val_loss: 1.0585 - val_acc: 0.7567\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.2732 - acc: 0.9158 - val_loss: 1.0789 - val_acc: 0.7433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.2620 - acc: 0.9167 - val_loss: 1.1003 - val_acc: 0.7300\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 0.2636 - acc: 0.9058 - val_loss: 1.0597 - val_acc: 0.7567\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 0.2375 - acc: 0.9292 - val_loss: 1.1495 - val_acc: 0.7167\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 669us/step - loss: 2.0258 - acc: 0.2492 - val_loss: 1.6825 - val_acc: 0.3800\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 1.3402 - acc: 0.5158 - val_loss: 1.1045 - val_acc: 0.6467\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 0.9089 - acc: 0.6942 - val_loss: 0.8121 - val_acc: 0.7333\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.6826 - acc: 0.7783 - val_loss: 0.7693 - val_acc: 0.7600\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 124us/step - loss: 0.5268 - acc: 0.8450 - val_loss: 0.6744 - val_acc: 0.7900\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 0.4215 - acc: 0.8783 - val_loss: 0.6562 - val_acc: 0.7900\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.3603 - acc: 0.8950 - val_loss: 0.6409 - val_acc: 0.8133\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 0.2870 - acc: 0.9200 - val_loss: 0.6774 - val_acc: 0.7867\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 127us/step - loss: 0.2477 - acc: 0.9300 - val_loss: 0.6545 - val_acc: 0.8033\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 0.2025 - acc: 0.9458 - val_loss: 0.6384 - val_acc: 0.8367\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 0.1462 - acc: 0.9642 - val_loss: 0.6364 - val_acc: 0.8267\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 105us/step - loss: 0.1260 - acc: 0.9692 - val_loss: 0.6499 - val_acc: 0.8267\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 106us/step - loss: 0.0930 - acc: 0.9842 - val_loss: 0.6159 - val_acc: 0.8433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 0.0892 - acc: 0.9808 - val_loss: 0.6516 - val_acc: 0.8433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 0.0563 - acc: 0.9933 - val_loss: 0.7120 - val_acc: 0.8600\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 0.0483 - acc: 0.9925 - val_loss: 0.7604 - val_acc: 0.8533\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 114us/step - loss: 0.0522 - acc: 0.9900 - val_loss: 0.7146 - val_acc: 0.8567\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 0.0372 - acc: 0.9942 - val_loss: 0.7096 - val_acc: 0.8633\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 0.0291 - acc: 0.9967 - val_loss: 0.7047 - val_acc: 0.8567\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 0.0221 - acc: 0.9967 - val_loss: 0.7451 - val_acc: 0.8633\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 122us/step - loss: 0.0174 - acc: 0.9975 - val_loss: 0.8208 - val_acc: 0.8600\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 0.0140 - acc: 0.9983 - val_loss: 0.8227 - val_acc: 0.8600\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 0.0120 - acc: 0.9983 - val_loss: 0.8066 - val_acc: 0.8567\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 0.0081 - acc: 0.9992 - val_loss: 0.8013 - val_acc: 0.8500\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.8263 - val_acc: 0.8600\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.8168 - val_acc: 0.8600\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.8467 - val_acc: 0.8533\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.8667 - val_acc: 0.8600\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.8660 - val_acc: 0.8533\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.8846 - val_acc: 0.8533\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 673us/step - loss: 2.2999 - acc: 0.0975 - val_loss: 2.2938 - val_acc: 0.0933\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 2.2787 - acc: 0.1217 - val_loss: 2.2617 - val_acc: 0.1133\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 2.2312 - acc: 0.1508 - val_loss: 2.2050 - val_acc: 0.1767\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 95us/step - loss: 2.1694 - acc: 0.1875 - val_loss: 2.1480 - val_acc: 0.1833\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 2.1089 - acc: 0.2042 - val_loss: 2.1093 - val_acc: 0.1867\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 2.0696 - acc: 0.2067 - val_loss: 2.0921 - val_acc: 0.1867\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 106us/step - loss: 2.0435 - acc: 0.2108 - val_loss: 2.0796 - val_acc: 0.2000\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 2.0240 - acc: 0.2125 - val_loss: 2.0741 - val_acc: 0.1900\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 2.0087 - acc: 0.2150 - val_loss: 2.0669 - val_acc: 0.1967\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 1.9976 - acc: 0.2133 - val_loss: 2.0648 - val_acc: 0.2133\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 1.9841 - acc: 0.2208 - val_loss: 2.0676 - val_acc: 0.2133\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 1.9741 - acc: 0.2200 - val_loss: 2.0681 - val_acc: 0.2167\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 95us/step - loss: 1.9643 - acc: 0.2225 - val_loss: 2.0516 - val_acc: 0.2233\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 1.9544 - acc: 0.2275 - val_loss: 2.0601 - val_acc: 0.2200\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 1.9454 - acc: 0.2375 - val_loss: 2.0647 - val_acc: 0.2200\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 1.9375 - acc: 0.2308 - val_loss: 2.0656 - val_acc: 0.2167\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 1.9300 - acc: 0.2392 - val_loss: 2.0419 - val_acc: 0.2300\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 1.9234 - acc: 0.2392 - val_loss: 2.0417 - val_acc: 0.2333\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 1.9175 - acc: 0.2383 - val_loss: 2.0611 - val_acc: 0.2233\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 1.9073 - acc: 0.2392 - val_loss: 2.0481 - val_acc: 0.2433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 1.9016 - acc: 0.2483 - val_loss: 2.0468 - val_acc: 0.2367\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 1.8935 - acc: 0.2525 - val_loss: 2.0642 - val_acc: 0.2433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 1.8843 - acc: 0.2500 - val_loss: 2.0425 - val_acc: 0.2433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 1.8805 - acc: 0.2575 - val_loss: 2.0633 - val_acc: 0.2333\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 1.8708 - acc: 0.2533 - val_loss: 2.0435 - val_acc: 0.2367\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 1.8621 - acc: 0.2567 - val_loss: 2.0436 - val_acc: 0.2300\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 1.8603 - acc: 0.2583 - val_loss: 2.0467 - val_acc: 0.2233\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 1.8539 - acc: 0.2608 - val_loss: 2.0496 - val_acc: 0.2333\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 1.8458 - acc: 0.2575 - val_loss: 2.0746 - val_acc: 0.2333\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 1.8390 - acc: 0.2700 - val_loss: 2.0385 - val_acc: 0.2367\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 648us/step - loss: 2.3025 - acc: 0.1150 - val_loss: 2.3025 - val_acc: 0.1433\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 106us/step - loss: 2.3017 - acc: 0.1192 - val_loss: 2.3018 - val_acc: 0.1433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 105us/step - loss: 2.3011 - acc: 0.1192 - val_loss: 2.3018 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 2.3008 - acc: 0.1192 - val_loss: 2.3016 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 2.3003 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 2.3000 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 2.2997 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 2.2995 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 106us/step - loss: 2.2993 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 2.2991 - acc: 0.1192 - val_loss: 2.3008 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 105us/step - loss: 2.2990 - acc: 0.1192 - val_loss: 2.3009 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 121us/step - loss: 2.2988 - acc: 0.1192 - val_loss: 2.3009 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 108us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3009 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3009 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3009 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3009 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3008 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 106us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3009 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 106us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 105us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 105us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 814us/step - loss: 2.2763 - acc: 0.1208 - val_loss: 2.2356 - val_acc: 0.2133\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 122us/step - loss: 2.0835 - acc: 0.2750 - val_loss: 2.0046 - val_acc: 0.2600\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 1.8130 - acc: 0.2958 - val_loss: 1.8187 - val_acc: 0.2733\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 1.6070 - acc: 0.3317 - val_loss: 1.6730 - val_acc: 0.3067\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 1.4401 - acc: 0.3792 - val_loss: 1.4639 - val_acc: 0.4567\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 1.2545 - acc: 0.5525 - val_loss: 1.2775 - val_acc: 0.6033\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 1.0731 - acc: 0.6400 - val_loss: 1.2021 - val_acc: 0.6033\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 0.9504 - acc: 0.6783 - val_loss: 1.1128 - val_acc: 0.6267\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 0.9191 - acc: 0.6500 - val_loss: 1.0348 - val_acc: 0.6667\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 0.8080 - acc: 0.7250 - val_loss: 0.9894 - val_acc: 0.6900\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 0.7409 - acc: 0.7458 - val_loss: 0.9924 - val_acc: 0.6700\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 0.6980 - acc: 0.7667 - val_loss: 0.9552 - val_acc: 0.7033\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.6625 - acc: 0.7858 - val_loss: 0.9451 - val_acc: 0.7100\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 0.6326 - acc: 0.7875 - val_loss: 0.9530 - val_acc: 0.7067\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.5919 - acc: 0.8158 - val_loss: 0.9679 - val_acc: 0.7000\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 106us/step - loss: 0.5436 - acc: 0.8250 - val_loss: 0.9310 - val_acc: 0.7400\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 0.5139 - acc: 0.8333 - val_loss: 0.9411 - val_acc: 0.7033\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 0.5049 - acc: 0.8258 - val_loss: 1.0096 - val_acc: 0.7200\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.4677 - acc: 0.8533 - val_loss: 0.9660 - val_acc: 0.7500\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 0.4420 - acc: 0.8633 - val_loss: 0.9894 - val_acc: 0.7333\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 0.4024 - acc: 0.8775 - val_loss: 1.0627 - val_acc: 0.7233\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 0.3755 - acc: 0.8925 - val_loss: 1.0148 - val_acc: 0.7367\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.3663 - acc: 0.8925 - val_loss: 1.0707 - val_acc: 0.7500\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 0.3457 - acc: 0.9017 - val_loss: 1.0361 - val_acc: 0.7433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 0.3363 - acc: 0.9083 - val_loss: 1.0838 - val_acc: 0.7533\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.3173 - acc: 0.9058 - val_loss: 1.1153 - val_acc: 0.7333\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 105us/step - loss: 0.2886 - acc: 0.9183 - val_loss: 1.2012 - val_acc: 0.7167\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 0.2723 - acc: 0.9300 - val_loss: 1.1123 - val_acc: 0.7600\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.2651 - acc: 0.9333 - val_loss: 1.1034 - val_acc: 0.7367\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.2536 - acc: 0.9375 - val_loss: 1.1702 - val_acc: 0.7667\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 627us/step - loss: 2.1557 - acc: 0.2217 - val_loss: 1.9687 - val_acc: 0.2267\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 1.7025 - acc: 0.3808 - val_loss: 1.4386 - val_acc: 0.5367\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 1.0442 - acc: 0.6825 - val_loss: 0.8273 - val_acc: 0.7233\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 0.6536 - acc: 0.8017 - val_loss: 0.6161 - val_acc: 0.7900\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 0.4941 - acc: 0.8483 - val_loss: 0.5977 - val_acc: 0.8067\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 0.3887 - acc: 0.8817 - val_loss: 0.5459 - val_acc: 0.8233\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 0.3023 - acc: 0.9100 - val_loss: 0.5117 - val_acc: 0.8367\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 116us/step - loss: 0.2364 - acc: 0.9367 - val_loss: 0.5546 - val_acc: 0.8367\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 116us/step - loss: 0.2085 - acc: 0.9408 - val_loss: 0.5786 - val_acc: 0.8267\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 116us/step - loss: 0.1669 - acc: 0.9583 - val_loss: 0.5262 - val_acc: 0.8500\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 0.1458 - acc: 0.9592 - val_loss: 0.5833 - val_acc: 0.8167\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 0.1110 - acc: 0.9742 - val_loss: 0.5549 - val_acc: 0.8533\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 0.0757 - acc: 0.9858 - val_loss: 0.5385 - val_acc: 0.8500\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 0.0700 - acc: 0.9850 - val_loss: 0.5337 - val_acc: 0.8600\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 120us/step - loss: 0.0592 - acc: 0.9875 - val_loss: 0.6423 - val_acc: 0.8400\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 0.0458 - acc: 0.9892 - val_loss: 0.6148 - val_acc: 0.8267\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 116us/step - loss: 0.0325 - acc: 0.9933 - val_loss: 0.6354 - val_acc: 0.8333\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 121us/step - loss: 0.0271 - acc: 0.9942 - val_loss: 0.6141 - val_acc: 0.8567\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 0.0314 - acc: 0.9917 - val_loss: 0.6917 - val_acc: 0.8567\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 119us/step - loss: 0.0208 - acc: 0.9958 - val_loss: 0.6830 - val_acc: 0.8533\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 0.0168 - acc: 0.9958 - val_loss: 0.6550 - val_acc: 0.8500\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 0.0131 - acc: 0.9975 - val_loss: 0.7248 - val_acc: 0.8600\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 0.0097 - acc: 0.9983 - val_loss: 0.7068 - val_acc: 0.8633\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 0.0093 - acc: 0.9992 - val_loss: 0.7605 - val_acc: 0.8633\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 0.0069 - acc: 0.9992 - val_loss: 0.7634 - val_acc: 0.8500\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 120us/step - loss: 0.0058 - acc: 0.9992 - val_loss: 0.7803 - val_acc: 0.8500\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 0.0062 - acc: 0.9992 - val_loss: 0.7838 - val_acc: 0.8567\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 0.0051 - acc: 0.9992 - val_loss: 0.8522 - val_acc: 0.8500\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 0.0072 - acc: 0.9992 - val_loss: 0.7826 - val_acc: 0.8400\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 0.0048 - acc: 0.9992 - val_loss: 0.8079 - val_acc: 0.8533\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 601us/step - loss: 2.3535 - acc: 0.1017 - val_loss: 2.3120 - val_acc: 0.1167\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.3401 - acc: 0.1017 - val_loss: 2.3051 - val_acc: 0.1167\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.3300 - acc: 0.1017 - val_loss: 2.3005 - val_acc: 0.1167\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.3226 - acc: 0.1017 - val_loss: 2.2972 - val_acc: 0.1167\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.3170 - acc: 0.1017 - val_loss: 2.2951 - val_acc: 0.1167\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.3123 - acc: 0.1075 - val_loss: 2.2941 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.3088 - acc: 0.1192 - val_loss: 2.2941 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.3063 - acc: 0.1192 - val_loss: 2.2935 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.3039 - acc: 0.1192 - val_loss: 2.2938 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.3026 - acc: 0.1192 - val_loss: 2.2942 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.3013 - acc: 0.1192 - val_loss: 2.2950 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.3004 - acc: 0.1192 - val_loss: 2.2954 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2997 - acc: 0.1192 - val_loss: 2.2954 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2993 - acc: 0.1192 - val_loss: 2.2964 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2988 - acc: 0.1192 - val_loss: 2.2965 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.2973 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.2974 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2978 - acc: 0.1192 - val_loss: 2.2976 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2976 - acc: 0.1192 - val_loss: 2.2980 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2971 - acc: 0.1192 - val_loss: 2.2984 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2966 - acc: 0.1192 - val_loss: 2.2980 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2961 - acc: 0.1192 - val_loss: 2.2979 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2957 - acc: 0.1192 - val_loss: 2.2977 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2950 - acc: 0.1192 - val_loss: 2.2977 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2939 - acc: 0.1192 - val_loss: 2.2962 - val_acc: 0.1433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2928 - acc: 0.1192 - val_loss: 2.2961 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2915 - acc: 0.1192 - val_loss: 2.2944 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2895 - acc: 0.1192 - val_loss: 2.2932 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2870 - acc: 0.1192 - val_loss: 2.2905 - val_acc: 0.1433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.2836 - acc: 0.1192 - val_loss: 2.2880 - val_acc: 0.1433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 583us/step - loss: 2.3490 - acc: 0.0908 - val_loss: 2.3238 - val_acc: 0.1233\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.3294 - acc: 0.0908 - val_loss: 2.3145 - val_acc: 0.1233\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.3181 - acc: 0.0908 - val_loss: 2.3084 - val_acc: 0.1233\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.3110 - acc: 0.0883 - val_loss: 2.3055 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.3068 - acc: 0.1192 - val_loss: 2.3037 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.3039 - acc: 0.1192 - val_loss: 2.3030 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.3021 - acc: 0.1192 - val_loss: 2.3023 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.3008 - acc: 0.1192 - val_loss: 2.3023 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2998 - acc: 0.1192 - val_loss: 2.3017 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2994 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2988 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3006 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2978 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2975 - acc: 0.1192 - val_loss: 2.3006 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2973 - acc: 0.1192 - val_loss: 2.3006 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2969 - acc: 0.1192 - val_loss: 2.3003 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2964 - acc: 0.1192 - val_loss: 2.2999 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2957 - acc: 0.1192 - val_loss: 2.2988 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2940 - acc: 0.1192 - val_loss: 2.2961 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2924 - acc: 0.1192 - val_loss: 2.2948 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2897 - acc: 0.1192 - val_loss: 2.2920 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2869 - acc: 0.1192 - val_loss: 2.2880 - val_acc: 0.1433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 2.2825 - acc: 0.1192 - val_loss: 2.2845 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2774 - acc: 0.1192 - val_loss: 2.2783 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2707 - acc: 0.1192 - val_loss: 2.2713 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2632 - acc: 0.1192 - val_loss: 2.2637 - val_acc: 0.1433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2532 - acc: 0.1692 - val_loss: 2.2543 - val_acc: 0.2133\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 630us/step - loss: 2.3549 - acc: 0.1192 - val_loss: 2.3138 - val_acc: 0.1433\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.3175 - acc: 0.1192 - val_loss: 2.2979 - val_acc: 0.1433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.3043 - acc: 0.1192 - val_loss: 2.2963 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.3006 - acc: 0.1192 - val_loss: 2.2980 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2999 - acc: 0.1192 - val_loss: 2.2992 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.2993 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.2975 - acc: 0.1192 - val_loss: 2.2987 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.2968 - acc: 0.1192 - val_loss: 2.2977 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.2942 - acc: 0.1192 - val_loss: 2.2953 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 2.2885 - acc: 0.1192 - val_loss: 2.2883 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.2754 - acc: 0.1358 - val_loss: 2.2742 - val_acc: 0.2033\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.2516 - acc: 0.2025 - val_loss: 2.2461 - val_acc: 0.2000\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.2136 - acc: 0.2167 - val_loss: 2.2069 - val_acc: 0.2033\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.1618 - acc: 0.2175 - val_loss: 2.1573 - val_acc: 0.2033\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 2.1023 - acc: 0.2183 - val_loss: 2.1087 - val_acc: 0.2033\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 2.0427 - acc: 0.2017 - val_loss: 2.0612 - val_acc: 0.1867\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 1.9888 - acc: 0.1983 - val_loss: 2.0229 - val_acc: 0.2000\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 1.9432 - acc: 0.2167 - val_loss: 1.9836 - val_acc: 0.2000\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 1.9059 - acc: 0.2183 - val_loss: 1.9644 - val_acc: 0.2000\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 1.8760 - acc: 0.2183 - val_loss: 1.9332 - val_acc: 0.2033\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 1.8510 - acc: 0.2192 - val_loss: 1.9122 - val_acc: 0.2067\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 1.8293 - acc: 0.2200 - val_loss: 1.8993 - val_acc: 0.2033\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 1.8138 - acc: 0.2150 - val_loss: 1.8799 - val_acc: 0.2100\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 1.8002 - acc: 0.2208 - val_loss: 1.8703 - val_acc: 0.2067\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 1.7923 - acc: 0.2075 - val_loss: 1.8670 - val_acc: 0.2067\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 1.7815 - acc: 0.2217 - val_loss: 1.8789 - val_acc: 0.2067\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 58us/step - loss: 1.7744 - acc: 0.2225 - val_loss: 1.8633 - val_acc: 0.2067\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 57us/step - loss: 1.7662 - acc: 0.2225 - val_loss: 1.8606 - val_acc: 0.2067\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 1.7609 - acc: 0.2225 - val_loss: 1.8549 - val_acc: 0.2067\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 56us/step - loss: 1.7568 - acc: 0.2150 - val_loss: 1.8488 - val_acc: 0.2067\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 578us/step - loss: 2.3549 - acc: 0.0867 - val_loss: 2.3074 - val_acc: 0.0733\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.3019 - acc: 0.1208 - val_loss: 2.2994 - val_acc: 0.1433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.3016 - acc: 0.1067 - val_loss: 2.3006 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.3009 - acc: 0.1192 - val_loss: 2.3029 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 2.3006 - acc: 0.1033 - val_loss: 2.3034 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.3011 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.2960 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.2920 - acc: 0.1175 - val_loss: 2.2810 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 2.2675 - acc: 0.1225 - val_loss: 2.2448 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 2.2096 - acc: 0.1917 - val_loss: 2.1671 - val_acc: 0.1900\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 2.1120 - acc: 0.1975 - val_loss: 2.0671 - val_acc: 0.1933\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.0078 - acc: 0.1925 - val_loss: 1.9897 - val_acc: 0.1933\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 1.9282 - acc: 0.1950 - val_loss: 1.9519 - val_acc: 0.1933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 1.8765 - acc: 0.2183 - val_loss: 1.9261 - val_acc: 0.2033\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.8498 - acc: 0.2033 - val_loss: 1.9187 - val_acc: 0.2000\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 1.8314 - acc: 0.2167 - val_loss: 1.9080 - val_acc: 0.1933\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 1.8181 - acc: 0.2200 - val_loss: 1.9083 - val_acc: 0.1933\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 1.8100 - acc: 0.2100 - val_loss: 1.8988 - val_acc: 0.2033\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.8163 - acc: 0.2167 - val_loss: 1.9055 - val_acc: 0.2033\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.8030 - acc: 0.2208 - val_loss: 1.9141 - val_acc: 0.1867\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 1.7966 - acc: 0.2217 - val_loss: 1.8779 - val_acc: 0.2167\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 1.7921 - acc: 0.2125 - val_loss: 1.9152 - val_acc: 0.1833\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 1.7928 - acc: 0.2092 - val_loss: 1.8948 - val_acc: 0.1967\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 1.7835 - acc: 0.2125 - val_loss: 1.9003 - val_acc: 0.2067\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.7818 - acc: 0.2167 - val_loss: 1.8885 - val_acc: 0.2100\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 1.7810 - acc: 0.2050 - val_loss: 1.8892 - val_acc: 0.1900\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 1.7788 - acc: 0.2225 - val_loss: 1.8536 - val_acc: 0.2067\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 1.7716 - acc: 0.2167 - val_loss: 1.8495 - val_acc: 0.2167\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.7685 - acc: 0.2283 - val_loss: 1.8572 - val_acc: 0.2100\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.7671 - acc: 0.2167 - val_loss: 1.8546 - val_acc: 0.2100\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 723us/step - loss: 2.3027 - acc: 0.1042 - val_loss: 2.2992 - val_acc: 0.1233\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.2878 - acc: 0.1233 - val_loss: 2.2777 - val_acc: 0.0967\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.2145 - acc: 0.1575 - val_loss: 2.1947 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.1155 - acc: 0.1875 - val_loss: 2.1220 - val_acc: 0.1400\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.0673 - acc: 0.2008 - val_loss: 2.1092 - val_acc: 0.1267\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.0382 - acc: 0.2025 - val_loss: 2.0739 - val_acc: 0.1533\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.0153 - acc: 0.2058 - val_loss: 2.0533 - val_acc: 0.1533\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.9966 - acc: 0.2042 - val_loss: 2.0444 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 1.9755 - acc: 0.2075 - val_loss: 2.0304 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 1.9609 - acc: 0.2117 - val_loss: 2.0246 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.9468 - acc: 0.2117 - val_loss: 2.0120 - val_acc: 0.1467\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.9310 - acc: 0.2133 - val_loss: 2.0144 - val_acc: 0.1467\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.9253 - acc: 0.2158 - val_loss: 2.0652 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.9054 - acc: 0.2158 - val_loss: 2.0013 - val_acc: 0.1533\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.8932 - acc: 0.2483 - val_loss: 2.0420 - val_acc: 0.2333\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 1.8805 - acc: 0.2950 - val_loss: 1.9894 - val_acc: 0.2767\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.8579 - acc: 0.2908 - val_loss: 1.9648 - val_acc: 0.2533\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 1.8394 - acc: 0.2917 - val_loss: 1.9503 - val_acc: 0.2500\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.8152 - acc: 0.2875 - val_loss: 1.9333 - val_acc: 0.3067\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.7875 - acc: 0.3267 - val_loss: 1.9056 - val_acc: 0.3367\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 1.7542 - acc: 0.3400 - val_loss: 1.8634 - val_acc: 0.3533\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 1.7211 - acc: 0.3525 - val_loss: 1.8069 - val_acc: 0.3900\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 1.6863 - acc: 0.3583 - val_loss: 1.7878 - val_acc: 0.3900\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.6691 - acc: 0.3592 - val_loss: 1.7759 - val_acc: 0.3800\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 1.6379 - acc: 0.3650 - val_loss: 1.8307 - val_acc: 0.3767\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 1.6051 - acc: 0.3750 - val_loss: 1.7157 - val_acc: 0.4133\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 1.5771 - acc: 0.3833 - val_loss: 1.6732 - val_acc: 0.4067\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 1.5770 - acc: 0.3692 - val_loss: 1.7329 - val_acc: 0.3967\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 1.5455 - acc: 0.3750 - val_loss: 1.6674 - val_acc: 0.4167\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 1.5301 - acc: 0.3908 - val_loss: 1.6747 - val_acc: 0.4300\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 761us/step - loss: 2.2741 - acc: 0.0900 - val_loss: 2.2126 - val_acc: 0.1333\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.1650 - acc: 0.1758 - val_loss: 2.0847 - val_acc: 0.2300\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.0687 - acc: 0.2425 - val_loss: 1.9973 - val_acc: 0.3100\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.9935 - acc: 0.2717 - val_loss: 1.9203 - val_acc: 0.3033\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.9239 - acc: 0.2717 - val_loss: 1.8423 - val_acc: 0.3033\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.8535 - acc: 0.2933 - val_loss: 1.7745 - val_acc: 0.3067\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.7886 - acc: 0.3108 - val_loss: 1.7161 - val_acc: 0.3133\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.7213 - acc: 0.3358 - val_loss: 1.6562 - val_acc: 0.3600\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.6359 - acc: 0.3625 - val_loss: 1.5862 - val_acc: 0.3733\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.5399 - acc: 0.3758 - val_loss: 1.5178 - val_acc: 0.3900\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.4435 - acc: 0.3925 - val_loss: 1.4584 - val_acc: 0.4000\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 1.3487 - acc: 0.4100 - val_loss: 1.4146 - val_acc: 0.4067\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.2699 - acc: 0.4533 - val_loss: 1.3835 - val_acc: 0.4367\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.1938 - acc: 0.4925 - val_loss: 1.3534 - val_acc: 0.4533\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 1.1541 - acc: 0.5267 - val_loss: 1.3404 - val_acc: 0.4833\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 1.0935 - acc: 0.5642 - val_loss: 1.3280 - val_acc: 0.5200\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 1.0581 - acc: 0.5908 - val_loss: 1.3560 - val_acc: 0.5133\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.0199 - acc: 0.6108 - val_loss: 1.3459 - val_acc: 0.5000\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 0.9890 - acc: 0.6192 - val_loss: 1.3414 - val_acc: 0.5333\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 0.9559 - acc: 0.6292 - val_loss: 1.3413 - val_acc: 0.5567\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 0.9306 - acc: 0.6358 - val_loss: 1.3624 - val_acc: 0.5400\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 0.9027 - acc: 0.6500 - val_loss: 1.3447 - val_acc: 0.5667\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 0.8699 - acc: 0.6667 - val_loss: 1.3526 - val_acc: 0.5833\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 0.8562 - acc: 0.6750 - val_loss: 1.3537 - val_acc: 0.5867\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 0.8290 - acc: 0.6950 - val_loss: 1.3357 - val_acc: 0.5900\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 0.8055 - acc: 0.7117 - val_loss: 1.3518 - val_acc: 0.6100\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 0.7955 - acc: 0.7150 - val_loss: 1.3693 - val_acc: 0.6333\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 0.7756 - acc: 0.7333 - val_loss: 1.3402 - val_acc: 0.6167\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 0.7485 - acc: 0.7433 - val_loss: 1.3495 - val_acc: 0.6267\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 0.7160 - acc: 0.7625 - val_loss: 1.3229 - val_acc: 0.6533\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 792us/step - loss: 2.2466 - acc: 0.1317 - val_loss: 2.1306 - val_acc: 0.1667\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 2.0279 - acc: 0.2017 - val_loss: 1.9110 - val_acc: 0.3433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.8275 - acc: 0.3092 - val_loss: 1.6512 - val_acc: 0.3467\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.5755 - acc: 0.3458 - val_loss: 1.4803 - val_acc: 0.3867\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.3835 - acc: 0.4383 - val_loss: 1.3289 - val_acc: 0.5533\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.1935 - acc: 0.6292 - val_loss: 1.1682 - val_acc: 0.6567\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.0434 - acc: 0.6700 - val_loss: 1.0326 - val_acc: 0.6833\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.9090 - acc: 0.6992 - val_loss: 0.9638 - val_acc: 0.6933\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 0.8164 - acc: 0.7150 - val_loss: 0.8869 - val_acc: 0.7367\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.7274 - acc: 0.7558 - val_loss: 0.8718 - val_acc: 0.7300\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 0.6658 - acc: 0.7608 - val_loss: 0.8830 - val_acc: 0.7333\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 0.6109 - acc: 0.7783 - val_loss: 0.9222 - val_acc: 0.7367\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.5826 - acc: 0.7833 - val_loss: 0.8663 - val_acc: 0.7400\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 0.5342 - acc: 0.8008 - val_loss: 0.8722 - val_acc: 0.7367\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.4948 - acc: 0.8292 - val_loss: 0.8720 - val_acc: 0.7367\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.4635 - acc: 0.8367 - val_loss: 0.9213 - val_acc: 0.7667\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.4371 - acc: 0.8567 - val_loss: 0.8752 - val_acc: 0.7433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.4158 - acc: 0.8583 - val_loss: 0.8807 - val_acc: 0.7500\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.3892 - acc: 0.8700 - val_loss: 0.9492 - val_acc: 0.7667\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.3534 - acc: 0.8817 - val_loss: 0.9365 - val_acc: 0.7600\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 0.3320 - acc: 0.8850 - val_loss: 0.9602 - val_acc: 0.7767\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 0.3392 - acc: 0.8850 - val_loss: 1.0841 - val_acc: 0.7200\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 0.3184 - acc: 0.8767 - val_loss: 0.9955 - val_acc: 0.7433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 0.2856 - acc: 0.9058 - val_loss: 1.0384 - val_acc: 0.7600\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 0.2753 - acc: 0.9025 - val_loss: 0.9844 - val_acc: 0.7433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 0.2629 - acc: 0.9150 - val_loss: 1.0281 - val_acc: 0.7533\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 0.2400 - acc: 0.9208 - val_loss: 1.0399 - val_acc: 0.7400\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 0.2276 - acc: 0.9258 - val_loss: 1.1009 - val_acc: 0.7467\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 0.2286 - acc: 0.9225 - val_loss: 1.0462 - val_acc: 0.7733\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 0.2135 - acc: 0.9242 - val_loss: 1.0791 - val_acc: 0.7500\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 739us/step - loss: 2.0922 - acc: 0.2133 - val_loss: 1.6568 - val_acc: 0.3800\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 1.2024 - acc: 0.5525 - val_loss: 0.8983 - val_acc: 0.6967\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 0.7711 - acc: 0.7500 - val_loss: 0.7857 - val_acc: 0.7633\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.5614 - acc: 0.8100 - val_loss: 0.6470 - val_acc: 0.7800\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.4309 - acc: 0.8708 - val_loss: 0.6135 - val_acc: 0.7900\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 0.3500 - acc: 0.8942 - val_loss: 0.5684 - val_acc: 0.8433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 0.2963 - acc: 0.9108 - val_loss: 0.5992 - val_acc: 0.8100\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 0.2687 - acc: 0.9167 - val_loss: 0.5928 - val_acc: 0.8433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 0.2271 - acc: 0.9267 - val_loss: 0.5682 - val_acc: 0.8500\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 97us/step - loss: 0.1888 - acc: 0.9367 - val_loss: 0.5752 - val_acc: 0.8333\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 0.1801 - acc: 0.9425 - val_loss: 0.5620 - val_acc: 0.8533\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 0.1850 - acc: 0.9400 - val_loss: 0.7333 - val_acc: 0.8167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 0.1372 - acc: 0.9592 - val_loss: 0.7050 - val_acc: 0.8300\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 97us/step - loss: 0.0943 - acc: 0.9767 - val_loss: 0.6255 - val_acc: 0.8533\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 0.0643 - acc: 0.9867 - val_loss: 0.6362 - val_acc: 0.8533\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.0546 - acc: 0.9867 - val_loss: 0.7017 - val_acc: 0.8433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 95us/step - loss: 0.0430 - acc: 0.9900 - val_loss: 0.6825 - val_acc: 0.8467\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 94us/step - loss: 0.0280 - acc: 0.9967 - val_loss: 0.7516 - val_acc: 0.8400\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 0.0224 - acc: 0.9992 - val_loss: 0.7491 - val_acc: 0.8600\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 0.0208 - acc: 0.9975 - val_loss: 0.8297 - val_acc: 0.8367\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 97us/step - loss: 0.0144 - acc: 0.9992 - val_loss: 0.8058 - val_acc: 0.8400\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 0.0109 - acc: 0.9992 - val_loss: 0.8050 - val_acc: 0.8533\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 0.0089 - acc: 0.9992 - val_loss: 0.8561 - val_acc: 0.8500\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 0.0115 - acc: 0.9983 - val_loss: 0.8671 - val_acc: 0.8367\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 0.0073 - acc: 0.9992 - val_loss: 0.8466 - val_acc: 0.8500\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.8213 - val_acc: 0.8500\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.8857 - val_acc: 0.8467\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.8831 - val_acc: 0.8400\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.8901 - val_acc: 0.8533\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.8844 - val_acc: 0.8433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 765us/step - loss: 2.3018 - acc: 0.1058 - val_loss: 2.2993 - val_acc: 0.0833\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 141us/step - loss: 2.2957 - acc: 0.0908 - val_loss: 2.2884 - val_acc: 0.0833\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 2.2749 - acc: 0.1142 - val_loss: 2.2477 - val_acc: 0.1133\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 106us/step - loss: 2.2124 - acc: 0.1558 - val_loss: 2.1502 - val_acc: 0.1333\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 2.1170 - acc: 0.1858 - val_loss: 2.0526 - val_acc: 0.1900\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 2.0398 - acc: 0.1883 - val_loss: 1.9874 - val_acc: 0.1967\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 1.9902 - acc: 0.1883 - val_loss: 1.9634 - val_acc: 0.2033\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 1.9490 - acc: 0.1875 - val_loss: 1.9263 - val_acc: 0.1933\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 1.9220 - acc: 0.1825 - val_loss: 1.9049 - val_acc: 0.2133\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 1.8901 - acc: 0.1992 - val_loss: 1.9029 - val_acc: 0.2133\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 1.8669 - acc: 0.2017 - val_loss: 1.8678 - val_acc: 0.2133\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 1.8461 - acc: 0.2042 - val_loss: 1.8635 - val_acc: 0.2200\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 1.8228 - acc: 0.2075 - val_loss: 1.8515 - val_acc: 0.2167\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 1.8048 - acc: 0.2100 - val_loss: 1.8409 - val_acc: 0.2200\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 1.7885 - acc: 0.2117 - val_loss: 1.8204 - val_acc: 0.2233\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 1.7789 - acc: 0.2183 - val_loss: 1.8240 - val_acc: 0.2300\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 1.7573 - acc: 0.2158 - val_loss: 1.8251 - val_acc: 0.2333\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 1.7465 - acc: 0.2200 - val_loss: 1.7876 - val_acc: 0.2233\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 1.7289 - acc: 0.2208 - val_loss: 1.7901 - val_acc: 0.2333\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 1.7162 - acc: 0.2317 - val_loss: 1.7759 - val_acc: 0.2233\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 1.7063 - acc: 0.2408 - val_loss: 1.7862 - val_acc: 0.2300\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 1.6934 - acc: 0.2483 - val_loss: 1.7848 - val_acc: 0.2300\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 1.6783 - acc: 0.2542 - val_loss: 1.7575 - val_acc: 0.2467\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 1.6730 - acc: 0.2767 - val_loss: 1.7633 - val_acc: 0.2567\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 1.6556 - acc: 0.2833 - val_loss: 1.7792 - val_acc: 0.2600\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 1.6494 - acc: 0.2925 - val_loss: 1.7814 - val_acc: 0.2600\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 1.6316 - acc: 0.3025 - val_loss: 1.7774 - val_acc: 0.2633\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 105us/step - loss: 1.6229 - acc: 0.3083 - val_loss: 1.7473 - val_acc: 0.2567\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 1.6243 - acc: 0.3167 - val_loss: 1.7441 - val_acc: 0.2567\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 1.5984 - acc: 0.3183 - val_loss: 1.7848 - val_acc: 0.2600\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 727us/step - loss: 2.2999 - acc: 0.1367 - val_loss: 2.2897 - val_acc: 0.1367\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 2.2521 - acc: 0.1617 - val_loss: 2.1850 - val_acc: 0.1900\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 2.1034 - acc: 0.1883 - val_loss: 2.0049 - val_acc: 0.2100\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 1.9379 - acc: 0.1933 - val_loss: 1.8679 - val_acc: 0.2200\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 1.8051 - acc: 0.2200 - val_loss: 1.7486 - val_acc: 0.3000\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 1.6811 - acc: 0.3225 - val_loss: 1.6233 - val_acc: 0.3467\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 1.5522 - acc: 0.3517 - val_loss: 1.5277 - val_acc: 0.3467\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 1.4475 - acc: 0.3733 - val_loss: 1.4443 - val_acc: 0.3633\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 1.3670 - acc: 0.4267 - val_loss: 1.3931 - val_acc: 0.4033\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 1.3022 - acc: 0.4642 - val_loss: 1.3764 - val_acc: 0.4167\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 1.2626 - acc: 0.5133 - val_loss: 1.3179 - val_acc: 0.5300\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 109us/step - loss: 1.2107 - acc: 0.5633 - val_loss: 1.3635 - val_acc: 0.4733\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 1.1803 - acc: 0.5892 - val_loss: 1.2841 - val_acc: 0.5400\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 1.1254 - acc: 0.6308 - val_loss: 1.2943 - val_acc: 0.5533\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 1.1036 - acc: 0.6592 - val_loss: 1.2654 - val_acc: 0.5600\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 1.0725 - acc: 0.6475 - val_loss: 1.2382 - val_acc: 0.5600\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 1.0562 - acc: 0.6542 - val_loss: 1.2412 - val_acc: 0.5667\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 1.0036 - acc: 0.6942 - val_loss: 1.2709 - val_acc: 0.5700\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 0.9689 - acc: 0.7092 - val_loss: 1.2318 - val_acc: 0.5767\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 0.9262 - acc: 0.7183 - val_loss: 1.1863 - val_acc: 0.6033\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 0.9050 - acc: 0.7175 - val_loss: 1.1933 - val_acc: 0.6033\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 0.8711 - acc: 0.7400 - val_loss: 1.2060 - val_acc: 0.6200\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 0.8303 - acc: 0.7458 - val_loss: 1.1896 - val_acc: 0.6400\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 105us/step - loss: 0.8030 - acc: 0.7708 - val_loss: 1.1703 - val_acc: 0.6567\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 0.7931 - acc: 0.7708 - val_loss: 1.1965 - val_acc: 0.6500\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 0.7574 - acc: 0.7800 - val_loss: 1.1744 - val_acc: 0.6700\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 0.7273 - acc: 0.7908 - val_loss: 1.2067 - val_acc: 0.6500\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 0.7136 - acc: 0.7875 - val_loss: 1.1739 - val_acc: 0.6433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 0.6913 - acc: 0.7950 - val_loss: 1.1648 - val_acc: 0.6600\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 129us/step - loss: 0.6581 - acc: 0.8067 - val_loss: 1.1776 - val_acc: 0.6633\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 745us/step - loss: 2.2942 - acc: 0.1675 - val_loss: 2.2682 - val_acc: 0.2300\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 2.2037 - acc: 0.1817 - val_loss: 2.1427 - val_acc: 0.1267\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 2.0316 - acc: 0.1792 - val_loss: 2.0141 - val_acc: 0.1367\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 123us/step - loss: 1.8831 - acc: 0.2483 - val_loss: 1.8723 - val_acc: 0.3033\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 1.7406 - acc: 0.2958 - val_loss: 1.7432 - val_acc: 0.3400\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 1.5900 - acc: 0.3342 - val_loss: 1.6044 - val_acc: 0.3767\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 1.4736 - acc: 0.3633 - val_loss: 1.5083 - val_acc: 0.4200\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 1.3576 - acc: 0.4325 - val_loss: 1.4444 - val_acc: 0.4800\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 116us/step - loss: 1.2683 - acc: 0.4800 - val_loss: 1.4745 - val_acc: 0.5333\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 1.1872 - acc: 0.5342 - val_loss: 1.3287 - val_acc: 0.5933\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 1.0994 - acc: 0.5592 - val_loss: 1.3181 - val_acc: 0.6033\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 1.0399 - acc: 0.6125 - val_loss: 1.2887 - val_acc: 0.6333\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 0.9697 - acc: 0.6408 - val_loss: 1.2769 - val_acc: 0.6367\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 0.9084 - acc: 0.6850 - val_loss: 1.4594 - val_acc: 0.5867\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 0.8704 - acc: 0.6858 - val_loss: 1.2063 - val_acc: 0.6800\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 0.8022 - acc: 0.7292 - val_loss: 1.2418 - val_acc: 0.6833\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 0.7607 - acc: 0.7417 - val_loss: 1.2730 - val_acc: 0.6833\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 0.7147 - acc: 0.7650 - val_loss: 1.2354 - val_acc: 0.6833\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 0.6814 - acc: 0.7683 - val_loss: 1.2256 - val_acc: 0.6933\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 0.6413 - acc: 0.7892 - val_loss: 1.4039 - val_acc: 0.6800\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 0.6140 - acc: 0.8008 - val_loss: 1.3662 - val_acc: 0.6900\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 0.5774 - acc: 0.8150 - val_loss: 1.2619 - val_acc: 0.7200\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 0.5471 - acc: 0.8233 - val_loss: 1.4080 - val_acc: 0.7167\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 0.5076 - acc: 0.8467 - val_loss: 1.3403 - val_acc: 0.7100\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 0.4864 - acc: 0.8500 - val_loss: 1.3568 - val_acc: 0.7133\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 0.4825 - acc: 0.8517 - val_loss: 1.5025 - val_acc: 0.7067\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 0.4438 - acc: 0.8617 - val_loss: 1.3746 - val_acc: 0.7267\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 0.4156 - acc: 0.8817 - val_loss: 1.3292 - val_acc: 0.7300\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 0.3836 - acc: 0.8883 - val_loss: 1.4178 - val_acc: 0.7467\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 0.3652 - acc: 0.8933 - val_loss: 1.4361 - val_acc: 0.7433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 706us/step - loss: 2.1859 - acc: 0.2167 - val_loss: 1.9297 - val_acc: 0.3600\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 127us/step - loss: 1.5563 - acc: 0.4700 - val_loss: 1.3888 - val_acc: 0.5967\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 135us/step - loss: 0.9750 - acc: 0.7050 - val_loss: 0.9728 - val_acc: 0.6800\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 139us/step - loss: 0.6919 - acc: 0.8000 - val_loss: 0.8638 - val_acc: 0.7333\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 0.5652 - acc: 0.8217 - val_loss: 0.7290 - val_acc: 0.7633\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 142us/step - loss: 0.4738 - acc: 0.8475 - val_loss: 0.7694 - val_acc: 0.7633\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 0.3812 - acc: 0.8925 - val_loss: 0.6549 - val_acc: 0.8000\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 136us/step - loss: 0.2742 - acc: 0.9275 - val_loss: 0.7033 - val_acc: 0.8067\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 0.2686 - acc: 0.9175 - val_loss: 0.6926 - val_acc: 0.8167\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 0.2320 - acc: 0.9375 - val_loss: 0.7300 - val_acc: 0.8000\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 134us/step - loss: 0.1935 - acc: 0.9492 - val_loss: 0.8025 - val_acc: 0.8167\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 139us/step - loss: 0.2367 - acc: 0.9333 - val_loss: 0.6742 - val_acc: 0.8200\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 138us/step - loss: 0.1224 - acc: 0.9692 - val_loss: 0.7242 - val_acc: 0.8233\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 137us/step - loss: 0.0765 - acc: 0.9833 - val_loss: 0.7527 - val_acc: 0.8367\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 0.0652 - acc: 0.9858 - val_loss: 0.8957 - val_acc: 0.8300\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 143us/step - loss: 0.0561 - acc: 0.9883 - val_loss: 0.9558 - val_acc: 0.8400\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 140us/step - loss: 0.0960 - acc: 0.9692 - val_loss: 0.7234 - val_acc: 0.8400\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 0.0516 - acc: 0.9883 - val_loss: 0.7779 - val_acc: 0.8433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 141us/step - loss: 0.0299 - acc: 0.9967 - val_loss: 0.7945 - val_acc: 0.8433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 135us/step - loss: 0.0208 - acc: 0.9975 - val_loss: 0.8730 - val_acc: 0.8400\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 151us/step - loss: 0.0138 - acc: 0.9983 - val_loss: 0.9104 - val_acc: 0.8367\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 135us/step - loss: 0.0119 - acc: 0.9983 - val_loss: 0.9406 - val_acc: 0.8500\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 143us/step - loss: 0.0105 - acc: 0.9992 - val_loss: 0.9605 - val_acc: 0.8367\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 0.0083 - acc: 0.9992 - val_loss: 1.0389 - val_acc: 0.8400\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 148us/step - loss: 0.0092 - acc: 0.9992 - val_loss: 0.9828 - val_acc: 0.8333\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 130us/step - loss: 0.0063 - acc: 0.9992 - val_loss: 1.0329 - val_acc: 0.8367\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 144us/step - loss: 0.0050 - acc: 0.9992 - val_loss: 0.9891 - val_acc: 0.8533\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 138us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 1.0054 - val_acc: 0.8567\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 137us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 1.0695 - val_acc: 0.8400\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 141us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 1.0758 - val_acc: 0.8400\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 671us/step - loss: 2.3361 - acc: 0.1075 - val_loss: 2.3245 - val_acc: 0.0833\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.3230 - acc: 0.1075 - val_loss: 2.3148 - val_acc: 0.0833\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.3145 - acc: 0.1075 - val_loss: 2.3082 - val_acc: 0.0833\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.3090 - acc: 0.1075 - val_loss: 2.3047 - val_acc: 0.0833\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.3053 - acc: 0.1075 - val_loss: 2.3024 - val_acc: 0.0833\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.3031 - acc: 0.1025 - val_loss: 2.3006 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.3014 - acc: 0.1192 - val_loss: 2.3000 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.3004 - acc: 0.1192 - val_loss: 2.3003 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2996 - acc: 0.1192 - val_loss: 2.3001 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2991 - acc: 0.1192 - val_loss: 2.2999 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 2.2989 - acc: 0.1192 - val_loss: 2.3002 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3002 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.2996 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3004 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3004 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3004 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3005 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3017 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3018 - val_acc: 0.1433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2979 - acc: 0.1192 - val_loss: 2.3018 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3019 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2978 - acc: 0.1192 - val_loss: 2.3017 - val_acc: 0.1433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 653us/step - loss: 2.4495 - acc: 0.0908 - val_loss: 2.3668 - val_acc: 0.1233\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.3967 - acc: 0.0908 - val_loss: 2.3371 - val_acc: 0.1233\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.3640 - acc: 0.0908 - val_loss: 2.3186 - val_acc: 0.1233\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.3428 - acc: 0.0908 - val_loss: 2.3077 - val_acc: 0.1233\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.3292 - acc: 0.0908 - val_loss: 2.3017 - val_acc: 0.1233\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.3201 - acc: 0.0908 - val_loss: 2.2977 - val_acc: 0.1233\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.3140 - acc: 0.0908 - val_loss: 2.2963 - val_acc: 0.1233\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.3099 - acc: 0.0908 - val_loss: 2.2954 - val_acc: 0.1233\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.3065 - acc: 0.0908 - val_loss: 2.2948 - val_acc: 0.1233\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.3043 - acc: 0.0908 - val_loss: 2.2953 - val_acc: 0.1233\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.3027 - acc: 0.0908 - val_loss: 2.2957 - val_acc: 0.1233\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.3016 - acc: 0.1017 - val_loss: 2.2961 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.3007 - acc: 0.1192 - val_loss: 2.2962 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.3002 - acc: 0.1192 - val_loss: 2.2966 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2997 - acc: 0.1192 - val_loss: 2.2970 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2992 - acc: 0.1192 - val_loss: 2.2976 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2989 - acc: 0.1192 - val_loss: 2.2980 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.2985 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.2989 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.2992 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.2996 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.2997 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.2999 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3005 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3003 - val_acc: 0.1433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3005 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3007 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3005 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 689us/step - loss: 2.3801 - acc: 0.0817 - val_loss: 2.3226 - val_acc: 0.0900\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.3356 - acc: 0.0817 - val_loss: 2.3055 - val_acc: 0.0900\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.3159 - acc: 0.1133 - val_loss: 2.2998 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.3074 - acc: 0.1192 - val_loss: 2.2980 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.3033 - acc: 0.1192 - val_loss: 2.2986 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.3013 - acc: 0.1192 - val_loss: 2.2982 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.3001 - acc: 0.1192 - val_loss: 2.2994 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2992 - acc: 0.1192 - val_loss: 2.2996 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3001 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2988 - acc: 0.1192 - val_loss: 2.3007 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3003 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3005 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3004 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2976 - acc: 0.1192 - val_loss: 2.2993 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2954 - acc: 0.1192 - val_loss: 2.2974 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 2.2923 - acc: 0.1192 - val_loss: 2.2919 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2857 - acc: 0.1192 - val_loss: 2.2839 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2738 - acc: 0.1192 - val_loss: 2.2687 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2536 - acc: 0.1192 - val_loss: 2.2454 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2213 - acc: 0.2050 - val_loss: 2.2066 - val_acc: 0.2133\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.1721 - acc: 0.2200 - val_loss: 2.1528 - val_acc: 0.2133\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.1114 - acc: 0.2200 - val_loss: 2.1001 - val_acc: 0.2100\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.0488 - acc: 0.2192 - val_loss: 2.0472 - val_acc: 0.2167\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 1.9959 - acc: 0.2183 - val_loss: 2.0103 - val_acc: 0.2167\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 1.9491 - acc: 0.2208 - val_loss: 1.9836 - val_acc: 0.2167\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 1.9136 - acc: 0.2075 - val_loss: 1.9615 - val_acc: 0.2167\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 1.8872 - acc: 0.2200 - val_loss: 1.9461 - val_acc: 0.2167\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 1.8658 - acc: 0.2083 - val_loss: 1.9256 - val_acc: 0.2133\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 59us/step - loss: 1.8660 - acc: 0.2158 - val_loss: 1.9108 - val_acc: 0.2167\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 1.8432 - acc: 0.2033 - val_loss: 1.9077 - val_acc: 0.2000\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 697us/step - loss: 2.4097 - acc: 0.1075 - val_loss: 2.3459 - val_acc: 0.0833\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.3089 - acc: 0.1008 - val_loss: 2.3041 - val_acc: 0.0933\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.3017 - acc: 0.1183 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.3011 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.3020 - acc: 0.1192 - val_loss: 2.3034 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.3010 - acc: 0.1192 - val_loss: 2.3034 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.3005 - acc: 0.1192 - val_loss: 2.2999 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.2993 - acc: 0.1192 - val_loss: 2.3004 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 2.2971 - acc: 0.1192 - val_loss: 2.2954 - val_acc: 0.1433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.2861 - acc: 0.1392 - val_loss: 2.2738 - val_acc: 0.2067\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.2486 - acc: 0.1725 - val_loss: 2.2205 - val_acc: 0.2100\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.1640 - acc: 0.2150 - val_loss: 2.1192 - val_acc: 0.2133\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 2.0526 - acc: 0.2017 - val_loss: 2.0107 - val_acc: 0.2200\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 1.9554 - acc: 0.2200 - val_loss: 1.9388 - val_acc: 0.2167\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 1.8997 - acc: 0.2192 - val_loss: 1.9283 - val_acc: 0.2167\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.8545 - acc: 0.2058 - val_loss: 1.9364 - val_acc: 0.2100\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.8342 - acc: 0.2192 - val_loss: 1.9320 - val_acc: 0.2100\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 1.8156 - acc: 0.2175 - val_loss: 1.8953 - val_acc: 0.2133\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 1.8068 - acc: 0.2217 - val_loss: 1.9130 - val_acc: 0.2133\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 1.8018 - acc: 0.2200 - val_loss: 1.9479 - val_acc: 0.2100\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 1.7889 - acc: 0.2200 - val_loss: 1.9335 - val_acc: 0.2100\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 1.7817 - acc: 0.2058 - val_loss: 1.8747 - val_acc: 0.2167\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.7926 - acc: 0.2192 - val_loss: 1.9478 - val_acc: 0.2033\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.7689 - acc: 0.2050 - val_loss: 1.9566 - val_acc: 0.2100\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 1.7732 - acc: 0.2208 - val_loss: 1.9451 - val_acc: 0.2100\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 1.7620 - acc: 0.2142 - val_loss: 1.9491 - val_acc: 0.2100\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 1.7550 - acc: 0.2200 - val_loss: 1.9150 - val_acc: 0.2133\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 1.7606 - acc: 0.2225 - val_loss: 1.8743 - val_acc: 0.2233\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 1.7605 - acc: 0.2208 - val_loss: 1.9476 - val_acc: 0.2067\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 1.7657 - acc: 0.2042 - val_loss: 1.9308 - val_acc: 0.2100\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 850us/step - loss: 2.3020 - acc: 0.1042 - val_loss: 2.3005 - val_acc: 0.0933\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 2.2941 - acc: 0.1100 - val_loss: 2.2843 - val_acc: 0.0933\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 2.2666 - acc: 0.1125 - val_loss: 2.2427 - val_acc: 0.0833\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.2104 - acc: 0.1175 - val_loss: 2.1759 - val_acc: 0.0800\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.1327 - acc: 0.1625 - val_loss: 2.1209 - val_acc: 0.1800\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 2.0698 - acc: 0.2000 - val_loss: 2.0783 - val_acc: 0.1867\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 2.0234 - acc: 0.2000 - val_loss: 2.0584 - val_acc: 0.2067\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.9868 - acc: 0.2108 - val_loss: 2.0389 - val_acc: 0.1833\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.9542 - acc: 0.2000 - val_loss: 2.0132 - val_acc: 0.1833\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.9262 - acc: 0.2092 - val_loss: 1.9988 - val_acc: 0.1967\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.8966 - acc: 0.2075 - val_loss: 2.0570 - val_acc: 0.2067\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.8737 - acc: 0.2100 - val_loss: 1.9954 - val_acc: 0.2033\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.8509 - acc: 0.2100 - val_loss: 1.9508 - val_acc: 0.2033\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.8559 - acc: 0.1992 - val_loss: 1.9757 - val_acc: 0.1967\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 1.8070 - acc: 0.2083 - val_loss: 1.9499 - val_acc: 0.2033\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.7972 - acc: 0.2217 - val_loss: 1.9437 - val_acc: 0.1900\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.7810 - acc: 0.2175 - val_loss: 1.9496 - val_acc: 0.2067\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.7759 - acc: 0.2208 - val_loss: 1.9645 - val_acc: 0.2100\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.7669 - acc: 0.2158 - val_loss: 1.9448 - val_acc: 0.1967\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.7660 - acc: 0.2142 - val_loss: 1.9539 - val_acc: 0.1900\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.7410 - acc: 0.2133 - val_loss: 1.9274 - val_acc: 0.2133\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 1.7297 - acc: 0.2192 - val_loss: 1.9345 - val_acc: 0.2167\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 97us/step - loss: 1.7204 - acc: 0.2208 - val_loss: 1.9709 - val_acc: 0.2133\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 1.7153 - acc: 0.2225 - val_loss: 1.9382 - val_acc: 0.2033\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 1.7363 - acc: 0.2283 - val_loss: 2.0216 - val_acc: 0.2233\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.7205 - acc: 0.2225 - val_loss: 1.9243 - val_acc: 0.2133\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 1.6716 - acc: 0.2283 - val_loss: 1.9159 - val_acc: 0.2167\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.6583 - acc: 0.2300 - val_loss: 1.9903 - val_acc: 0.2167\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.6528 - acc: 0.2267 - val_loss: 1.9094 - val_acc: 0.2100\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.6404 - acc: 0.2333 - val_loss: 1.9097 - val_acc: 0.2300\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 848us/step - loss: 2.2784 - acc: 0.1075 - val_loss: 2.2342 - val_acc: 0.0767\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.1656 - acc: 0.1875 - val_loss: 2.1568 - val_acc: 0.1800\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 2.0914 - acc: 0.2092 - val_loss: 2.0959 - val_acc: 0.1767\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 2.0314 - acc: 0.2117 - val_loss: 2.0428 - val_acc: 0.1867\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.9806 - acc: 0.2275 - val_loss: 1.9985 - val_acc: 0.2333\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.9359 - acc: 0.2300 - val_loss: 1.9546 - val_acc: 0.2300\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 1.8884 - acc: 0.2117 - val_loss: 1.9036 - val_acc: 0.2500\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.8423 - acc: 0.2642 - val_loss: 1.8669 - val_acc: 0.2200\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.8024 - acc: 0.2250 - val_loss: 1.8616 - val_acc: 0.2267\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 1.7616 - acc: 0.2525 - val_loss: 1.8046 - val_acc: 0.2700\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.7303 - acc: 0.2725 - val_loss: 1.7799 - val_acc: 0.2967\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.6546 - acc: 0.3008 - val_loss: 1.7360 - val_acc: 0.3367\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.5986 - acc: 0.3450 - val_loss: 1.6975 - val_acc: 0.3400\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 1.5431 - acc: 0.3667 - val_loss: 1.6196 - val_acc: 0.3700\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.4788 - acc: 0.3825 - val_loss: 1.6153 - val_acc: 0.3833\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.4326 - acc: 0.3975 - val_loss: 1.5446 - val_acc: 0.3933\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.3815 - acc: 0.4200 - val_loss: 1.5133 - val_acc: 0.4033\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 1.3449 - acc: 0.4325 - val_loss: 1.5414 - val_acc: 0.4000\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 1.3056 - acc: 0.4517 - val_loss: 1.5195 - val_acc: 0.4500\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.2687 - acc: 0.4725 - val_loss: 1.5081 - val_acc: 0.4700\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 1.2341 - acc: 0.5017 - val_loss: 1.4817 - val_acc: 0.4600\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 94us/step - loss: 1.2028 - acc: 0.5125 - val_loss: 1.4743 - val_acc: 0.4767\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 1.1754 - acc: 0.5358 - val_loss: 1.5293 - val_acc: 0.4867\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.1606 - acc: 0.5575 - val_loss: 1.4392 - val_acc: 0.5400\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.1114 - acc: 0.5783 - val_loss: 1.4467 - val_acc: 0.5367\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.1233 - acc: 0.5833 - val_loss: 1.4744 - val_acc: 0.5233\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.0802 - acc: 0.5842 - val_loss: 1.4637 - val_acc: 0.5567\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.0272 - acc: 0.6217 - val_loss: 1.4543 - val_acc: 0.5533\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.0101 - acc: 0.6400 - val_loss: 1.4812 - val_acc: 0.5567\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 0.9890 - acc: 0.6458 - val_loss: 1.4826 - val_acc: 0.5633\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 869us/step - loss: 2.2788 - acc: 0.1575 - val_loss: 2.2202 - val_acc: 0.2300\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.1375 - acc: 0.2425 - val_loss: 2.0211 - val_acc: 0.2733\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 1.9413 - acc: 0.2800 - val_loss: 1.8516 - val_acc: 0.3233\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 1.7706 - acc: 0.3083 - val_loss: 1.6805 - val_acc: 0.3133\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 1.6304 - acc: 0.3492 - val_loss: 1.5685 - val_acc: 0.3700\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 1.4736 - acc: 0.3892 - val_loss: 1.4714 - val_acc: 0.4267\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 1.3510 - acc: 0.4575 - val_loss: 1.3471 - val_acc: 0.5100\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 1.2278 - acc: 0.5117 - val_loss: 1.2717 - val_acc: 0.5333\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 1.1413 - acc: 0.5700 - val_loss: 1.2353 - val_acc: 0.5367\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 1.0670 - acc: 0.5858 - val_loss: 1.2068 - val_acc: 0.5500\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 0.9876 - acc: 0.6042 - val_loss: 1.1725 - val_acc: 0.5567\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 0.9402 - acc: 0.6317 - val_loss: 1.1447 - val_acc: 0.5533\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 0.8911 - acc: 0.6625 - val_loss: 1.1442 - val_acc: 0.5833\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 0.8465 - acc: 0.6808 - val_loss: 1.1480 - val_acc: 0.5733\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 0.7761 - acc: 0.7300 - val_loss: 1.1352 - val_acc: 0.6200\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 0.7561 - acc: 0.7500 - val_loss: 1.1473 - val_acc: 0.6333\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 0.6883 - acc: 0.7725 - val_loss: 1.1262 - val_acc: 0.6633\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 0.6558 - acc: 0.7983 - val_loss: 1.1059 - val_acc: 0.6600\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 0.6078 - acc: 0.8108 - val_loss: 1.1538 - val_acc: 0.6400\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 0.5751 - acc: 0.8308 - val_loss: 1.1078 - val_acc: 0.6700\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 95us/step - loss: 0.5529 - acc: 0.8342 - val_loss: 1.0534 - val_acc: 0.7000\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 0.4767 - acc: 0.8525 - val_loss: 1.1426 - val_acc: 0.6800\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 0.4496 - acc: 0.8558 - val_loss: 1.0707 - val_acc: 0.6867\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 0.4254 - acc: 0.8617 - val_loss: 1.1028 - val_acc: 0.6900\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 0.3915 - acc: 0.8725 - val_loss: 1.1498 - val_acc: 0.6767\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 0.3718 - acc: 0.8825 - val_loss: 1.2008 - val_acc: 0.6967\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 0.3608 - acc: 0.8817 - val_loss: 1.1578 - val_acc: 0.6800\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 0.3350 - acc: 0.8908 - val_loss: 1.1022 - val_acc: 0.7267\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 0.2890 - acc: 0.9092 - val_loss: 1.1504 - val_acc: 0.7133\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 0.2488 - acc: 0.9300 - val_loss: 1.1758 - val_acc: 0.7233\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 908us/step - loss: 2.1550 - acc: 0.1917 - val_loss: 1.8557 - val_acc: 0.3000\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 1.5183 - acc: 0.4217 - val_loss: 1.3642 - val_acc: 0.5800\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 119us/step - loss: 1.0003 - acc: 0.6733 - val_loss: 1.0059 - val_acc: 0.7333\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 121us/step - loss: 0.7098 - acc: 0.7642 - val_loss: 0.8797 - val_acc: 0.7200\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 0.5688 - acc: 0.8250 - val_loss: 0.7881 - val_acc: 0.7567\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 119us/step - loss: 0.4465 - acc: 0.8667 - val_loss: 0.7990 - val_acc: 0.7833\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 0.4078 - acc: 0.8825 - val_loss: 0.8934 - val_acc: 0.7633\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 148us/step - loss: 0.3451 - acc: 0.8967 - val_loss: 0.7749 - val_acc: 0.7867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 152us/step - loss: 0.2717 - acc: 0.9200 - val_loss: 0.7810 - val_acc: 0.8100\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 124us/step - loss: 0.2347 - acc: 0.9375 - val_loss: 0.8275 - val_acc: 0.8100\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 135us/step - loss: 0.2066 - acc: 0.9408 - val_loss: 0.8844 - val_acc: 0.7900\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 148us/step - loss: 0.1959 - acc: 0.9408 - val_loss: 1.0503 - val_acc: 0.7900\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 0.1773 - acc: 0.9517 - val_loss: 0.9856 - val_acc: 0.7933\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 0.1508 - acc: 0.9600 - val_loss: 0.9425 - val_acc: 0.8000\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 152us/step - loss: 0.1122 - acc: 0.9683 - val_loss: 0.8714 - val_acc: 0.8167\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 0.0865 - acc: 0.9792 - val_loss: 0.8817 - val_acc: 0.8200\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 0.0741 - acc: 0.9808 - val_loss: 0.9607 - val_acc: 0.8267\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 129us/step - loss: 0.0685 - acc: 0.9858 - val_loss: 0.9619 - val_acc: 0.8167\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 0.0647 - acc: 0.9833 - val_loss: 1.0210 - val_acc: 0.8233\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 152us/step - loss: 0.0511 - acc: 0.9892 - val_loss: 1.0111 - val_acc: 0.8233\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 0.0576 - acc: 0.9850 - val_loss: 1.0949 - val_acc: 0.8167\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 0.0741 - acc: 0.9758 - val_loss: 1.1706 - val_acc: 0.8133\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 0.0784 - acc: 0.9767 - val_loss: 1.2651 - val_acc: 0.7967\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 0.0798 - acc: 0.9750 - val_loss: 1.0831 - val_acc: 0.8167\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 0.0369 - acc: 0.9875 - val_loss: 1.0444 - val_acc: 0.8400\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 0.0207 - acc: 0.9958 - val_loss: 1.1179 - val_acc: 0.8200\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 0.0127 - acc: 0.9975 - val_loss: 1.1153 - val_acc: 0.8200\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 0.0098 - acc: 0.9975 - val_loss: 1.1401 - val_acc: 0.8233\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 0.0069 - acc: 0.9992 - val_loss: 1.1792 - val_acc: 0.8267\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 1.1440 - val_acc: 0.8300\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 757us/step - loss: 2.3028 - acc: 0.1000 - val_loss: 2.3026 - val_acc: 0.0733\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 2.3005 - acc: 0.1300 - val_loss: 2.2921 - val_acc: 0.2133\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 2.2700 - acc: 0.1425 - val_loss: 2.2217 - val_acc: 0.1833\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 119us/step - loss: 2.1992 - acc: 0.1517 - val_loss: 2.1504 - val_acc: 0.1900\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 2.1419 - acc: 0.1950 - val_loss: 2.1076 - val_acc: 0.1800\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 116us/step - loss: 2.1000 - acc: 0.1967 - val_loss: 2.0701 - val_acc: 0.1800\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 2.0676 - acc: 0.2108 - val_loss: 2.0421 - val_acc: 0.1833\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 2.0448 - acc: 0.2158 - val_loss: 2.0286 - val_acc: 0.1933\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 2.0250 - acc: 0.2108 - val_loss: 2.0172 - val_acc: 0.1933\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 2.0076 - acc: 0.2125 - val_loss: 2.0108 - val_acc: 0.1900\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 1.9938 - acc: 0.2192 - val_loss: 2.0118 - val_acc: 0.1867\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 116us/step - loss: 1.9779 - acc: 0.2192 - val_loss: 2.0181 - val_acc: 0.1800\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 1.9664 - acc: 0.2167 - val_loss: 2.0075 - val_acc: 0.1900\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 1.9535 - acc: 0.2183 - val_loss: 1.9996 - val_acc: 0.1933\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 1.9547 - acc: 0.2242 - val_loss: 1.9990 - val_acc: 0.2000\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 1.9330 - acc: 0.2217 - val_loss: 1.9818 - val_acc: 0.1967\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 1.9280 - acc: 0.2258 - val_loss: 2.0573 - val_acc: 0.2067\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 1.9253 - acc: 0.2267 - val_loss: 1.9929 - val_acc: 0.1933\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 1.9101 - acc: 0.2258 - val_loss: 2.0132 - val_acc: 0.2067\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 1.9002 - acc: 0.2308 - val_loss: 2.0053 - val_acc: 0.1933\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 1.8904 - acc: 0.2258 - val_loss: 2.0377 - val_acc: 0.1300\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 1.8869 - acc: 0.2192 - val_loss: 2.0221 - val_acc: 0.2400\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 116us/step - loss: 1.8787 - acc: 0.2367 - val_loss: 2.0247 - val_acc: 0.2500\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 1.8672 - acc: 0.2350 - val_loss: 2.0408 - val_acc: 0.2433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 1.8672 - acc: 0.2417 - val_loss: 2.0212 - val_acc: 0.2400\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 1.8616 - acc: 0.2333 - val_loss: 2.0194 - val_acc: 0.2500\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 1.8548 - acc: 0.2400 - val_loss: 2.0046 - val_acc: 0.2433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 1.8440 - acc: 0.2417 - val_loss: 2.0064 - val_acc: 0.2400\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 1.8344 - acc: 0.2433 - val_loss: 2.0390 - val_acc: 0.2500\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 1.8236 - acc: 0.2467 - val_loss: 2.0056 - val_acc: 0.2433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 838us/step - loss: 2.2985 - acc: 0.1000 - val_loss: 2.2815 - val_acc: 0.1067\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 120us/step - loss: 2.2349 - acc: 0.1467 - val_loss: 2.1759 - val_acc: 0.1967\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 124us/step - loss: 2.1360 - acc: 0.1892 - val_loss: 2.0980 - val_acc: 0.2033\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 121us/step - loss: 2.0755 - acc: 0.2050 - val_loss: 2.0614 - val_acc: 0.2067\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 2.0408 - acc: 0.2092 - val_loss: 2.0291 - val_acc: 0.2133\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 120us/step - loss: 2.0130 - acc: 0.2075 - val_loss: 2.0143 - val_acc: 0.2167\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 1.9896 - acc: 0.2125 - val_loss: 2.0078 - val_acc: 0.2133\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 121us/step - loss: 1.9761 - acc: 0.2100 - val_loss: 1.9847 - val_acc: 0.2167\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 119us/step - loss: 1.9519 - acc: 0.2133 - val_loss: 1.9691 - val_acc: 0.2133\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 1.9346 - acc: 0.2142 - val_loss: 1.9625 - val_acc: 0.2133\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 1.9157 - acc: 0.2192 - val_loss: 1.9499 - val_acc: 0.2133\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 119us/step - loss: 1.9047 - acc: 0.2142 - val_loss: 1.9428 - val_acc: 0.2133\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 1.8843 - acc: 0.2158 - val_loss: 1.9303 - val_acc: 0.2167\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 120us/step - loss: 1.8681 - acc: 0.2192 - val_loss: 1.9234 - val_acc: 0.2200\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 120us/step - loss: 1.8501 - acc: 0.2217 - val_loss: 1.9057 - val_acc: 0.2133\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 122us/step - loss: 1.8382 - acc: 0.2192 - val_loss: 1.8923 - val_acc: 0.2133\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 121us/step - loss: 1.8198 - acc: 0.2208 - val_loss: 1.9052 - val_acc: 0.2133\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 122us/step - loss: 1.8026 - acc: 0.2242 - val_loss: 1.8967 - val_acc: 0.2200\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 120us/step - loss: 1.7862 - acc: 0.2158 - val_loss: 1.8798 - val_acc: 0.2167\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 121us/step - loss: 1.7687 - acc: 0.2233 - val_loss: 1.8814 - val_acc: 0.2133\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 1.7593 - acc: 0.2292 - val_loss: 1.8959 - val_acc: 0.2200\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 122us/step - loss: 1.7432 - acc: 0.2342 - val_loss: 1.8698 - val_acc: 0.2133\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 121us/step - loss: 1.7309 - acc: 0.2292 - val_loss: 1.8887 - val_acc: 0.2133\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 124us/step - loss: 1.7390 - acc: 0.2367 - val_loss: 1.9072 - val_acc: 0.2467\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 1.7663 - acc: 0.2317 - val_loss: 1.9168 - val_acc: 0.2067\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 119us/step - loss: 1.7083 - acc: 0.2400 - val_loss: 1.8944 - val_acc: 0.2233\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 1.6980 - acc: 0.2467 - val_loss: 1.8847 - val_acc: 0.2133\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 119us/step - loss: 1.6854 - acc: 0.2400 - val_loss: 1.8874 - val_acc: 0.2233\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 121us/step - loss: 1.6715 - acc: 0.2583 - val_loss: 1.9091 - val_acc: 0.2667\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 1.6655 - acc: 0.2600 - val_loss: 1.9147 - val_acc: 0.2600\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 809us/step - loss: 2.2763 - acc: 0.1475 - val_loss: 2.1985 - val_acc: 0.1867\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 121us/step - loss: 2.1290 - acc: 0.1842 - val_loss: 2.0313 - val_acc: 0.2067\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 122us/step - loss: 2.0277 - acc: 0.2108 - val_loss: 1.9538 - val_acc: 0.2133\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 120us/step - loss: 1.9406 - acc: 0.2217 - val_loss: 1.9111 - val_acc: 0.2167\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 128us/step - loss: 1.8426 - acc: 0.2258 - val_loss: 1.8352 - val_acc: 0.2167\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 122us/step - loss: 1.7302 - acc: 0.2442 - val_loss: 1.7591 - val_acc: 0.2667\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 1.6074 - acc: 0.3050 - val_loss: 1.6817 - val_acc: 0.3500\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 130us/step - loss: 1.4636 - acc: 0.3558 - val_loss: 1.5776 - val_acc: 0.3800\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 128us/step - loss: 1.3436 - acc: 0.3908 - val_loss: 1.4874 - val_acc: 0.4567\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 121us/step - loss: 1.2159 - acc: 0.4983 - val_loss: 1.4200 - val_acc: 0.4633\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 122us/step - loss: 1.1264 - acc: 0.5142 - val_loss: 1.4756 - val_acc: 0.5233\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 119us/step - loss: 1.0529 - acc: 0.5433 - val_loss: 1.3790 - val_acc: 0.4833\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 0.9723 - acc: 0.5608 - val_loss: 1.4416 - val_acc: 0.5333\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 0.9374 - acc: 0.5667 - val_loss: 1.3957 - val_acc: 0.5133\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 128us/step - loss: 0.8721 - acc: 0.6000 - val_loss: 1.4061 - val_acc: 0.5433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 121us/step - loss: 0.8588 - acc: 0.6208 - val_loss: 1.3847 - val_acc: 0.5300\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 121us/step - loss: 0.8298 - acc: 0.6350 - val_loss: 1.4466 - val_acc: 0.5533\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 122us/step - loss: 0.7882 - acc: 0.6475 - val_loss: 1.5420 - val_acc: 0.5300\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 123us/step - loss: 0.7763 - acc: 0.6508 - val_loss: 1.4794 - val_acc: 0.5600\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 0.7436 - acc: 0.6975 - val_loss: 1.3796 - val_acc: 0.5733\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 121us/step - loss: 0.7001 - acc: 0.7033 - val_loss: 1.4430 - val_acc: 0.5567\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 123us/step - loss: 0.6733 - acc: 0.7425 - val_loss: 1.4136 - val_acc: 0.5800\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 0.6448 - acc: 0.7508 - val_loss: 1.5331 - val_acc: 0.5867\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 120us/step - loss: 0.6136 - acc: 0.7675 - val_loss: 1.5468 - val_acc: 0.5833\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 123us/step - loss: 0.6129 - acc: 0.7600 - val_loss: 1.5004 - val_acc: 0.6000\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 119us/step - loss: 0.5658 - acc: 0.7875 - val_loss: 1.5699 - val_acc: 0.6000\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 124us/step - loss: 0.5502 - acc: 0.7950 - val_loss: 1.7048 - val_acc: 0.5933\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 0.5310 - acc: 0.7933 - val_loss: 1.5592 - val_acc: 0.6200\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 0.4807 - acc: 0.8258 - val_loss: 1.6054 - val_acc: 0.6033\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 121us/step - loss: 0.4502 - acc: 0.8342 - val_loss: 1.5838 - val_acc: 0.6367\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 935us/step - loss: 2.2269 - acc: 0.1358 - val_loss: 2.0440 - val_acc: 0.2067\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 145us/step - loss: 1.9736 - acc: 0.2992 - val_loss: 1.7517 - val_acc: 0.3933\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 141us/step - loss: 1.4228 - acc: 0.4475 - val_loss: 1.2135 - val_acc: 0.4867\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 151us/step - loss: 0.8954 - acc: 0.6833 - val_loss: 0.7917 - val_acc: 0.7200\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 168us/step - loss: 0.5719 - acc: 0.8150 - val_loss: 0.8128 - val_acc: 0.7633\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 152us/step - loss: 0.4534 - acc: 0.8608 - val_loss: 0.7407 - val_acc: 0.7767\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 175us/step - loss: 0.3579 - acc: 0.8900 - val_loss: 0.5589 - val_acc: 0.8267\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 160us/step - loss: 0.2858 - acc: 0.9100 - val_loss: 0.6115 - val_acc: 0.8300\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 162us/step - loss: 0.2510 - acc: 0.9242 - val_loss: 0.6304 - val_acc: 0.8467\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 143us/step - loss: 0.1977 - acc: 0.9417 - val_loss: 0.6841 - val_acc: 0.8200\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 182us/step - loss: 0.1684 - acc: 0.9508 - val_loss: 0.5810 - val_acc: 0.8733\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 170us/step - loss: 0.1595 - acc: 0.9542 - val_loss: 0.6009 - val_acc: 0.8733\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 145us/step - loss: 0.1213 - acc: 0.9667 - val_loss: 0.6175 - val_acc: 0.8533\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 148us/step - loss: 0.1071 - acc: 0.9683 - val_loss: 0.5666 - val_acc: 0.8800\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 149us/step - loss: 0.1449 - acc: 0.9542 - val_loss: 0.7569 - val_acc: 0.8533\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 143us/step - loss: 0.0869 - acc: 0.9742 - val_loss: 0.6237 - val_acc: 0.8800\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 132us/step - loss: 0.0544 - acc: 0.9858 - val_loss: 0.7543 - val_acc: 0.8633\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 136us/step - loss: 0.0705 - acc: 0.9817 - val_loss: 0.7625 - val_acc: 0.8600\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 144us/step - loss: 0.0386 - acc: 0.9908 - val_loss: 0.7157 - val_acc: 0.8700\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 147us/step - loss: 0.0353 - acc: 0.9933 - val_loss: 0.9113 - val_acc: 0.8467\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 147us/step - loss: 0.0471 - acc: 0.9858 - val_loss: 0.7963 - val_acc: 0.8533\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 154us/step - loss: 0.0418 - acc: 0.9867 - val_loss: 0.8124 - val_acc: 0.8733\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 138us/step - loss: 0.0334 - acc: 0.9917 - val_loss: 0.8445 - val_acc: 0.8600\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 150us/step - loss: 0.0400 - acc: 0.9900 - val_loss: 0.8191 - val_acc: 0.8600\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 141us/step - loss: 0.0251 - acc: 0.9933 - val_loss: 0.9036 - val_acc: 0.8567\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 148us/step - loss: 0.0236 - acc: 0.9933 - val_loss: 0.8916 - val_acc: 0.8667\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 146us/step - loss: 0.0291 - acc: 0.9892 - val_loss: 0.8250 - val_acc: 0.8700\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 137us/step - loss: 0.0370 - acc: 0.9892 - val_loss: 0.9613 - val_acc: 0.8567\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 139us/step - loss: 0.0731 - acc: 0.9767 - val_loss: 0.7498 - val_acc: 0.8500\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 159us/step - loss: 0.0557 - acc: 0.9833 - val_loss: 0.9113 - val_acc: 0.8567\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 734us/step - loss: 2.3233 - acc: 0.1017 - val_loss: 2.3109 - val_acc: 0.1167\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 2.3161 - acc: 0.1017 - val_loss: 2.3070 - val_acc: 0.1167\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 2.3107 - acc: 0.1017 - val_loss: 2.3043 - val_acc: 0.1167\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 2.3071 - acc: 0.1017 - val_loss: 2.3031 - val_acc: 0.1167\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 2.3045 - acc: 0.1167 - val_loss: 2.3022 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.3026 - acc: 0.1192 - val_loss: 2.3022 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.3013 - acc: 0.1192 - val_loss: 2.3016 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.3006 - acc: 0.1192 - val_loss: 2.3007 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - ETA: 0s - loss: 2.2997 - acc: 0.123 - 0s 68us/step - loss: 2.2998 - acc: 0.1192 - val_loss: 2.3009 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2994 - acc: 0.1192 - val_loss: 2.3008 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.2992 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2988 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3018 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3016 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2978 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3017 - val_acc: 0.1433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2978 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2978 - acc: 0.1192 - val_loss: 2.3005 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2976 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2972 - acc: 0.1192 - val_loss: 2.3004 - val_acc: 0.1433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2973 - acc: 0.1192 - val_loss: 2.2994 - val_acc: 0.1433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 760us/step - loss: 2.3574 - acc: 0.1017 - val_loss: 2.3322 - val_acc: 0.1167\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 2.3344 - acc: 0.1017 - val_loss: 2.3167 - val_acc: 0.1167\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.3210 - acc: 0.1017 - val_loss: 2.3092 - val_acc: 0.1167\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.3133 - acc: 0.1017 - val_loss: 2.3055 - val_acc: 0.1167\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.3081 - acc: 0.1017 - val_loss: 2.3028 - val_acc: 0.1167\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.3047 - acc: 0.1017 - val_loss: 2.3007 - val_acc: 0.1167\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.3026 - acc: 0.1017 - val_loss: 2.2994 - val_acc: 0.1167\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.3010 - acc: 0.1017 - val_loss: 2.2996 - val_acc: 0.1167\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.3003 - acc: 0.1017 - val_loss: 2.2995 - val_acc: 0.1167\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2995 - acc: 0.0950 - val_loss: 2.2997 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2993 - acc: 0.1192 - val_loss: 2.2998 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2988 - acc: 0.1192 - val_loss: 2.3002 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2988 - acc: 0.1192 - val_loss: 2.3004 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3008 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3007 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3017 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3019 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3019 - val_acc: 0.1433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2978 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2977 - acc: 0.1192 - val_loss: 2.3009 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2975 - acc: 0.1192 - val_loss: 2.3016 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.2975 - acc: 0.1192 - val_loss: 2.3007 - val_acc: 0.1433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2972 - acc: 0.1192 - val_loss: 2.3005 - val_acc: 0.1433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 703us/step - loss: 2.4052 - acc: 0.0983 - val_loss: 2.3343 - val_acc: 0.1067\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.3525 - acc: 0.0983 - val_loss: 2.3110 - val_acc: 0.1067\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 2.3265 - acc: 0.0983 - val_loss: 2.3013 - val_acc: 0.1067\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.3127 - acc: 0.0983 - val_loss: 2.2982 - val_acc: 0.1067\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.3060 - acc: 0.1042 - val_loss: 2.2965 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.3027 - acc: 0.1192 - val_loss: 2.2978 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.3006 - acc: 0.1192 - val_loss: 2.2982 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2994 - acc: 0.1192 - val_loss: 2.2982 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2992 - acc: 0.1192 - val_loss: 2.2991 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2988 - acc: 0.1192 - val_loss: 2.3005 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2991 - acc: 0.1192 - val_loss: 2.3009 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3021 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2990 - acc: 0.1192 - val_loss: 2.3019 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2992 - acc: 0.1192 - val_loss: 2.3017 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 60us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3004 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3008 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2967 - acc: 0.1192 - val_loss: 2.2992 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2942 - acc: 0.1192 - val_loss: 2.2962 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2895 - acc: 0.1192 - val_loss: 2.2908 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2804 - acc: 0.1192 - val_loss: 2.2803 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2647 - acc: 0.1192 - val_loss: 2.2620 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2397 - acc: 0.1758 - val_loss: 2.2355 - val_acc: 0.1767\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.2014 - acc: 0.1792 - val_loss: 2.2004 - val_acc: 0.1800\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.1567 - acc: 0.1800 - val_loss: 2.1604 - val_acc: 0.1833\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.1114 - acc: 0.1775 - val_loss: 2.1197 - val_acc: 0.1867\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.0690 - acc: 0.1800 - val_loss: 2.0900 - val_acc: 0.1800\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.0275 - acc: 0.1792 - val_loss: 2.0763 - val_acc: 0.1667\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.9956 - acc: 0.1750 - val_loss: 2.0453 - val_acc: 0.1700\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 1.9608 - acc: 0.1767 - val_loss: 2.0290 - val_acc: 0.1467\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 755us/step - loss: 2.4379 - acc: 0.0983 - val_loss: 2.3223 - val_acc: 0.1067\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.3121 - acc: 0.0942 - val_loss: 2.3047 - val_acc: 0.1433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 2.3024 - acc: 0.1192 - val_loss: 2.3028 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 2.3011 - acc: 0.1108 - val_loss: 2.3019 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.3012 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 94us/step - loss: 2.3007 - acc: 0.1117 - val_loss: 2.3058 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 2.3001 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 2.3010 - acc: 0.1192 - val_loss: 2.3025 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 2.3010 - acc: 0.1108 - val_loss: 2.3034 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 2.3027 - acc: 0.1192 - val_loss: 2.3004 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.2997 - acc: 0.1192 - val_loss: 2.3035 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 2.3001 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 2.3004 - acc: 0.1192 - val_loss: 2.3025 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.2976 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.2883 - acc: 0.1458 - val_loss: 2.2766 - val_acc: 0.2000\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.2563 - acc: 0.1917 - val_loss: 2.2198 - val_acc: 0.1967\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 2.1921 - acc: 0.2050 - val_loss: 2.1375 - val_acc: 0.2400\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.1149 - acc: 0.2050 - val_loss: 2.1039 - val_acc: 0.2233\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.0598 - acc: 0.1908 - val_loss: 2.0039 - val_acc: 0.2000\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.0201 - acc: 0.1958 - val_loss: 1.9699 - val_acc: 0.2300\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.9789 - acc: 0.1908 - val_loss: 2.0057 - val_acc: 0.1900\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.9912 - acc: 0.2067 - val_loss: 1.9439 - val_acc: 0.2067\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 1.9735 - acc: 0.2000 - val_loss: 1.9680 - val_acc: 0.2300\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.9809 - acc: 0.1992 - val_loss: 1.9124 - val_acc: 0.2067\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 1.9697 - acc: 0.2008 - val_loss: 1.9347 - val_acc: 0.2333\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.9598 - acc: 0.1983 - val_loss: 1.9983 - val_acc: 0.2267\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.9789 - acc: 0.1992 - val_loss: 1.9586 - val_acc: 0.2333\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 1.9711 - acc: 0.2042 - val_loss: 1.9818 - val_acc: 0.1933\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.9676 - acc: 0.2008 - val_loss: 1.9121 - val_acc: 0.2033\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 1.9626 - acc: 0.2075 - val_loss: 1.9916 - val_acc: 0.1933\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 873us/step - loss: 2.2999 - acc: 0.1417 - val_loss: 2.2942 - val_acc: 0.1300\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 2.2776 - acc: 0.1725 - val_loss: 2.2549 - val_acc: 0.1467\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 2.2146 - acc: 0.1850 - val_loss: 2.1749 - val_acc: 0.1933\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 2.1224 - acc: 0.2033 - val_loss: 2.0941 - val_acc: 0.2233\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 2.0433 - acc: 0.2167 - val_loss: 2.0258 - val_acc: 0.2267\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.9774 - acc: 0.2167 - val_loss: 1.9634 - val_acc: 0.2267\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.9286 - acc: 0.2192 - val_loss: 1.9277 - val_acc: 0.2367\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.8919 - acc: 0.2233 - val_loss: 1.9031 - val_acc: 0.2300\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.8623 - acc: 0.2208 - val_loss: 1.8654 - val_acc: 0.2333\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.8348 - acc: 0.2292 - val_loss: 1.8486 - val_acc: 0.2267\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.8132 - acc: 0.2258 - val_loss: 1.8271 - val_acc: 0.2233\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 1.7869 - acc: 0.2317 - val_loss: 1.8194 - val_acc: 0.2267\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.7688 - acc: 0.2308 - val_loss: 1.8197 - val_acc: 0.2367\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.7478 - acc: 0.2375 - val_loss: 1.8034 - val_acc: 0.2333\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.7302 - acc: 0.2442 - val_loss: 1.7859 - val_acc: 0.2367\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 1.7138 - acc: 0.2450 - val_loss: 1.7762 - val_acc: 0.2400\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.7006 - acc: 0.2408 - val_loss: 1.7907 - val_acc: 0.2400\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.6835 - acc: 0.2667 - val_loss: 1.7719 - val_acc: 0.2700\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 1.6685 - acc: 0.2725 - val_loss: 1.7645 - val_acc: 0.2833\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 1.6551 - acc: 0.2917 - val_loss: 1.7563 - val_acc: 0.2900\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 1.6407 - acc: 0.2967 - val_loss: 1.7927 - val_acc: 0.2933\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 1.6340 - acc: 0.3150 - val_loss: 1.7703 - val_acc: 0.3033\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.6173 - acc: 0.3208 - val_loss: 1.7657 - val_acc: 0.3433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.6052 - acc: 0.3383 - val_loss: 1.7457 - val_acc: 0.3533\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.5939 - acc: 0.3417 - val_loss: 1.7354 - val_acc: 0.3567\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.5823 - acc: 0.3483 - val_loss: 1.7368 - val_acc: 0.3767\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 1.5689 - acc: 0.3633 - val_loss: 1.7399 - val_acc: 0.3967\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 1.5581 - acc: 0.3750 - val_loss: 1.7518 - val_acc: 0.3833\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 1.5451 - acc: 0.3733 - val_loss: 1.7357 - val_acc: 0.4000\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 1.5318 - acc: 0.3758 - val_loss: 1.7281 - val_acc: 0.4233\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 933us/step - loss: 2.2990 - acc: 0.1042 - val_loss: 2.2868 - val_acc: 0.0767\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 2.2395 - acc: 0.1883 - val_loss: 2.1634 - val_acc: 0.1800\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 2.1070 - acc: 0.2075 - val_loss: 2.0385 - val_acc: 0.2100\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 95us/step - loss: 2.0206 - acc: 0.2217 - val_loss: 1.9926 - val_acc: 0.2100\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.9718 - acc: 0.2192 - val_loss: 1.9556 - val_acc: 0.2100\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.9249 - acc: 0.2242 - val_loss: 1.9287 - val_acc: 0.2167\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.8873 - acc: 0.2250 - val_loss: 1.8727 - val_acc: 0.2233\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.8443 - acc: 0.2308 - val_loss: 1.8500 - val_acc: 0.2233\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 1.8103 - acc: 0.2350 - val_loss: 1.8448 - val_acc: 0.2300\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 1.7665 - acc: 0.2433 - val_loss: 1.7815 - val_acc: 0.2433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 1.7079 - acc: 0.2708 - val_loss: 1.7503 - val_acc: 0.2433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.6642 - acc: 0.2967 - val_loss: 1.7190 - val_acc: 0.2700\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 1.6105 - acc: 0.3100 - val_loss: 1.6962 - val_acc: 0.2800\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 95us/step - loss: 1.5478 - acc: 0.3283 - val_loss: 1.7603 - val_acc: 0.2800\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 1.5024 - acc: 0.3525 - val_loss: 1.6560 - val_acc: 0.2900\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 1.4411 - acc: 0.3658 - val_loss: 1.6328 - val_acc: 0.3367\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.3887 - acc: 0.4125 - val_loss: 1.6772 - val_acc: 0.3433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 1.3522 - acc: 0.4158 - val_loss: 1.6528 - val_acc: 0.3567\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.3020 - acc: 0.4525 - val_loss: 1.7410 - val_acc: 0.3433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.2767 - acc: 0.4783 - val_loss: 1.7597 - val_acc: 0.3533\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.2381 - acc: 0.5058 - val_loss: 1.7018 - val_acc: 0.4300\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 1.2206 - acc: 0.5275 - val_loss: 1.7316 - val_acc: 0.4667\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.1974 - acc: 0.5158 - val_loss: 1.7567 - val_acc: 0.4633\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.1759 - acc: 0.5533 - val_loss: 1.7176 - val_acc: 0.4100\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 1.1618 - acc: 0.5692 - val_loss: 1.7636 - val_acc: 0.4233\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 1.1694 - acc: 0.5433 - val_loss: 1.7550 - val_acc: 0.4567\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 1.1200 - acc: 0.5867 - val_loss: 1.7354 - val_acc: 0.4700\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 1.0939 - acc: 0.6100 - val_loss: 1.7564 - val_acc: 0.4800\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 1.0751 - acc: 0.6417 - val_loss: 1.7864 - val_acc: 0.5133\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.0424 - acc: 0.6567 - val_loss: 1.8277 - val_acc: 0.5333\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 977us/step - loss: 2.2379 - acc: 0.1592 - val_loss: 2.1697 - val_acc: 0.1700\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 97us/step - loss: 2.1053 - acc: 0.2083 - val_loss: 2.0981 - val_acc: 0.1733\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 97us/step - loss: 2.0203 - acc: 0.2608 - val_loss: 2.0376 - val_acc: 0.2500\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 1.9232 - acc: 0.2975 - val_loss: 1.9151 - val_acc: 0.2567\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 1.6963 - acc: 0.2975 - val_loss: 1.7440 - val_acc: 0.2333\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 97us/step - loss: 1.5060 - acc: 0.2942 - val_loss: 1.6949 - val_acc: 0.2867\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.4108 - acc: 0.3850 - val_loss: 1.7051 - val_acc: 0.3400\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 97us/step - loss: 1.3143 - acc: 0.4450 - val_loss: 1.5215 - val_acc: 0.4500\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 1.2294 - acc: 0.5183 - val_loss: 1.4967 - val_acc: 0.4900\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 1.1016 - acc: 0.5692 - val_loss: 1.4474 - val_acc: 0.5467\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 95us/step - loss: 0.9563 - acc: 0.6458 - val_loss: 1.4289 - val_acc: 0.5967\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 0.8833 - acc: 0.6642 - val_loss: 1.4331 - val_acc: 0.6033\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 0.8145 - acc: 0.6975 - val_loss: 1.3621 - val_acc: 0.5967\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 0.7417 - acc: 0.7358 - val_loss: 1.2966 - val_acc: 0.7067\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 97us/step - loss: 0.6966 - acc: 0.7658 - val_loss: 1.2529 - val_acc: 0.6600\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 97us/step - loss: 0.6495 - acc: 0.7600 - val_loss: 1.2807 - val_acc: 0.6700\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 0.6034 - acc: 0.8025 - val_loss: 1.2415 - val_acc: 0.6667\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 0.5578 - acc: 0.8150 - val_loss: 1.2919 - val_acc: 0.7167\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 0.5075 - acc: 0.8258 - val_loss: 1.3746 - val_acc: 0.7133\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 97us/step - loss: 0.4664 - acc: 0.8500 - val_loss: 1.2790 - val_acc: 0.7133\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 0.4355 - acc: 0.8667 - val_loss: 1.3018 - val_acc: 0.7433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 0.3969 - acc: 0.8758 - val_loss: 1.3141 - val_acc: 0.7167\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.3724 - acc: 0.8867 - val_loss: 1.2782 - val_acc: 0.7300\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 0.3405 - acc: 0.8942 - val_loss: 1.3115 - val_acc: 0.7633\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 97us/step - loss: 0.3194 - acc: 0.9100 - val_loss: 1.2811 - val_acc: 0.7600\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 0.2855 - acc: 0.9158 - val_loss: 1.3606 - val_acc: 0.7667\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 97us/step - loss: 0.3030 - acc: 0.9058 - val_loss: 1.3170 - val_acc: 0.7800\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 97us/step - loss: 0.3010 - acc: 0.9142 - val_loss: 1.3372 - val_acc: 0.7433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 0.3010 - acc: 0.9250 - val_loss: 1.3351 - val_acc: 0.7767\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 0.3062 - acc: 0.9150 - val_loss: 1.4115 - val_acc: 0.7500\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "  64/1200 [>.............................] - ETA: 45s - loss: 2.3026 - acc: 0.1094 WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.133867). Check your callbacks.\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2057 - acc: 0.1983 - val_loss: 1.9330 - val_acc: 0.2833\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 164us/step - loss: 1.6345 - acc: 0.3783 - val_loss: 1.3818 - val_acc: 0.4467\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 119us/step - loss: 1.1353 - acc: 0.6042 - val_loss: 0.9907 - val_acc: 0.6367\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 0.8987 - acc: 0.6917 - val_loss: 0.9263 - val_acc: 0.6933\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 0.6861 - acc: 0.8033 - val_loss: 0.8660 - val_acc: 0.7100\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 119us/step - loss: 0.5720 - acc: 0.8242 - val_loss: 0.7667 - val_acc: 0.7667\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 122us/step - loss: 0.4943 - acc: 0.8467 - val_loss: 0.7033 - val_acc: 0.8067\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 121us/step - loss: 0.4105 - acc: 0.8750 - val_loss: 0.8639 - val_acc: 0.7467\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 0.4118 - acc: 0.8742 - val_loss: 0.6067 - val_acc: 0.8167\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 0.3029 - acc: 0.9083 - val_loss: 0.6787 - val_acc: 0.8233\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 0.2933 - acc: 0.9083 - val_loss: 0.7223 - val_acc: 0.8200\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 130us/step - loss: 0.2362 - acc: 0.9283 - val_loss: 0.8131 - val_acc: 0.7800\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 128us/step - loss: 0.2145 - acc: 0.9325 - val_loss: 0.6672 - val_acc: 0.8200\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 0.2038 - acc: 0.9375 - val_loss: 0.8701 - val_acc: 0.7767\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 127us/step - loss: 0.1498 - acc: 0.9600 - val_loss: 0.7629 - val_acc: 0.8233\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 0.1235 - acc: 0.9633 - val_loss: 0.8223 - val_acc: 0.8300\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 123us/step - loss: 0.1047 - acc: 0.9733 - val_loss: 0.7207 - val_acc: 0.8233\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 120us/step - loss: 0.0911 - acc: 0.9792 - val_loss: 0.7750 - val_acc: 0.8167\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 155us/step - loss: 0.0954 - acc: 0.9725 - val_loss: 0.6991 - val_acc: 0.8333\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 118us/step - loss: 0.0881 - acc: 0.9658 - val_loss: 0.9246 - val_acc: 0.8333\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 172us/step - loss: 0.0666 - acc: 0.9825 - val_loss: 0.8510 - val_acc: 0.8400\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 0.0498 - acc: 0.9858 - val_loss: 0.8851 - val_acc: 0.8367\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 151us/step - loss: 0.0319 - acc: 0.9917 - val_loss: 0.8689 - val_acc: 0.8467\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 0.0245 - acc: 0.9958 - val_loss: 0.8176 - val_acc: 0.8467\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 122us/step - loss: 0.0238 - acc: 0.9942 - val_loss: 0.8960 - val_acc: 0.8500\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 117us/step - loss: 0.0479 - acc: 0.9867 - val_loss: 0.9271 - val_acc: 0.8300\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 0.0354 - acc: 0.9875 - val_loss: 1.1400 - val_acc: 0.8000\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 116us/step - loss: 0.1030 - acc: 0.9650 - val_loss: 0.9826 - val_acc: 0.8467\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 0.0903 - acc: 0.9675 - val_loss: 1.0135 - val_acc: 0.8167\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 0.0548 - acc: 0.9842 - val_loss: 0.8980 - val_acc: 0.8500\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3009 - acc: 0.1125 - val_loss: 2.2980 - val_acc: 0.1433\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 123us/step - loss: 2.2860 - acc: 0.1083 - val_loss: 2.2602 - val_acc: 0.0933\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 129us/step - loss: 2.2230 - acc: 0.1033 - val_loss: 2.1748 - val_acc: 0.1667\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 2.1564 - acc: 0.1975 - val_loss: 2.1056 - val_acc: 0.2100\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 193us/step - loss: 2.1047 - acc: 0.2092 - val_loss: 2.0566 - val_acc: 0.2167\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 164us/step - loss: 2.0629 - acc: 0.2200 - val_loss: 2.0150 - val_acc: 0.2100\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 128us/step - loss: 2.0266 - acc: 0.2183 - val_loss: 1.9832 - val_acc: 0.2200\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 2.0014 - acc: 0.2242 - val_loss: 1.9629 - val_acc: 0.2133\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 1.9806 - acc: 0.2208 - val_loss: 1.9445 - val_acc: 0.2233\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 124us/step - loss: 1.9622 - acc: 0.2242 - val_loss: 1.9319 - val_acc: 0.2167\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 129us/step - loss: 1.9478 - acc: 0.2267 - val_loss: 1.9217 - val_acc: 0.2433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 1.9312 - acc: 0.2275 - val_loss: 1.9138 - val_acc: 0.2400\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 130us/step - loss: 1.9186 - acc: 0.2317 - val_loss: 1.9024 - val_acc: 0.2300\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 128us/step - loss: 1.9059 - acc: 0.2333 - val_loss: 1.8953 - val_acc: 0.2200\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 1.8948 - acc: 0.2375 - val_loss: 1.8923 - val_acc: 0.2233\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 124us/step - loss: 1.8833 - acc: 0.2342 - val_loss: 1.8824 - val_acc: 0.2533\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 128us/step - loss: 1.8716 - acc: 0.2383 - val_loss: 1.8797 - val_acc: 0.2400\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 1.8655 - acc: 0.2467 - val_loss: 1.8803 - val_acc: 0.2400\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 130us/step - loss: 1.8526 - acc: 0.2300 - val_loss: 1.8688 - val_acc: 0.2367\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 1.8415 - acc: 0.2367 - val_loss: 1.8674 - val_acc: 0.2200\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 124us/step - loss: 1.8315 - acc: 0.2483 - val_loss: 1.8663 - val_acc: 0.2400\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 167us/step - loss: 1.8251 - acc: 0.2450 - val_loss: 1.8646 - val_acc: 0.2433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 129us/step - loss: 1.8178 - acc: 0.2500 - val_loss: 1.8650 - val_acc: 0.2433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 1.8068 - acc: 0.2558 - val_loss: 1.8498 - val_acc: 0.2200\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 127us/step - loss: 1.7968 - acc: 0.2533 - val_loss: 1.8516 - val_acc: 0.2400\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 124us/step - loss: 1.7871 - acc: 0.2525 - val_loss: 1.8480 - val_acc: 0.2300\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 1.7797 - acc: 0.2467 - val_loss: 1.8500 - val_acc: 0.2300\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 1.7749 - acc: 0.2508 - val_loss: 1.8413 - val_acc: 0.2167\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 1.7647 - acc: 0.2675 - val_loss: 1.8380 - val_acc: 0.2300\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 129us/step - loss: 1.7548 - acc: 0.2642 - val_loss: 1.8357 - val_acc: 0.2233\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 823us/step - loss: 2.2756 - acc: 0.1183 - val_loss: 2.2452 - val_acc: 0.1400\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 124us/step - loss: 2.1697 - acc: 0.1692 - val_loss: 2.1718 - val_acc: 0.1967\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 129us/step - loss: 2.0906 - acc: 0.1950 - val_loss: 2.1394 - val_acc: 0.2033\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 2.0367 - acc: 0.2108 - val_loss: 2.0826 - val_acc: 0.2000\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 1.9990 - acc: 0.2133 - val_loss: 2.0679 - val_acc: 0.2233\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 124us/step - loss: 1.9684 - acc: 0.2217 - val_loss: 2.0555 - val_acc: 0.2233\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 1.9445 - acc: 0.2208 - val_loss: 2.0173 - val_acc: 0.2200\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 128us/step - loss: 1.9173 - acc: 0.2233 - val_loss: 2.0346 - val_acc: 0.2233\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 128us/step - loss: 1.8947 - acc: 0.2183 - val_loss: 2.0058 - val_acc: 0.2333\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 1.8715 - acc: 0.2225 - val_loss: 1.9897 - val_acc: 0.2333\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 123us/step - loss: 1.8498 - acc: 0.2350 - val_loss: 1.9967 - val_acc: 0.2467\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 1.8257 - acc: 0.2383 - val_loss: 1.9766 - val_acc: 0.2400\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 1.7981 - acc: 0.2742 - val_loss: 1.9855 - val_acc: 0.2267\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 127us/step - loss: 1.7589 - acc: 0.2692 - val_loss: 1.9653 - val_acc: 0.1967\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 123us/step - loss: 1.7006 - acc: 0.2792 - val_loss: 1.8851 - val_acc: 0.2700\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 1.6354 - acc: 0.2983 - val_loss: 1.8444 - val_acc: 0.2433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 1.5980 - acc: 0.2875 - val_loss: 1.7812 - val_acc: 0.2433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 130us/step - loss: 1.5361 - acc: 0.2967 - val_loss: 1.8471 - val_acc: 0.2800\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 128us/step - loss: 1.4902 - acc: 0.3067 - val_loss: 1.7931 - val_acc: 0.2433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 127us/step - loss: 1.4521 - acc: 0.3100 - val_loss: 1.7447 - val_acc: 0.2800\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 1.4176 - acc: 0.3100 - val_loss: 1.6914 - val_acc: 0.3067\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 1.3824 - acc: 0.3333 - val_loss: 1.7384 - val_acc: 0.3467\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 1.3519 - acc: 0.3442 - val_loss: 1.7844 - val_acc: 0.3333\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 135us/step - loss: 1.3640 - acc: 0.3425 - val_loss: 1.7538 - val_acc: 0.3367\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 137us/step - loss: 1.3124 - acc: 0.3850 - val_loss: 1.7309 - val_acc: 0.3967\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 1.3074 - acc: 0.3800 - val_loss: 1.7172 - val_acc: 0.3567\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 128us/step - loss: 1.2996 - acc: 0.3692 - val_loss: 1.6703 - val_acc: 0.3700\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 1.2766 - acc: 0.4067 - val_loss: 1.7061 - val_acc: 0.3900\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 1.2333 - acc: 0.4125 - val_loss: 1.6356 - val_acc: 0.3900\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 1.2101 - acc: 0.4458 - val_loss: 1.7580 - val_acc: 0.4267\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 906us/step - loss: 2.2855 - acc: 0.1025 - val_loss: 2.2616 - val_acc: 0.1867\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 129us/step - loss: 2.1666 - acc: 0.2025 - val_loss: 2.1502 - val_acc: 0.2100\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 154us/step - loss: 2.0429 - acc: 0.2125 - val_loss: 2.0303 - val_acc: 0.2333\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 136us/step - loss: 1.9416 - acc: 0.2158 - val_loss: 1.9229 - val_acc: 0.2667\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 157us/step - loss: 1.8350 - acc: 0.2283 - val_loss: 1.8114 - val_acc: 0.2333\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 151us/step - loss: 1.7404 - acc: 0.2425 - val_loss: 1.7324 - val_acc: 0.2300\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 1.6568 - acc: 0.2692 - val_loss: 1.6541 - val_acc: 0.3000\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 128us/step - loss: 1.5894 - acc: 0.2975 - val_loss: 1.6549 - val_acc: 0.2733\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 1.5236 - acc: 0.3267 - val_loss: 1.5790 - val_acc: 0.3033\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 1.4390 - acc: 0.3708 - val_loss: 1.5547 - val_acc: 0.3200\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 132us/step - loss: 1.3542 - acc: 0.3875 - val_loss: 1.4849 - val_acc: 0.3500\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 1.2351 - acc: 0.4425 - val_loss: 1.5945 - val_acc: 0.3767\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 127us/step - loss: 1.1537 - acc: 0.4892 - val_loss: 1.4440 - val_acc: 0.3933\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 123us/step - loss: 1.0422 - acc: 0.5617 - val_loss: 1.4838 - val_acc: 0.4867\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 124us/step - loss: 0.9700 - acc: 0.6233 - val_loss: 1.5365 - val_acc: 0.4733\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 0.8870 - acc: 0.6583 - val_loss: 1.2959 - val_acc: 0.5633\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 0.8361 - acc: 0.6758 - val_loss: 1.4173 - val_acc: 0.5733\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 124us/step - loss: 0.7599 - acc: 0.7408 - val_loss: 1.3150 - val_acc: 0.5700\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 128us/step - loss: 0.7209 - acc: 0.7408 - val_loss: 1.3406 - val_acc: 0.6567\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 0.6866 - acc: 0.7567 - val_loss: 1.3862 - val_acc: 0.6233\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 0.6277 - acc: 0.7742 - val_loss: 1.2958 - val_acc: 0.6500\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 0.5888 - acc: 0.7925 - val_loss: 1.3087 - val_acc: 0.6533\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 127us/step - loss: 0.5191 - acc: 0.7967 - val_loss: 1.4737 - val_acc: 0.6767\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 127us/step - loss: 0.5190 - acc: 0.8000 - val_loss: 1.4320 - val_acc: 0.6800\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 130us/step - loss: 0.5134 - acc: 0.8183 - val_loss: 1.3796 - val_acc: 0.6333\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 0.4864 - acc: 0.8192 - val_loss: 1.3656 - val_acc: 0.6733\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 127us/step - loss: 0.4087 - acc: 0.8533 - val_loss: 1.4228 - val_acc: 0.6867\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 123us/step - loss: 0.4128 - acc: 0.8583 - val_loss: 1.4281 - val_acc: 0.6700\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 123us/step - loss: 0.3824 - acc: 0.8558 - val_loss: 1.3394 - val_acc: 0.7133\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 125us/step - loss: 0.3502 - acc: 0.8808 - val_loss: 1.3670 - val_acc: 0.6967\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 1s 860us/step - loss: 2.2650 - acc: 0.1250 - val_loss: 2.1109 - val_acc: 0.2633\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 157us/step - loss: 1.9486 - acc: 0.2417 - val_loss: 1.6601 - val_acc: 0.4100\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 174us/step - loss: 1.4609 - acc: 0.4567 - val_loss: 1.3204 - val_acc: 0.5633\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 165us/step - loss: 1.0158 - acc: 0.6358 - val_loss: 1.0177 - val_acc: 0.6767\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 154us/step - loss: 0.7700 - acc: 0.7308 - val_loss: 0.8149 - val_acc: 0.7600\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 157us/step - loss: 0.5606 - acc: 0.8242 - val_loss: 0.7580 - val_acc: 0.7833\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 160us/step - loss: 0.4260 - acc: 0.8650 - val_loss: 0.7820 - val_acc: 0.8100\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 159us/step - loss: 0.3570 - acc: 0.8917 - val_loss: 0.7524 - val_acc: 0.8200\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 149us/step - loss: 0.2653 - acc: 0.9250 - val_loss: 0.7755 - val_acc: 0.8400\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 156us/step - loss: 0.2509 - acc: 0.9258 - val_loss: 0.8231 - val_acc: 0.8067\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 161us/step - loss: 0.2782 - acc: 0.9083 - val_loss: 0.7851 - val_acc: 0.8267\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 167us/step - loss: 0.1683 - acc: 0.9600 - val_loss: 0.8154 - val_acc: 0.8467\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 156us/step - loss: 0.1771 - acc: 0.9425 - val_loss: 0.9044 - val_acc: 0.8367\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 155us/step - loss: 0.1408 - acc: 0.9608 - val_loss: 0.8291 - val_acc: 0.8400\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 165us/step - loss: 0.1108 - acc: 0.9667 - val_loss: 0.9336 - val_acc: 0.8300\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 162us/step - loss: 0.0886 - acc: 0.9750 - val_loss: 0.9211 - val_acc: 0.8233\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 158us/step - loss: 0.0681 - acc: 0.9833 - val_loss: 0.8998 - val_acc: 0.8533\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 160us/step - loss: 0.0641 - acc: 0.9842 - val_loss: 0.9135 - val_acc: 0.8400\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 157us/step - loss: 0.0721 - acc: 0.9817 - val_loss: 0.8946 - val_acc: 0.8467\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 170us/step - loss: 0.0467 - acc: 0.9892 - val_loss: 1.1974 - val_acc: 0.8300\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 152us/step - loss: 0.1036 - acc: 0.9650 - val_loss: 1.0434 - val_acc: 0.8267\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 156us/step - loss: 0.0652 - acc: 0.9817 - val_loss: 1.0448 - val_acc: 0.8467\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 152us/step - loss: 0.0711 - acc: 0.9733 - val_loss: 1.1556 - val_acc: 0.8333\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 154us/step - loss: 0.0497 - acc: 0.9850 - val_loss: 1.0566 - val_acc: 0.8467\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 156us/step - loss: 0.0372 - acc: 0.9908 - val_loss: 1.1495 - val_acc: 0.8300\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 161us/step - loss: 0.0228 - acc: 0.9967 - val_loss: 1.1781 - val_acc: 0.8233\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 159us/step - loss: 0.0194 - acc: 0.9958 - val_loss: 1.0959 - val_acc: 0.8333\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 157us/step - loss: 0.0141 - acc: 0.9958 - val_loss: 1.1392 - val_acc: 0.8467\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 155us/step - loss: 0.0134 - acc: 0.9967 - val_loss: 1.1879 - val_acc: 0.8467\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 159us/step - loss: 0.0218 - acc: 0.9958 - val_loss: 1.1235 - val_acc: 0.8433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 896us/step - loss: 2.3249 - acc: 0.1017 - val_loss: 2.3313 - val_acc: 0.1167\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 2.3166 - acc: 0.1017 - val_loss: 2.3237 - val_acc: 0.1167\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.3112 - acc: 0.1017 - val_loss: 2.3179 - val_acc: 0.1167\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.3072 - acc: 0.1017 - val_loss: 2.3140 - val_acc: 0.1167\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.3045 - acc: 0.1017 - val_loss: 2.3117 - val_acc: 0.1167\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.3028 - acc: 0.0992 - val_loss: 2.3086 - val_acc: 0.0733\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.3013 - acc: 0.1000 - val_loss: 2.3073 - val_acc: 0.0733\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.3006 - acc: 0.1075 - val_loss: 2.3061 - val_acc: 0.0733\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 2.2997 - acc: 0.1075 - val_loss: 2.3055 - val_acc: 0.0733\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.2993 - acc: 0.1075 - val_loss: 2.3048 - val_acc: 0.0733\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 83us/step - loss: 2.2989 - acc: 0.1083 - val_loss: 2.3034 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3036 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.3030 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3028 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3023 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3020 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3020 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3020 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3023 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3020 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3024 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 94us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3023 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3025 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3020 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3019 - val_acc: 0.1433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3016 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3017 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3016 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3019 - val_acc: 0.1433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3573 - acc: 0.0933 - val_loss: 2.3203 - val_acc: 0.0833\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 338us/step - loss: 2.3300 - acc: 0.1075 - val_loss: 2.3066 - val_acc: 0.0833\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.3155 - acc: 0.1075 - val_loss: 2.3006 - val_acc: 0.0833\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.3080 - acc: 0.1075 - val_loss: 2.2984 - val_acc: 0.0833\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.3037 - acc: 0.1017 - val_loss: 2.2972 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - ETA: 0s - loss: 2.2998 - acc: 0.126 - 0s 69us/step - loss: 2.3012 - acc: 0.1192 - val_loss: 2.2971 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.3000 - acc: 0.1192 - val_loss: 2.2988 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2992 - acc: 0.1192 - val_loss: 2.2986 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.2989 - acc: 0.1192 - val_loss: 2.2994 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.2991 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2988 - acc: 0.1192 - val_loss: 2.2998 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3001 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3008 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3016 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2989 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3020 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 62us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 61us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3018 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3019 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3020 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3022 - val_acc: 0.1433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3009 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3022 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3018 - val_acc: 0.1433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 63us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3021 - val_acc: 0.1433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 972us/step - loss: 2.3878 - acc: 0.1192 - val_loss: 2.3401 - val_acc: 0.1433\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 2.3409 - acc: 0.1192 - val_loss: 2.3185 - val_acc: 0.1433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 65us/step - loss: 2.3194 - acc: 0.1192 - val_loss: 2.3089 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.3099 - acc: 0.1192 - val_loss: 2.3039 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 2.3042 - acc: 0.1192 - val_loss: 2.3025 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 2.3015 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.3001 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 2.2990 - acc: 0.1192 - val_loss: 2.3023 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.2991 - acc: 0.1192 - val_loss: 2.3009 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3016 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 2.2988 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2991 - acc: 0.1192 - val_loss: 2.3032 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.3001 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.3006 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.2996 - acc: 0.1192 - val_loss: 2.3016 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 69us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3020 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.2989 - acc: 0.1192 - val_loss: 2.3025 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.3018 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3016 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3019 - val_acc: 0.1433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 66us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3027 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 67us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3025 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - ETA: 0s - loss: 2.2981 - acc: 0.117 - 0s 65us/step - loss: 2.2993 - acc: 0.1192 - val_loss: 2.3007 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2990 - acc: 0.1192 - val_loss: 2.3028 - val_acc: 0.1433\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 64us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3019 - val_acc: 0.1433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 778us/step - loss: 2.3843 - acc: 0.0983 - val_loss: 2.3384 - val_acc: 0.1067\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 2.3088 - acc: 0.0950 - val_loss: 2.3110 - val_acc: 0.0733\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.3048 - acc: 0.1017 - val_loss: 2.3017 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.3040 - acc: 0.1033 - val_loss: 2.3069 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.3019 - acc: 0.1192 - val_loss: 2.3020 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.3012 - acc: 0.1192 - val_loss: 2.3033 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.3006 - acc: 0.1192 - val_loss: 2.2988 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 2.3006 - acc: 0.1192 - val_loss: 2.3030 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.3008 - acc: 0.1192 - val_loss: 2.3024 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.3015 - acc: 0.1033 - val_loss: 2.3008 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 2.3018 - acc: 0.1108 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.3019 - acc: 0.1142 - val_loss: 2.3019 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.3007 - acc: 0.1050 - val_loss: 2.3001 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.3017 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.3012 - acc: 0.1192 - val_loss: 2.3002 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.3022 - acc: 0.1192 - val_loss: 2.3007 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.3012 - acc: 0.1133 - val_loss: 2.3003 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.3005 - acc: 0.1192 - val_loss: 2.3017 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 2.3013 - acc: 0.1192 - val_loss: 2.3030 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 2.3015 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.2969 - acc: 0.1192 - val_loss: 2.2952 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 2.2834 - acc: 0.1183 - val_loss: 2.2714 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.2314 - acc: 0.1667 - val_loss: 2.1944 - val_acc: 0.2000\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.1403 - acc: 0.2033 - val_loss: 2.1206 - val_acc: 0.1967\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.0381 - acc: 0.2050 - val_loss: 2.0193 - val_acc: 0.2000\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.9771 - acc: 0.1892 - val_loss: 1.9908 - val_acc: 0.1833\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 1.9299 - acc: 0.2067 - val_loss: 1.9674 - val_acc: 0.2033\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 1.9063 - acc: 0.1900 - val_loss: 1.9635 - val_acc: 0.1833\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 1.8889 - acc: 0.2017 - val_loss: 1.9468 - val_acc: 0.1967\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 1.8619 - acc: 0.2092 - val_loss: 1.9619 - val_acc: 0.1833\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.2972 - acc: 0.1133 - val_loss: 2.2836 - val_acc: 0.1433\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 2.2415 - acc: 0.1242 - val_loss: 2.1911 - val_acc: 0.1867\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 2.1371 - acc: 0.1950 - val_loss: 2.0928 - val_acc: 0.2067\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 2.0732 - acc: 0.2050 - val_loss: 2.0439 - val_acc: 0.2100\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 2.0305 - acc: 0.2067 - val_loss: 2.0187 - val_acc: 0.2067\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.9966 - acc: 0.2125 - val_loss: 2.0029 - val_acc: 0.2067\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.9627 - acc: 0.2100 - val_loss: 1.9865 - val_acc: 0.2033\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.9292 - acc: 0.2150 - val_loss: 1.9696 - val_acc: 0.2033\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.9001 - acc: 0.2158 - val_loss: 1.9527 - val_acc: 0.1967\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 1.8797 - acc: 0.2100 - val_loss: 1.9523 - val_acc: 0.2133\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.8545 - acc: 0.2142 - val_loss: 1.9390 - val_acc: 0.2033\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.8370 - acc: 0.2167 - val_loss: 1.9329 - val_acc: 0.1800\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 1.8208 - acc: 0.2142 - val_loss: 1.9263 - val_acc: 0.1733\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.8104 - acc: 0.2267 - val_loss: 1.9260 - val_acc: 0.1800\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 94us/step - loss: 1.8020 - acc: 0.2308 - val_loss: 1.9152 - val_acc: 0.2100\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 1.7874 - acc: 0.2292 - val_loss: 1.8886 - val_acc: 0.2067\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 123us/step - loss: 1.7706 - acc: 0.2417 - val_loss: 1.8849 - val_acc: 0.2067\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 1.7580 - acc: 0.2483 - val_loss: 1.8717 - val_acc: 0.2033\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 1.7472 - acc: 0.2625 - val_loss: 1.8699 - val_acc: 0.2200\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.7351 - acc: 0.2542 - val_loss: 1.8594 - val_acc: 0.2133\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 1.7203 - acc: 0.2725 - val_loss: 1.8598 - val_acc: 0.2267\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 164us/step - loss: 1.7064 - acc: 0.2658 - val_loss: 1.8614 - val_acc: 0.2333\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 1.6993 - acc: 0.2692 - val_loss: 1.8533 - val_acc: 0.2300\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 1.6836 - acc: 0.2883 - val_loss: 1.8538 - val_acc: 0.2300\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.6751 - acc: 0.2875 - val_loss: 1.8444 - val_acc: 0.2500\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 1.6630 - acc: 0.3042 - val_loss: 1.8381 - val_acc: 0.2500\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 1.6520 - acc: 0.2858 - val_loss: 1.8354 - val_acc: 0.2567\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.6404 - acc: 0.3125 - val_loss: 1.8348 - val_acc: 0.2367\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 116us/step - loss: 1.6365 - acc: 0.3108 - val_loss: 1.8316 - val_acc: 0.2567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 157us/step - loss: 1.6232 - acc: 0.3250 - val_loss: 1.8361 - val_acc: 0.2467\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.2678 - acc: 0.1367 - val_loss: 2.2371 - val_acc: 0.1800\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 95us/step - loss: 2.1533 - acc: 0.1775 - val_loss: 2.1479 - val_acc: 0.1667\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 106us/step - loss: 2.0665 - acc: 0.1908 - val_loss: 2.1004 - val_acc: 0.1700\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 97us/step - loss: 2.0191 - acc: 0.1925 - val_loss: 2.0708 - val_acc: 0.1733\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 1.9797 - acc: 0.1975 - val_loss: 2.0399 - val_acc: 0.1900\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.9514 - acc: 0.1858 - val_loss: 2.0315 - val_acc: 0.2333\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 94us/step - loss: 1.9106 - acc: 0.2417 - val_loss: 2.0115 - val_acc: 0.2367\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 1.8748 - acc: 0.2542 - val_loss: 1.9852 - val_acc: 0.2333\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 95us/step - loss: 1.8409 - acc: 0.2592 - val_loss: 1.9713 - val_acc: 0.2500\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.7987 - acc: 0.2525 - val_loss: 1.9221 - val_acc: 0.2700\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 1.7699 - acc: 0.2758 - val_loss: 1.8999 - val_acc: 0.2633\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.7154 - acc: 0.2742 - val_loss: 1.8938 - val_acc: 0.2633\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 95us/step - loss: 1.6658 - acc: 0.2925 - val_loss: 1.8489 - val_acc: 0.2800\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.6164 - acc: 0.2992 - val_loss: 1.8448 - val_acc: 0.2667\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 1.5710 - acc: 0.3017 - val_loss: 1.8572 - val_acc: 0.2633\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 1.5221 - acc: 0.3150 - val_loss: 1.7758 - val_acc: 0.2600\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 1.4677 - acc: 0.3350 - val_loss: 1.8447 - val_acc: 0.3067\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 1.4441 - acc: 0.3433 - val_loss: 1.7940 - val_acc: 0.3133\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 1.3964 - acc: 0.3650 - val_loss: 1.7665 - val_acc: 0.3100\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 1.3731 - acc: 0.3850 - val_loss: 1.8045 - val_acc: 0.3333\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.3318 - acc: 0.4217 - val_loss: 1.7556 - val_acc: 0.4667\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 1.2902 - acc: 0.4625 - val_loss: 1.7635 - val_acc: 0.4700\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 1.2535 - acc: 0.4967 - val_loss: 1.8493 - val_acc: 0.4733\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 1.2170 - acc: 0.4992 - val_loss: 1.8603 - val_acc: 0.4900\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 93us/step - loss: 1.2091 - acc: 0.5125 - val_loss: 1.8539 - val_acc: 0.4800\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 1.1906 - acc: 0.5192 - val_loss: 1.8720 - val_acc: 0.4533\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 1.1593 - acc: 0.5325 - val_loss: 1.8629 - val_acc: 0.4467\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 1.1224 - acc: 0.5475 - val_loss: 1.9863 - val_acc: 0.4933\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 1.1029 - acc: 0.5783 - val_loss: 1.9044 - val_acc: 0.4800\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 1.1410 - acc: 0.5450 - val_loss: 2.0711 - val_acc: 0.4800\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.2501 - acc: 0.1142 - val_loss: 2.1189 - val_acc: 0.2000\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 2.0787 - acc: 0.1883 - val_loss: 2.0148 - val_acc: 0.2233\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 1.9020 - acc: 0.1950 - val_loss: 1.8959 - val_acc: 0.1900\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 1.7526 - acc: 0.2392 - val_loss: 1.7265 - val_acc: 0.3033\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 1.5710 - acc: 0.3358 - val_loss: 1.5542 - val_acc: 0.4200\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 1.4255 - acc: 0.4275 - val_loss: 1.4243 - val_acc: 0.4900\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 1.3031 - acc: 0.4750 - val_loss: 1.4256 - val_acc: 0.5600\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 1.2257 - acc: 0.5442 - val_loss: 1.3748 - val_acc: 0.5833\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 1.1440 - acc: 0.5867 - val_loss: 1.3261 - val_acc: 0.6067\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 1.0696 - acc: 0.6217 - val_loss: 1.3261 - val_acc: 0.6200\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 1.0032 - acc: 0.6633 - val_loss: 1.3124 - val_acc: 0.6300\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 0.9566 - acc: 0.6775 - val_loss: 1.3237 - val_acc: 0.6200\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.9275 - acc: 0.6817 - val_loss: 1.3323 - val_acc: 0.6433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 0.8797 - acc: 0.7033 - val_loss: 1.3762 - val_acc: 0.6333\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 0.8603 - acc: 0.6942 - val_loss: 1.3316 - val_acc: 0.6500\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.8014 - acc: 0.7192 - val_loss: 1.3134 - val_acc: 0.6733\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 0.7575 - acc: 0.7308 - val_loss: 1.3329 - val_acc: 0.6500\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.7269 - acc: 0.7417 - val_loss: 1.2549 - val_acc: 0.6700\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.6808 - acc: 0.7608 - val_loss: 1.3193 - val_acc: 0.6467\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.6645 - acc: 0.7583 - val_loss: 1.2564 - val_acc: 0.6700\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.6293 - acc: 0.7742 - val_loss: 1.2412 - val_acc: 0.6800\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 120us/step - loss: 0.6260 - acc: 0.7742 - val_loss: 1.3009 - val_acc: 0.6933\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 0.5787 - acc: 0.7883 - val_loss: 1.2538 - val_acc: 0.6867\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.5484 - acc: 0.7900 - val_loss: 1.2977 - val_acc: 0.6800\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.5148 - acc: 0.8100 - val_loss: 1.3048 - val_acc: 0.7100\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 105us/step - loss: 0.4855 - acc: 0.8125 - val_loss: 1.3592 - val_acc: 0.7067\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.4715 - acc: 0.8275 - val_loss: 1.3647 - val_acc: 0.6933\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.4654 - acc: 0.8267 - val_loss: 1.3048 - val_acc: 0.7133\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 100us/step - loss: 0.4261 - acc: 0.8458 - val_loss: 1.3185 - val_acc: 0.7400\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 0.4076 - acc: 0.8542 - val_loss: 1.3947 - val_acc: 0.7233\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.1857 - acc: 0.2000 - val_loss: 1.9908 - val_acc: 0.2400\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 142us/step - loss: 1.6323 - acc: 0.4183 - val_loss: 1.3767 - val_acc: 0.5367\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 1.1638 - acc: 0.5683 - val_loss: 1.1387 - val_acc: 0.6433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 139us/step - loss: 0.9216 - acc: 0.6883 - val_loss: 0.9257 - val_acc: 0.7233\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 137us/step - loss: 0.6839 - acc: 0.7950 - val_loss: 0.7793 - val_acc: 0.7700\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 130us/step - loss: 0.5369 - acc: 0.8383 - val_loss: 0.7335 - val_acc: 0.7900\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 136us/step - loss: 0.4161 - acc: 0.8817 - val_loss: 0.8042 - val_acc: 0.7733\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 126us/step - loss: 0.3692 - acc: 0.8850 - val_loss: 0.7157 - val_acc: 0.7967\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 0.2730 - acc: 0.9125 - val_loss: 0.7112 - val_acc: 0.7833\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 128us/step - loss: 0.2156 - acc: 0.9358 - val_loss: 0.7677 - val_acc: 0.8067\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 139us/step - loss: 0.2099 - acc: 0.9400 - val_loss: 0.8020 - val_acc: 0.8033\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 129us/step - loss: 0.1756 - acc: 0.9517 - val_loss: 0.8209 - val_acc: 0.8000\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 139us/step - loss: 0.1771 - acc: 0.9492 - val_loss: 0.8365 - val_acc: 0.8067\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 0.1394 - acc: 0.9650 - val_loss: 0.7641 - val_acc: 0.8167\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 130us/step - loss: 0.1094 - acc: 0.9733 - val_loss: 0.8118 - val_acc: 0.8200\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 0.0787 - acc: 0.9800 - val_loss: 0.8396 - val_acc: 0.8167\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 135us/step - loss: 0.0645 - acc: 0.9883 - val_loss: 0.8470 - val_acc: 0.8233\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 130us/step - loss: 0.0654 - acc: 0.9817 - val_loss: 0.7686 - val_acc: 0.8200\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 137us/step - loss: 0.1142 - acc: 0.9658 - val_loss: 0.8555 - val_acc: 0.8200\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 0.0884 - acc: 0.9700 - val_loss: 0.8058 - val_acc: 0.8167\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 137us/step - loss: 0.0657 - acc: 0.9783 - val_loss: 0.9983 - val_acc: 0.8067\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 132us/step - loss: 0.0567 - acc: 0.9858 - val_loss: 0.9309 - val_acc: 0.8333\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 132us/step - loss: 0.1301 - acc: 0.9542 - val_loss: 1.0872 - val_acc: 0.8100\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 142us/step - loss: 0.2049 - acc: 0.9375 - val_loss: 0.7682 - val_acc: 0.8233\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 139us/step - loss: 0.1038 - acc: 0.9708 - val_loss: 0.7981 - val_acc: 0.8033\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 129us/step - loss: 0.0472 - acc: 0.9858 - val_loss: 0.7828 - val_acc: 0.8500\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 130us/step - loss: 0.0324 - acc: 0.9883 - val_loss: 0.9006 - val_acc: 0.8467\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 0.0214 - acc: 0.9942 - val_loss: 0.9214 - val_acc: 0.8333\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 0.0105 - acc: 0.9992 - val_loss: 0.8922 - val_acc: 0.8467\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 0.0073 - acc: 0.9992 - val_loss: 0.9402 - val_acc: 0.8333\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.3026 - acc: 0.1033 - val_loss: 2.3026 - val_acc: 0.0733\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 2.3018 - acc: 0.1092 - val_loss: 2.3022 - val_acc: 0.1433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 2.3014 - acc: 0.1192 - val_loss: 2.3022 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 2.3009 - acc: 0.1192 - val_loss: 2.3018 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 138us/step - loss: 2.3006 - acc: 0.1192 - val_loss: 2.3016 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 2.3002 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 2.2999 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 2.2996 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 137us/step - loss: 2.2993 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 132us/step - loss: 2.2992 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 132us/step - loss: 2.2989 - acc: 0.1192 - val_loss: 2.3009 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 158us/step - loss: 2.2988 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 172us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 156us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 161us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3009 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 139us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 140us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 149us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 140us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 138us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 140us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 132us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 134us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 2.2979 - acc: 0.1192 - val_loss: 2.3013 - val_acc: 0.1433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 139us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 884us/step - loss: 2.3023 - acc: 0.1033 - val_loss: 2.3014 - val_acc: 0.0933\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 143us/step - loss: 2.2957 - acc: 0.1033 - val_loss: 2.2890 - val_acc: 0.0933\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 146us/step - loss: 2.2251 - acc: 0.1542 - val_loss: 2.1699 - val_acc: 0.1700\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 153us/step - loss: 2.0738 - acc: 0.1867 - val_loss: 2.1400 - val_acc: 0.2033\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 150us/step - loss: 2.0191 - acc: 0.2075 - val_loss: 2.0965 - val_acc: 0.2100\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 147us/step - loss: 1.9862 - acc: 0.2075 - val_loss: 2.0902 - val_acc: 0.2033\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 141us/step - loss: 1.9680 - acc: 0.2108 - val_loss: 2.0736 - val_acc: 0.2167\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 142us/step - loss: 1.9467 - acc: 0.2083 - val_loss: 2.0657 - val_acc: 0.2100\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 151us/step - loss: 1.9329 - acc: 0.2117 - val_loss: 2.0528 - val_acc: 0.2100\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 1.9149 - acc: 0.2142 - val_loss: 2.0251 - val_acc: 0.2400\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 138us/step - loss: 1.8961 - acc: 0.2100 - val_loss: 2.0167 - val_acc: 0.2300\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 1.8873 - acc: 0.2200 - val_loss: 2.0136 - val_acc: 0.2333\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 1.8748 - acc: 0.2250 - val_loss: 2.0216 - val_acc: 0.2300\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 139us/step - loss: 1.8631 - acc: 0.2283 - val_loss: 1.9933 - val_acc: 0.2567\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 132us/step - loss: 1.8474 - acc: 0.2300 - val_loss: 2.0172 - val_acc: 0.2500\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 1.8351 - acc: 0.2267 - val_loss: 2.0280 - val_acc: 0.2267\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 140us/step - loss: 1.8228 - acc: 0.2375 - val_loss: 2.0254 - val_acc: 0.2600\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 132us/step - loss: 1.8141 - acc: 0.2475 - val_loss: 2.0049 - val_acc: 0.2433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 1.8025 - acc: 0.2383 - val_loss: 2.0191 - val_acc: 0.2467\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 1.7909 - acc: 0.2500 - val_loss: 2.0499 - val_acc: 0.2567\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 132us/step - loss: 1.7738 - acc: 0.2583 - val_loss: 2.0203 - val_acc: 0.2667\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 1.7598 - acc: 0.2742 - val_loss: 1.9626 - val_acc: 0.3233\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 1.7449 - acc: 0.2942 - val_loss: 1.9758 - val_acc: 0.3200\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 135us/step - loss: 1.7263 - acc: 0.3108 - val_loss: 2.0268 - val_acc: 0.3233\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 1.7058 - acc: 0.2967 - val_loss: 2.0167 - val_acc: 0.3067\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 135us/step - loss: 1.6830 - acc: 0.2925 - val_loss: 2.0225 - val_acc: 0.3033\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 137us/step - loss: 1.6643 - acc: 0.3183 - val_loss: 1.9874 - val_acc: 0.3067\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 132us/step - loss: 1.6514 - acc: 0.3075 - val_loss: 1.9625 - val_acc: 0.2800\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 135us/step - loss: 1.6392 - acc: 0.3017 - val_loss: 1.9759 - val_acc: 0.2867\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 1.6880 - acc: 0.2942 - val_loss: 1.9267 - val_acc: 0.3300\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 962us/step - loss: 2.3006 - acc: 0.1008 - val_loss: 2.2919 - val_acc: 0.1233\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 2.2467 - acc: 0.1075 - val_loss: 2.1508 - val_acc: 0.1900\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 138us/step - loss: 2.0821 - acc: 0.1883 - val_loss: 2.0205 - val_acc: 0.2233\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 135us/step - loss: 1.9981 - acc: 0.1958 - val_loss: 1.9626 - val_acc: 0.2200\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 137us/step - loss: 1.9594 - acc: 0.2192 - val_loss: 1.9063 - val_acc: 0.2200\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 136us/step - loss: 1.8958 - acc: 0.2283 - val_loss: 1.9196 - val_acc: 0.2233\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 136us/step - loss: 1.8359 - acc: 0.2300 - val_loss: 1.8467 - val_acc: 0.2167\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 139us/step - loss: 1.7755 - acc: 0.2300 - val_loss: 1.7778 - val_acc: 0.2300\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 135us/step - loss: 1.7261 - acc: 0.2350 - val_loss: 1.7492 - val_acc: 0.2400\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 1.6741 - acc: 0.2658 - val_loss: 1.7712 - val_acc: 0.2833\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 141us/step - loss: 1.6411 - acc: 0.2700 - val_loss: 1.7552 - val_acc: 0.2700\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 140us/step - loss: 1.5892 - acc: 0.2850 - val_loss: 1.7722 - val_acc: 0.2667\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 1.5522 - acc: 0.2867 - val_loss: 1.9221 - val_acc: 0.3100\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 1.5379 - acc: 0.3375 - val_loss: 1.6536 - val_acc: 0.3533\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 1.4441 - acc: 0.3667 - val_loss: 1.6650 - val_acc: 0.3500\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 138us/step - loss: 1.3927 - acc: 0.3975 - val_loss: 1.5231 - val_acc: 0.4200\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 138us/step - loss: 1.3439 - acc: 0.4458 - val_loss: 1.5159 - val_acc: 0.4233\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 135us/step - loss: 1.2751 - acc: 0.4633 - val_loss: 1.6150 - val_acc: 0.4700\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 137us/step - loss: 1.2578 - acc: 0.4825 - val_loss: 1.4974 - val_acc: 0.4833\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 136us/step - loss: 1.2374 - acc: 0.5058 - val_loss: 1.4851 - val_acc: 0.4633\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 136us/step - loss: 1.2015 - acc: 0.5050 - val_loss: 1.5843 - val_acc: 0.4600\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 132us/step - loss: 1.1669 - acc: 0.5158 - val_loss: 1.5368 - val_acc: 0.4933\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 141us/step - loss: 1.1287 - acc: 0.5267 - val_loss: 1.5491 - val_acc: 0.4867\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 133us/step - loss: 1.1191 - acc: 0.5433 - val_loss: 1.6989 - val_acc: 0.5267\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 137us/step - loss: 1.0801 - acc: 0.5658 - val_loss: 1.6242 - val_acc: 0.5133\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 134us/step - loss: 1.0513 - acc: 0.5733 - val_loss: 1.4739 - val_acc: 0.5133\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 132us/step - loss: 1.0391 - acc: 0.5658 - val_loss: 1.6960 - val_acc: 0.5367\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 135us/step - loss: 1.0215 - acc: 0.5833 - val_loss: 1.4603 - val_acc: 0.5333\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 138us/step - loss: 1.0260 - acc: 0.5742 - val_loss: 1.4543 - val_acc: 0.5100\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 135us/step - loss: 0.9814 - acc: 0.5925 - val_loss: 1.4105 - val_acc: 0.5500\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 898us/step - loss: 2.2712 - acc: 0.1242 - val_loss: 2.1468 - val_acc: 0.1333\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 180us/step - loss: 1.9130 - acc: 0.2892 - val_loss: 1.6104 - val_acc: 0.3767\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 160us/step - loss: 1.3626 - acc: 0.4225 - val_loss: 1.2166 - val_acc: 0.4733\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 178us/step - loss: 1.0514 - acc: 0.5392 - val_loss: 1.1120 - val_acc: 0.5033\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 157us/step - loss: 0.8682 - acc: 0.6633 - val_loss: 1.1743 - val_acc: 0.5467\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 162us/step - loss: 0.7110 - acc: 0.7350 - val_loss: 0.8448 - val_acc: 0.6967\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 170us/step - loss: 0.4737 - acc: 0.8542 - val_loss: 0.8794 - val_acc: 0.7267\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 167us/step - loss: 0.4557 - acc: 0.8575 - val_loss: 0.7904 - val_acc: 0.7700\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 155us/step - loss: 0.3470 - acc: 0.9033 - val_loss: 0.7474 - val_acc: 0.7967\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 161us/step - loss: 0.2513 - acc: 0.9225 - val_loss: 0.7863 - val_acc: 0.7900\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 164us/step - loss: 0.2362 - acc: 0.9283 - val_loss: 0.6655 - val_acc: 0.8200\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 175us/step - loss: 0.1838 - acc: 0.9442 - val_loss: 0.8707 - val_acc: 0.8000\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 171us/step - loss: 0.1605 - acc: 0.9492 - val_loss: 0.8479 - val_acc: 0.8100\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 168us/step - loss: 0.1871 - acc: 0.9425 - val_loss: 1.0597 - val_acc: 0.7767\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 167us/step - loss: 0.2209 - acc: 0.9383 - val_loss: 0.6825 - val_acc: 0.8400\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 179us/step - loss: 0.1039 - acc: 0.9775 - val_loss: 0.7519 - val_acc: 0.8233\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 160us/step - loss: 0.0733 - acc: 0.9842 - val_loss: 0.7852 - val_acc: 0.8400\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 164us/step - loss: 0.0796 - acc: 0.9833 - val_loss: 0.8438 - val_acc: 0.8167\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 164us/step - loss: 0.0621 - acc: 0.9817 - val_loss: 0.8177 - val_acc: 0.8267\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 160us/step - loss: 0.0430 - acc: 0.9908 - val_loss: 0.8550 - val_acc: 0.8400\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 156us/step - loss: 0.0412 - acc: 0.9908 - val_loss: 0.8280 - val_acc: 0.8167\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 158us/step - loss: 0.0317 - acc: 0.9942 - val_loss: 0.8785 - val_acc: 0.8200\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 161us/step - loss: 0.0293 - acc: 0.9933 - val_loss: 0.9677 - val_acc: 0.8300\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 175us/step - loss: 0.0429 - acc: 0.9908 - val_loss: 1.0649 - val_acc: 0.8333\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 164us/step - loss: 0.0472 - acc: 0.9892 - val_loss: 0.9603 - val_acc: 0.8300\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 164us/step - loss: 0.0630 - acc: 0.9850 - val_loss: 1.4550 - val_acc: 0.7600\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 155us/step - loss: 0.0767 - acc: 0.9775 - val_loss: 0.9670 - val_acc: 0.8133\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 165us/step - loss: 0.0846 - acc: 0.9742 - val_loss: 1.1481 - val_acc: 0.7867\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 154us/step - loss: 0.1414 - acc: 0.9592 - val_loss: 0.8933 - val_acc: 0.8133\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 176us/step - loss: 0.0902 - acc: 0.9717 - val_loss: 0.8401 - val_acc: 0.8267\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 937us/step - loss: 2.3214 - acc: 0.1033 - val_loss: 2.3043 - val_acc: 0.0933\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.3139 - acc: 0.1033 - val_loss: 2.3010 - val_acc: 0.0933\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.3093 - acc: 0.1033 - val_loss: 2.2990 - val_acc: 0.0933\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.3059 - acc: 0.1033 - val_loss: 2.2985 - val_acc: 0.0933\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.3033 - acc: 0.1125 - val_loss: 2.2984 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.3020 - acc: 0.1192 - val_loss: 2.2981 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.3010 - acc: 0.1192 - val_loss: 2.2982 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.3003 - acc: 0.1192 - val_loss: 2.2984 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.2995 - acc: 0.1192 - val_loss: 2.2992 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.2992 - acc: 0.1192 - val_loss: 2.2990 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.2988 - acc: 0.1192 - val_loss: 2.2995 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.2997 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.2997 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3005 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3005 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3003 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3005 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.2980 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3018 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 80us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3019 - val_acc: 0.1433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 79us/step - loss: 2.2981 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 791us/step - loss: 2.3843 - acc: 0.1075 - val_loss: 2.4011 - val_acc: 0.0733\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.3537 - acc: 0.1075 - val_loss: 2.3713 - val_acc: 0.0733\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.3349 - acc: 0.1075 - val_loss: 2.3521 - val_acc: 0.0733\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.3229 - acc: 0.1075 - val_loss: 2.3396 - val_acc: 0.0733\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 2.3151 - acc: 0.1075 - val_loss: 2.3300 - val_acc: 0.0733\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.3095 - acc: 0.1075 - val_loss: 2.3238 - val_acc: 0.0733\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.3061 - acc: 0.1075 - val_loss: 2.3185 - val_acc: 0.0733\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.3035 - acc: 0.1075 - val_loss: 2.3140 - val_acc: 0.0733\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.3018 - acc: 0.1075 - val_loss: 2.3116 - val_acc: 0.0733\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.3006 - acc: 0.1075 - val_loss: 2.3095 - val_acc: 0.0733\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.2998 - acc: 0.1075 - val_loss: 2.3076 - val_acc: 0.0733\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 78us/step - loss: 2.2993 - acc: 0.1075 - val_loss: 2.3061 - val_acc: 0.0733\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.2989 - acc: 0.1075 - val_loss: 2.3053 - val_acc: 0.0733\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.2990 - acc: 0.1075 - val_loss: 2.3044 - val_acc: 0.0733\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.2986 - acc: 0.0983 - val_loss: 2.3042 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.3034 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3035 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3033 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3026 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3023 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3018 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.3018 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3031 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3025 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3019 - val_acc: 0.1433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3016 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 75us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3017 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3028 - val_acc: 0.1433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 2.2982 - acc: 0.1192 - val_loss: 2.3021 - val_acc: 0.1433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 837us/step - loss: 2.4461 - acc: 0.0950 - val_loss: 2.4217 - val_acc: 0.0800\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.3721 - acc: 0.0950 - val_loss: 2.3628 - val_acc: 0.0800\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 2.3367 - acc: 0.0950 - val_loss: 2.3354 - val_acc: 0.0800\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 2.3195 - acc: 0.0950 - val_loss: 2.3209 - val_acc: 0.0800\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.3103 - acc: 0.0950 - val_loss: 2.3129 - val_acc: 0.0800\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 2.3053 - acc: 0.0950 - val_loss: 2.3079 - val_acc: 0.0800\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 105us/step - loss: 2.3028 - acc: 0.0867 - val_loss: 2.3060 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 81us/step - loss: 2.3007 - acc: 0.1192 - val_loss: 2.3047 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 82us/step - loss: 2.3001 - acc: 0.1192 - val_loss: 2.3039 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 96us/step - loss: 2.2994 - acc: 0.1192 - val_loss: 2.3028 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 2.2990 - acc: 0.1192 - val_loss: 2.3033 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 2.2988 - acc: 0.1192 - val_loss: 2.3029 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3015 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 2.2988 - acc: 0.1192 - val_loss: 2.3019 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 77us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3031 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.2986 - acc: 0.1192 - val_loss: 2.3017 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3026 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3020 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 2.2991 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 76us/step - loss: 2.2985 - acc: 0.1192 - val_loss: 2.3019 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3020 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3021 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 70us/step - loss: 2.2988 - acc: 0.1192 - val_loss: 2.3012 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 68us/step - loss: 2.2984 - acc: 0.1192 - val_loss: 2.3016 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.2983 - acc: 0.1192 - val_loss: 2.3019 - val_acc: 0.1433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 74us/step - loss: 2.2991 - acc: 0.1192 - val_loss: 2.3025 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 71us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3021 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 2.2989 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 72us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3016 - val_acc: 0.1433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 73us/step - loss: 2.2987 - acc: 0.1192 - val_loss: 2.3019 - val_acc: 0.1433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 878us/step - loss: 2.3482 - acc: 0.0983 - val_loss: 2.2989 - val_acc: 0.1067\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.3035 - acc: 0.1150 - val_loss: 2.3026 - val_acc: 0.1433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 2.3012 - acc: 0.1075 - val_loss: 2.3023 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 2.3011 - acc: 0.1192 - val_loss: 2.2997 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 2.3006 - acc: 0.1192 - val_loss: 2.3037 - val_acc: 0.1433\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 2.3029 - acc: 0.1192 - val_loss: 2.3026 - val_acc: 0.1433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 2.3014 - acc: 0.1192 - val_loss: 2.3021 - val_acc: 0.1433\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 2.3017 - acc: 0.1058 - val_loss: 2.3049 - val_acc: 0.1433\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 2.3006 - acc: 0.1192 - val_loss: 2.3011 - val_acc: 0.1433\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 2.3015 - acc: 0.1092 - val_loss: 2.3018 - val_acc: 0.1433\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 92us/step - loss: 2.3003 - acc: 0.1117 - val_loss: 2.3024 - val_acc: 0.1433\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 2.3020 - acc: 0.1192 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 2.3009 - acc: 0.1192 - val_loss: 2.2996 - val_acc: 0.1433\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 88us/step - loss: 2.3004 - acc: 0.1192 - val_loss: 2.3017 - val_acc: 0.1433\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 2.3016 - acc: 0.1192 - val_loss: 2.3010 - val_acc: 0.1433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 91us/step - loss: 2.3006 - acc: 0.1150 - val_loss: 2.3034 - val_acc: 0.1433\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 2.3002 - acc: 0.1192 - val_loss: 2.3043 - val_acc: 0.1433\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 2.3011 - acc: 0.1192 - val_loss: 2.3017 - val_acc: 0.1433\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 2.3033 - acc: 0.1108 - val_loss: 2.3050 - val_acc: 0.1433\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 2.3018 - acc: 0.1192 - val_loss: 2.3028 - val_acc: 0.1433\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 84us/step - loss: 2.3017 - acc: 0.1192 - val_loss: 2.3021 - val_acc: 0.1433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 2.3015 - acc: 0.1192 - val_loss: 2.3004 - val_acc: 0.1433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 86us/step - loss: 2.3018 - acc: 0.1083 - val_loss: 2.3033 - val_acc: 0.1433\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 90us/step - loss: 2.3033 - acc: 0.1008 - val_loss: 2.3047 - val_acc: 0.1433\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 2.3007 - acc: 0.1058 - val_loss: 2.3014 - val_acc: 0.1433\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 2.3020 - acc: 0.1192 - val_loss: 2.3004 - val_acc: 0.1433\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 89us/step - loss: 2.3010 - acc: 0.1192 - val_loss: 2.3034 - val_acc: 0.1433\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 85us/step - loss: 2.3002 - acc: 0.1192 - val_loss: 2.3005 - val_acc: 0.1433\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 87us/step - loss: 2.3003 - acc: 0.1192 - val_loss: 2.3003 - val_acc: 0.1433\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 95us/step - loss: 2.3005 - acc: 0.1192 - val_loss: 2.3033 - val_acc: 0.1433\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 4s 3ms/step - loss: 2.3025 - acc: 0.1075 - val_loss: 2.3023 - val_acc: 0.1433\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 2.3017 - acc: 0.1192 - val_loss: 2.3022 - val_acc: 0.1433\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 2.3011 - acc: 0.1192 - val_loss: 2.3018 - val_acc: 0.1433\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 131us/step - loss: 2.3001 - acc: 0.1192 - val_loss: 2.3007 - val_acc: 0.1433\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 2.2696 - acc: 0.1767 - val_loss: 2.2074 - val_acc: 0.2467\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 2.1682 - acc: 0.2092 - val_loss: 2.1186 - val_acc: 0.2567\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 2.0909 - acc: 0.2117 - val_loss: 2.0412 - val_acc: 0.2567\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 2.0223 - acc: 0.2117 - val_loss: 1.9682 - val_acc: 0.2567\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 1.9461 - acc: 0.2133 - val_loss: 1.9055 - val_acc: 0.2600\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 1.8853 - acc: 0.2158 - val_loss: 1.8726 - val_acc: 0.2633\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 106us/step - loss: 1.8304 - acc: 0.2192 - val_loss: 1.8239 - val_acc: 0.2700\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 1.7951 - acc: 0.2292 - val_loss: 1.7930 - val_acc: 0.2767\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 1.7560 - acc: 0.2375 - val_loss: 1.7637 - val_acc: 0.2800\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 1.7281 - acc: 0.2433 - val_loss: 1.7368 - val_acc: 0.2900\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 1.6976 - acc: 0.2633 - val_loss: 1.7124 - val_acc: 0.3067\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 1.6764 - acc: 0.2758 - val_loss: 1.6965 - val_acc: 0.3033\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 1.6519 - acc: 0.2917 - val_loss: 1.6932 - val_acc: 0.3000\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 1.6358 - acc: 0.3017 - val_loss: 1.7054 - val_acc: 0.3000\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 1.6210 - acc: 0.3192 - val_loss: 1.6899 - val_acc: 0.3267\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 1.6093 - acc: 0.3267 - val_loss: 1.6821 - val_acc: 0.3367\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 1.5851 - acc: 0.3208 - val_loss: 1.6604 - val_acc: 0.3433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 105us/step - loss: 1.5738 - acc: 0.3433 - val_loss: 1.6447 - val_acc: 0.3433\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 1.5620 - acc: 0.3375 - val_loss: 1.6429 - val_acc: 0.3333\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 120us/step - loss: 1.5435 - acc: 0.3442 - val_loss: 1.6453 - val_acc: 0.3367\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 103us/step - loss: 1.5339 - acc: 0.3467 - val_loss: 1.6447 - val_acc: 0.3533\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 1.5437 - acc: 0.3408 - val_loss: 1.6362 - val_acc: 0.3667\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 1.5156 - acc: 0.3517 - val_loss: 1.6492 - val_acc: 0.3500\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 97us/step - loss: 1.5039 - acc: 0.3450 - val_loss: 1.6239 - val_acc: 0.3500\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 94us/step - loss: 1.4931 - acc: 0.3425 - val_loss: 1.6547 - val_acc: 0.3533\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 1.4861 - acc: 0.3525 - val_loss: 1.6495 - val_acc: 0.3467\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.2993 - acc: 0.1050 - val_loss: 2.2816 - val_acc: 0.1167\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 106us/step - loss: 2.2258 - acc: 0.1450 - val_loss: 2.1236 - val_acc: 0.1867\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 2.0968 - acc: 0.1983 - val_loss: 2.0250 - val_acc: 0.1833\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 2.0109 - acc: 0.2075 - val_loss: 1.9462 - val_acc: 0.1867\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 99us/step - loss: 1.9335 - acc: 0.2125 - val_loss: 1.8901 - val_acc: 0.2133\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 1.8627 - acc: 0.2183 - val_loss: 1.8124 - val_acc: 0.2267\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 1.8029 - acc: 0.2242 - val_loss: 1.7600 - val_acc: 0.2400\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 1.7351 - acc: 0.2375 - val_loss: 1.7305 - val_acc: 0.2367\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 1.6783 - acc: 0.2483 - val_loss: 1.6992 - val_acc: 0.2533\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 106us/step - loss: 1.6328 - acc: 0.2600 - val_loss: 1.6701 - val_acc: 0.2500\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 1.5996 - acc: 0.2550 - val_loss: 1.6580 - val_acc: 0.2733\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 106us/step - loss: 1.5748 - acc: 0.2833 - val_loss: 1.6337 - val_acc: 0.2700\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 104us/step - loss: 1.5531 - acc: 0.3008 - val_loss: 1.6253 - val_acc: 0.2667\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 1.5076 - acc: 0.2950 - val_loss: 1.6218 - val_acc: 0.2767\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 1.4715 - acc: 0.2983 - val_loss: 1.5722 - val_acc: 0.2600\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 106us/step - loss: 1.4300 - acc: 0.3100 - val_loss: 1.5590 - val_acc: 0.2733\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 98us/step - loss: 1.3848 - acc: 0.3242 - val_loss: 1.5909 - val_acc: 0.2833\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 1.3487 - acc: 0.3350 - val_loss: 1.5524 - val_acc: 0.2900\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 1.2936 - acc: 0.3767 - val_loss: 1.5686 - val_acc: 0.3300\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 1.2519 - acc: 0.3900 - val_loss: 1.5026 - val_acc: 0.3533\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 1.2215 - acc: 0.3933 - val_loss: 1.5320 - val_acc: 0.3200\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 1.1789 - acc: 0.4192 - val_loss: 1.4623 - val_acc: 0.3600\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 105us/step - loss: 1.1579 - acc: 0.4392 - val_loss: 1.4996 - val_acc: 0.3300\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 100us/step - loss: 1.1415 - acc: 0.4375 - val_loss: 1.5876 - val_acc: 0.3533\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 103us/step - loss: 1.1160 - acc: 0.4575 - val_loss: 1.6521 - val_acc: 0.4000\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 1.0909 - acc: 0.4858 - val_loss: 1.5330 - val_acc: 0.3867\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 1.0601 - acc: 0.4892 - val_loss: 1.6274 - val_acc: 0.3933\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 105us/step - loss: 1.0494 - acc: 0.4808 - val_loss: 1.5709 - val_acc: 0.4100\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 102us/step - loss: 1.0359 - acc: 0.5283 - val_loss: 1.5857 - val_acc: 0.4567\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 101us/step - loss: 1.0174 - acc: 0.5225 - val_loss: 1.5811 - val_acc: 0.4033\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 2s 1ms/step - loss: 2.2761 - acc: 0.1092 - val_loss: 2.1841 - val_acc: 0.1700\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 2.0789 - acc: 0.1983 - val_loss: 1.9831 - val_acc: 0.1867\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 1.9572 - acc: 0.2150 - val_loss: 1.9233 - val_acc: 0.2233\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 113us/step - loss: 1.8417 - acc: 0.2233 - val_loss: 1.7853 - val_acc: 0.2200\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 1.7428 - acc: 0.2592 - val_loss: 1.7326 - val_acc: 0.2200\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 1.6584 - acc: 0.2733 - val_loss: 1.6803 - val_acc: 0.2433\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 1.5968 - acc: 0.2950 - val_loss: 1.6418 - val_acc: 0.2767\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 1.5591 - acc: 0.3067 - val_loss: 1.6385 - val_acc: 0.3100\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 114us/step - loss: 1.5301 - acc: 0.3142 - val_loss: 1.6397 - val_acc: 0.3467\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 1.4812 - acc: 0.3542 - val_loss: 1.5704 - val_acc: 0.3233\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 1.4252 - acc: 0.3925 - val_loss: 1.5291 - val_acc: 0.3900\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 1.3695 - acc: 0.4042 - val_loss: 1.4703 - val_acc: 0.3833\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 1.2777 - acc: 0.4517 - val_loss: 1.4332 - val_acc: 0.5000\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 1.2255 - acc: 0.5167 - val_loss: 1.4299 - val_acc: 0.5033\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 1.1466 - acc: 0.5650 - val_loss: 1.3819 - val_acc: 0.5433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 1.1070 - acc: 0.5767 - val_loss: 1.3943 - val_acc: 0.5500\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 1.0319 - acc: 0.6092 - val_loss: 1.3283 - val_acc: 0.5500\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 107us/step - loss: 0.9706 - acc: 0.6258 - val_loss: 1.3958 - val_acc: 0.5600\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 0.9226 - acc: 0.6608 - val_loss: 1.3353 - val_acc: 0.5700\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 109us/step - loss: 0.8733 - acc: 0.6775 - val_loss: 1.3421 - val_acc: 0.5833\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 110us/step - loss: 0.8429 - acc: 0.6842 - val_loss: 1.2850 - val_acc: 0.6333\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 178us/step - loss: 0.7881 - acc: 0.7108 - val_loss: 1.4123 - val_acc: 0.6100\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 115us/step - loss: 0.7866 - acc: 0.7150 - val_loss: 1.3003 - val_acc: 0.6133\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 0s 111us/step - loss: 0.7930 - acc: 0.7108 - val_loss: 1.2711 - val_acc: 0.5967\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 0.7207 - acc: 0.7450 - val_loss: 1.2826 - val_acc: 0.6167\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 0.7048 - acc: 0.7483 - val_loss: 1.3300 - val_acc: 0.6400\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 112us/step - loss: 0.6462 - acc: 0.7875 - val_loss: 1.3179 - val_acc: 0.6500\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 0.6871 - acc: 0.7617 - val_loss: 1.3047 - val_acc: 0.6467\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 111us/step - loss: 0.6150 - acc: 0.8133 - val_loss: 1.2650 - val_acc: 0.6300\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 108us/step - loss: 0.5806 - acc: 0.8025 - val_loss: 1.3882 - val_acc: 0.6367\n",
      "Train on 1200 samples, validate on 300 samples\n",
      "Epoch 1/30\n",
      "1200/1200 [==============================] - 1s 1ms/step - loss: 2.2288 - acc: 0.1567 - val_loss: 2.0574 - val_acc: 0.1633\n",
      "Epoch 2/30\n",
      "1200/1200 [==============================] - 0s 162us/step - loss: 1.8709 - acc: 0.2625 - val_loss: 1.7662 - val_acc: 0.3200\n",
      "Epoch 3/30\n",
      "1200/1200 [==============================] - 0s 163us/step - loss: 1.5205 - acc: 0.4017 - val_loss: 1.4724 - val_acc: 0.4200\n",
      "Epoch 4/30\n",
      "1200/1200 [==============================] - 0s 158us/step - loss: 1.2238 - acc: 0.5033 - val_loss: 1.1862 - val_acc: 0.5533\n",
      "Epoch 5/30\n",
      "1200/1200 [==============================] - 0s 166us/step - loss: 0.9091 - acc: 0.6633 - val_loss: 1.0133 - val_acc: 0.7000\n",
      "Epoch 6/30\n",
      "1200/1200 [==============================] - 0s 178us/step - loss: 0.6511 - acc: 0.7892 - val_loss: 0.8475 - val_acc: 0.7800\n",
      "Epoch 7/30\n",
      "1200/1200 [==============================] - 0s 166us/step - loss: 0.4686 - acc: 0.8542 - val_loss: 0.7672 - val_acc: 0.8000\n",
      "Epoch 8/30\n",
      "1200/1200 [==============================] - 0s 162us/step - loss: 0.4208 - acc: 0.8675 - val_loss: 0.7581 - val_acc: 0.8000\n",
      "Epoch 9/30\n",
      "1200/1200 [==============================] - 0s 181us/step - loss: 0.3463 - acc: 0.8900 - val_loss: 0.7105 - val_acc: 0.8167\n",
      "Epoch 10/30\n",
      "1200/1200 [==============================] - 0s 161us/step - loss: 0.2700 - acc: 0.9142 - val_loss: 0.8585 - val_acc: 0.8233\n",
      "Epoch 11/30\n",
      "1200/1200 [==============================] - 0s 179us/step - loss: 0.2513 - acc: 0.9200 - val_loss: 0.8342 - val_acc: 0.7967\n",
      "Epoch 12/30\n",
      "1200/1200 [==============================] - 0s 164us/step - loss: 0.2362 - acc: 0.9267 - val_loss: 0.7556 - val_acc: 0.8467\n",
      "Epoch 13/30\n",
      "1200/1200 [==============================] - 0s 172us/step - loss: 0.1683 - acc: 0.9508 - val_loss: 0.7775 - val_acc: 0.8333\n",
      "Epoch 14/30\n",
      "1200/1200 [==============================] - 0s 158us/step - loss: 0.1296 - acc: 0.9642 - val_loss: 0.8793 - val_acc: 0.8133\n",
      "Epoch 15/30\n",
      "1200/1200 [==============================] - 0s 158us/step - loss: 0.1278 - acc: 0.9583 - val_loss: 0.8546 - val_acc: 0.8433\n",
      "Epoch 16/30\n",
      "1200/1200 [==============================] - 0s 192us/step - loss: 0.0905 - acc: 0.9700 - val_loss: 0.9075 - val_acc: 0.8233\n",
      "Epoch 17/30\n",
      "1200/1200 [==============================] - 0s 175us/step - loss: 0.0762 - acc: 0.9775 - val_loss: 0.9316 - val_acc: 0.8500\n",
      "Epoch 18/30\n",
      "1200/1200 [==============================] - 0s 165us/step - loss: 0.0919 - acc: 0.9683 - val_loss: 0.9108 - val_acc: 0.8367\n",
      "Epoch 19/30\n",
      "1200/1200 [==============================] - 0s 165us/step - loss: 0.0535 - acc: 0.9833 - val_loss: 0.9358 - val_acc: 0.8267\n",
      "Epoch 20/30\n",
      "1200/1200 [==============================] - 0s 181us/step - loss: 0.0380 - acc: 0.9883 - val_loss: 1.0870 - val_acc: 0.8167\n",
      "Epoch 21/30\n",
      "1200/1200 [==============================] - 0s 170us/step - loss: 0.0406 - acc: 0.9858 - val_loss: 1.1035 - val_acc: 0.8433\n",
      "Epoch 22/30\n",
      "1200/1200 [==============================] - 0s 155us/step - loss: 0.1125 - acc: 0.9692 - val_loss: 1.0196 - val_acc: 0.8533\n",
      "Epoch 23/30\n",
      "1200/1200 [==============================] - 0s 161us/step - loss: 0.0974 - acc: 0.9675 - val_loss: 0.9431 - val_acc: 0.8367\n",
      "Epoch 24/30\n",
      "1200/1200 [==============================] - 0s 161us/step - loss: 0.0715 - acc: 0.9800 - val_loss: 1.0459 - val_acc: 0.8533\n",
      "Epoch 25/30\n",
      "1200/1200 [==============================] - 0s 159us/step - loss: 0.0641 - acc: 0.9850 - val_loss: 1.1270 - val_acc: 0.8333\n",
      "Epoch 26/30\n",
      "1200/1200 [==============================] - 0s 159us/step - loss: 0.0148 - acc: 0.9992 - val_loss: 0.9676 - val_acc: 0.8767\n",
      "Epoch 27/30\n",
      "1200/1200 [==============================] - 0s 164us/step - loss: 0.0074 - acc: 0.9992 - val_loss: 1.1245 - val_acc: 0.8500\n",
      "Epoch 28/30\n",
      "1200/1200 [==============================] - 0s 175us/step - loss: 0.0064 - acc: 0.9992 - val_loss: 1.0812 - val_acc: 0.8500\n",
      "Epoch 29/30\n",
      "1200/1200 [==============================] - 0s 162us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 1.0903 - val_acc: 0.8533\n",
      "Epoch 30/30\n",
      "1200/1200 [==============================] - 0s 177us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 1.1120 - val_acc: 0.8467\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "del model\n",
    "\n",
    "for l, a, n in itertools.product(layers, activations, neuro):\n",
    "    \n",
    "    if a==\"relu\": \n",
    "        model = get_model(input_dim=784, output_dim=10, num_hidden_layers=l, hidden_size=n, activation=a)\n",
    "        #!rm -rf log/relu\n",
    "        tb_callback = keras.callbacks.TensorBoard(log_dir='./log/relu', histogram_freq=1,  write_grads=True, write_graph=True, write_images=True)\n",
    "        model.fit(X_train, y_train_oh, epochs=30, batch_size=32, validation_data=(X_test, y_test_oh), callbacks=[tb_callback, time_callback])\n",
    "        \n",
    "        preds_train = model.predict(X_train).argmax(axis=1)\n",
    "        preds_test = model.predict(X_test).argmax(axis=1)\n",
    "        acc=(np.mean(preds_test==y_test))\n",
    "        r_testA1.loc[str(n),str(l)]=acc\n",
    "        acc=0\n",
    "        times = time_callback.times\n",
    "        t=0\n",
    "        for i in range(0,len(times)):\n",
    "            t = t+times[i]\n",
    "            \n",
    "        r_testT1.loc[str(n),str(l)]=t\n",
    "        \n",
    "    if a==\"sigmoid\": \n",
    "        model = get_model(input_dim=784, output_dim=10, num_hidden_layers=l, hidden_size=n, activation=a)\n",
    "        #!rm -rf log/sigmoid\n",
    "        tb_callback = keras.callbacks.TensorBoard(log_dir='./log/sigmoid', histogram_freq=1,  write_grads=True, write_graph=True, write_images=True)\n",
    "        model.fit(X_train, y_train_oh, epochs=30, batch_size=32, validation_data=(X_test, y_test_oh), callbacks=[tb_callback, time_callback])\n",
    "        \n",
    "        preds_train = model.predict(X_train).argmax(axis=1)\n",
    "        preds_test = model.predict(X_test).argmax(axis=1)\n",
    "        acc=(np.mean(preds_test==y_test))\n",
    "        r_testA2.loc[str(n),str(l)]=acc\n",
    "        acc=0\n",
    "        times = time_callback.times\n",
    "        t=0\n",
    "        for i in range(0,len(times)):\n",
    "            t = t+times[i]\n",
    "            \n",
    "        r_testT2.loc[str(n),str(l)]=t\n",
    "       \n",
    "        \n",
    "    if a==\"leakyRelu\": \n",
    "        model = get_model(input_dim=784, output_dim=10, num_hidden_layers=l, hidden_size=n, activation=tf.nn.leaky_relu)\n",
    "        #!rm -rf log/leaky_relu\n",
    "        tb_callback = keras.callbacks.TensorBoard(log_dir='./log/leaky_relu', histogram_freq=1,  write_grads=True, write_graph=True, write_images=True)\n",
    "        model.fit(X_train, y_train_oh, epochs=30, batch_size=32, validation_data=(X_test, y_test_oh), callbacks=[tb_callback, time_callback])\n",
    "        \n",
    "        preds_train = model.predict(X_train).argmax(axis=1)\n",
    "        preds_test = model.predict(X_test).argmax(axis=1)\n",
    "        acc=(np.mean(preds_test==y_test))\n",
    "        r_testA3.loc[str(n),str(l)]=acc\n",
    "        acc=0\n",
    "        times = time_callback.times\n",
    "        t=0\n",
    "        for i in range(0,len(times)):\n",
    "            t = t+times[i]\n",
    "            \n",
    "        r_testT3.loc[str(n),str(l)]=t\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.486667</td>\n",
       "      <td>0.143333</td>\n",
       "      <td>0.236667</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.243333</td>\n",
       "      <td>0.223333</td>\n",
       "      <td>0.143333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.696667</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.143333</td>\n",
       "      <td>0.663333</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.426667</td>\n",
       "      <td>0.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.803333</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.776667</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.743333</td>\n",
       "      <td>0.636667</td>\n",
       "      <td>0.696667</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.826667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           2         3         4         5         6         7         8  \\\n",
       "3   0.250000  0.250000  0.486667  0.143333  0.236667  0.260000  0.243333   \n",
       "5   0.696667  0.520000  0.430000  0.450000  0.143333  0.663333  0.260000   \n",
       "10  0.803333  0.786667  0.740000  0.776667  0.766667  0.743333  0.636667   \n",
       "30  0.866667  0.876667  0.863333  0.853333  0.853333  0.840000  0.856667   \n",
       "\n",
       "           9        10  \n",
       "3   0.223333  0.143333  \n",
       "5   0.426667  0.330000  \n",
       "10  0.696667  0.550000  \n",
       "30  0.843333  0.826667  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_testA1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.0049049854278564,\n",
       " 0.3197040557861328,\n",
       " 0.2684171199798584,\n",
       " 0.27872705459594727,\n",
       " 0.3241901397705078,\n",
       " 0.27143001556396484,\n",
       " 0.27536487579345703,\n",
       " 0.2716679573059082,\n",
       " 0.2683830261230469,\n",
       " 0.27752184867858887,\n",
       " 0.2761390209197998,\n",
       " 0.29306578636169434,\n",
       " 0.29108381271362305,\n",
       " 0.2745511531829834,\n",
       " 0.27055907249450684,\n",
       " 0.27074599266052246,\n",
       " 0.27405500411987305,\n",
       " 0.27101993560791016,\n",
       " 0.27189207077026367,\n",
       " 0.28968095779418945,\n",
       " 0.281749963760376,\n",
       " 0.28820013999938965,\n",
       " 0.27416300773620605,\n",
       " 0.27887511253356934,\n",
       " 0.27129387855529785,\n",
       " 0.2767641544342041,\n",
       " 0.2702209949493408,\n",
       " 0.26512885093688965,\n",
       " 0.27893900871276855,\n",
       " 0.26993680000305176]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.471160</td>\n",
       "      <td>4.490754</td>\n",
       "      <td>4.975682</td>\n",
       "      <td>5.905574</td>\n",
       "      <td>6.287317</td>\n",
       "      <td>6.875630</td>\n",
       "      <td>7.335145</td>\n",
       "      <td>7.780571</td>\n",
       "      <td>8.949683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.489604</td>\n",
       "      <td>4.806472</td>\n",
       "      <td>5.375358</td>\n",
       "      <td>5.852316</td>\n",
       "      <td>6.205827</td>\n",
       "      <td>7.168689</td>\n",
       "      <td>7.495427</td>\n",
       "      <td>8.298495</td>\n",
       "      <td>8.499672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.938597</td>\n",
       "      <td>4.851333</td>\n",
       "      <td>5.471946</td>\n",
       "      <td>6.610244</td>\n",
       "      <td>6.300147</td>\n",
       "      <td>7.460280</td>\n",
       "      <td>7.771602</td>\n",
       "      <td>8.541917</td>\n",
       "      <td>8.948889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.668166</td>\n",
       "      <td>6.365580</td>\n",
       "      <td>7.104553</td>\n",
       "      <td>8.022554</td>\n",
       "      <td>7.984202</td>\n",
       "      <td>9.591400</td>\n",
       "      <td>9.764355</td>\n",
       "      <td>11.124410</td>\n",
       "      <td>11.945088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           2         3         4         5         6         7         8  \\\n",
       "3   4.471160  4.490754  4.975682  5.905574  6.287317  6.875630  7.335145   \n",
       "5   4.489604  4.806472  5.375358  5.852316  6.205827  7.168689  7.495427   \n",
       "10  4.938597  4.851333  5.471946  6.610244  6.300147  7.460280  7.771602   \n",
       "30  5.668166  6.365580  7.104553  8.022554  7.984202  9.591400  9.764355   \n",
       "\n",
       "            9         10  \n",
       "3    7.780571   8.949683  \n",
       "5    8.298495   8.499672  \n",
       "10   8.541917   8.948889  \n",
       "30  11.124410  11.945088  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_testT1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fced1f4fcd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r_testA1\n",
    "plt.pcolor(r_testA1)\n",
    "plt.yticks(np.arange(0.5, len(r_testA1.index), 1), r_testA1.index)\n",
    "plt.xticks(np.arange(0.5, len(r_testA1.columns), 1), r_testA1.columns)\n",
    "plt.colorbar()\n",
    "plt.title('activation = relu')\n",
    "plt.xlabel('# Capas')\n",
    "plt.ylabel('# neuronas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fce8aa00990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r_testA2\n",
    "plt.pcolor(r_testA2)\n",
    "plt.yticks(np.arange(0.5, len(r_testA2.index), 1), r_testA2.index)\n",
    "plt.xticks(np.arange(0.5, len(r_testA2.columns), 1), r_testA2.columns)\n",
    "plt.colorbar()\n",
    "plt.title('activation = sigmoid')\n",
    "plt.xlabel('# Capas')\n",
    "plt.ylabel('# neuronas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcea2b90d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r_testA3\n",
    "plt.pcolor(r_testA3)\n",
    "plt.yticks(np.arange(0.5, len(r_testA3.index), 1), r_testA3.index)\n",
    "plt.xticks(np.arange(0.5, len(r_testA3.columns), 1), r_testA3.columns)\n",
    "plt.colorbar()\n",
    "plt.title('activation = Leakyrelu')\n",
    "plt.xlabel('# Capas')\n",
    "plt.ylabel('# neuronas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAEWCAYAAACg+rZnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAHlFJREFUeJzt3Xm0XGWd7vHvk5CQgUAgYQaZ5ALCkqGzEESRScR2QG3xwhVEGkl7rxM2rY1eFbSdsB3ae+32EpXBiQYRHFCBiCjthAYkMioyQ0JCIBAgDMk5z/1j7yNFcU5OnUrVrl0nz2etvapq1679e+sMv3rrt993b9kmIiLqbUKvGxAREaNLso6I6ANJ1hERfSDJOiKiDyRZR0T0gSTriIg+kGRdI5L+n6QPd3rbtSXpzZIuryJWt0j6oKSvdmnfP5F0fDf2XQVJb5X0y163I9ZMGWfdGZLuBN5m+6e9bsvakLQ9cAcwyfbq3ramPZIOAr5pe5su7Pt04Pm2j+30vntF0lsp/nZf0uu2xMjSs66IpPV63YZY96iQ//NxIL/EDpD0DeB5wA8lPSbp/ZK2l2RJJ0q6G/hZue13JN0v6RFJV0navWE/50j6eHn/IEn3SjpF0lJJiyWd0Oa2syT9UNIKSb+X9PE1fO29qrx9uHwv+zd/TS7f1/+SdKukRyX9i6SdJP2mjHGBpMkN279a0nWSHpb0a0kvbPHnurGkSyQ9IGl5eX+bhuc3kXS2pEXl89+TNB34CbBV2f7HJG0l6XRJ3yxfd6mkdzbFWijpDeX9L0q6p3wv10h6abn+COCDwH8v97uwXP9zSW8r70+Q9CFJd5W/i69L2qh8buhv4nhJd0taJul/t/KzGIuyPZ+Q9CtgJbCjpI0kfa3827iv/BuYOMxrh9q4XtP+3tbpdsbYJFl3gO3jgLuB19jewPZnGp5+GbAb8Iry8U+AnYHNgGuBb61h11sAGwFbAycC/y5p4za2/Xfg8XKb48tlJAeWtzPL9/KbEbY7AvgbYD/g/cA84M3AtsAewDEAkvYBzgL+AZgFnAn8QNL6a2jDkAnA2cB2FB+GTwBfanj+G8A0YHeKn+cXbD8OvBJYVLZ/A9uLmvb77aH2lW18QRnjR+Wq3wN7AZuU235H0hTblwKfBM4v97vnMG1+a7kcDOwIbNDUZoCXALsAhwIfkbTbcG9e0qnlB9ywy3CvaXAcMBeYAdwFnAusBp4P7A0cDiQB95Ek6+473fbjtp8AsH2W7UdtPwWcDuw51PMaxirgY7ZX2f4x8BjFP3nL25a9p78DTrO90vZNFP+4a+sM2yts3wjcAFxu+3bbj1B8IO1dbncScKbtq20P2D4XeIoiya+R7Qdtf7ds96PAJyg+/JC0JUVSfrvt5eX7/kWLbb8Y2EvSduXjNwMXlb8TbH+zjL3a9ueA9Rn5597szcDny5/FY8AHgKP17DLYR20/YXshsBAYLulj+9O2Z460jNKOc2zfWB532ITiZ3Vy+be4FPgCcHSL7ylqIMm6++4ZuiNpoqRPS7pN0grgzvKp2SO89sGmg3wrKXpqY9l2U2C9xnY03W/Xkob7TwzzeKid2wGnNPUItwW2Gi2ApGmSzixLCisoSjQzyw+gbYGHbC8fa8PLxP8jnklWR9PwDacsJ92solT1MMU3lpF+R822oujJDrmL4ue/ecO6+xvur+l3ujYaf8fbAZOAxQ2/gzMpvo1En0iy7pyRhtU0rv8fwJHAYRQJYPtyvbrXLB6g+PrbODJi2zVs3+nhQfcAn2jqFU6zfV4Lrz2Fokf7Itsb8kyJRuV+N5E0XA+zlfdwHnCMpP2BqcCVAGV9+p+BNwEblz3YR3jmdzTavhdRJMchz6P4+S8ZfvORqRhu+NhIyygvb2znPRTfZmY3/A42tL37MK97vLyd1rBui7G2PTovybpzllDUKNdkBsU/zYMU/wyf7HajbA8AFwGnlz3VXYG3rOElDwCDjP5eWvUV4O2SXqTCdEmvkjQD/nqg9JwRXjuDopf+sKRNgNOGnrC9mKLc8h/lgchJkoaS+RJg1hrKSwA/pkiqH6OoQQ82xFxN8XNYT9JHgA0bXrcE2F4jj7A4D3ivpB0kbcAzNe4xD4O0/cmGuvtzljHsZzFwOfA5SRuWB0F3kvSyYbZ9ALgPOLb8Jvj3wE5jbXt0XpJ153wK+FD5NfOfRtjm6xRfi+8DbgJ+W1Hb3knRk7+f4qDceRQfGs9heyVFbfhX5XsZtba8JrYXUNStvwQsB/5CcQBuyLbAr0Z4+b9R9HqXUfysLm16/jiKWv0twFLg5DLmLRTv8fbyPTyn5FLWpy+i+Jbz7YanLqP4EPgzxe/qSZ5dUvhOefugpGuHafNZFD/jqyjGqz8JvGuE91eltwCTKf7ulgMXAluOsO1JwPsoOhW7A7+uooGxZpkUsw6SdAawhe2ezrpTMbxvIfBC26t62ZaIukvPeh0gaVdJLyzLEPtSDO27uNftsv207d2SqCNGl1l164YZFGWBrSjKBZ8Dvt/TFkXEmKQMEhHRB1IGiYjoA31RBpk0ebqnTBtplnV3DEzq5tDnNehR2IEpPQg6oTff6qZNeboncWdNGm1odHdMVfWHBNZ/7mlHKnPNH59aZnvTdl//ioOn+8GHBlqNdZntI9qNNRZ9kaynTNuYfV76nkpjPrpVb340g5N6EpZHdhkcfaMO85TqYwLs/YI7exL3uC16MwJur/XvH32jDtt+vRmVxxwycctb7xp9q5E9+NAAv7vsea3GanVm61rri2QdEVEVA4P0piOxJknWERENjFnl1sogVUqyjohokp51RETNGTNQwyHNSdYREU0GO37yybWXZB0R0cDAQJJ1RET9pWcdEVFzBlalZh0RUW/GKYNERNSeYaB+uTrJOiKiUTGDsX6SrCMinkUM9OqMamuQZB0R0aA4wJhkHRFRa8U46yTriIjaG0zPOiKi3tKzjojoA0YM1PCKh0nWERFNUgaJiKg5I552764hOZIk64iIBsWkmJRBIiJqLwcYIyJqzhYDrl/PumstkjRF0u8kLZR0o6SPlut3kHS1pFslnS9pcrfaEBHRjkHU0lKlbn58PAUcYntPYC/gCEn7AWcAX7C9M7AcOLGLbYiIGJPiAON6LS1V6lqyduGx8uGkcjFwCHBhuf5c4HXdakNExFgNHWBsZalSV6NJmijpOmApMB+4DXjY9upyk3uBrbvZhoiIsRqwWlqq1NV+vO0BYC9JM4GLgd2G22y410qaC8wFWH/qzK61MSKiUV1nMFbSItsPAz8H9gNmShr6kNgGWDTCa+bZnmN7zqTJ06toZkQEAIOe0NIyGklnSVoq6YaGdZtIml8OspgvaeNW2tTN0SCblj1qJE0FDgNuBq4E3lhudjzw/W61ISJirIoTOU1oaWnBOcARTetOBa4oB1lcUT4eVTfLIFsC50qaSPGhcIHtSyTdBPynpI8DfwC+1sU2RESMiRGrOjTd3PZVkrZvWn0kcFB5/1yKqsM/j7avriVr238E9h5m/e3Avt2KGxGxNmzGMilmtqQFDY/n2Z43yms2t724iOXFkjZrJVBmMEZEPMuYJrwssz2nm60ZkmQdEdHAjKln3Y4lkrYse9VbUgxtHlX9xqdERPRYBw8wDucHFIMrYAyDLNKzjohoYNSxiw9IOo/iYOJsSfcCpwGfBi6QdCJwN3BUK/tKso6IaGBgVYfO+2H7mBGeOnSs+0qyjoh4FuV81hERdWdoaXZi1ZKsIyKapGcdEVFzttKzjoiou+IAY65uHhFRc/W8BmOSdUREg+IAY2rWERG1V8eLDyRZR0Q06OQMxk5Kso6IaFL1xXBbkWQdEdHAhlWDSdYREbVWlEGSrCMiai8zGCMiai5D9yIi+kLKIBERfWEM12CsTJJ1RESDYjRIzg3SlgkrnmDKT/9YacxpG82oNN6QgR227EncDRZNrTzm8p178+d30+Ln9yTuKZvs2JO4k7ZYWXnMmRs8UXnMZ3xqrV6dSTEREX0iZZCIiJrLaJCIiD6R0SARETVni9VJ1hER9VfHMkj9Pj4iInpoqGbdytIKSe+RdIOkGyWd3G670rOOiGjSqZ61pD2Ak4B9gaeBSyX9yPatY91XetYREQ2Gxll3qGe9G/Bb2yttrwZ+Aby+nXYlWUdENBlELS3AbEkLGpa5Tbu6AThQ0ixJ04C/BbZtp00pg0RENLBhdesXH1hme87I+/LNks4A5gOPAQuB1e20Kz3riIgmnTzAaPtrtvexfSDwEDDmejWkZx0R8SydPjeIpM1sL5X0POANwP7t7CfJOiKiiTs7zvq7kmYBq4B32F7ezk6SrCMimnTyRE62X9qJ/SRZR0Q0sOs5gzHJOiLiWcRA66NBKpNkHRHRpMM1645Iso6IaJDzWUdE9AMXdeu6SbKOiGiSy3pFRNScc4AxIqI/pAwSEdEH6jgapGt9fUlnSVoq6YaGdZtImi/p1vJ2427Fj4hoh10k61aWKnWzMHMOcETTulOBK2zvDFxRPo6IqJVOnnWvU7qWrG1fRXE6wEZHAueW988FXtet+BER7bJbW6o0arKWdJSkGeX9D0m6SNI+bcbb3PZigPJ2szXEnTt09YWn/VSb4SIixsaIwcEJLS1VaiXah20/KuklwCsoesRf7m6zwPY823Nsz5ms9bsdLiLir9ziUqVWkvVAefsq4Mu2vw9MbjPeEklbApS3S9vcT0REd/TxAcb7JJ0JvAn4saT1W3zdcH4AHF/ePx74fpv7iYjonhp2rVtJum8CLgOOsP0wsAnwvtFeJOk84DfALpLulXQi8Gng5ZJuBV5ePo6IqJU69qxHnRRjeyVwkaTNymuIAdzSwuuOGeGpQ8fQvoiIShkYHOzDSTGSXlv2hO8AflHe/qTbDYuI6AkDVmtLhVopg/wLsB/wZ9s7AIcBv+pqqyIieqgvx1kDq2w/CEyQNMH2lcBeXW5XRETv1PAAYysncnpY0gbAVcC3JC0FVne3WRERvVL9wcNWtNKzPhJ4AngvcClwG/CabjYqIqKn+rFnbfvxhofnjrhhRMR4YHAHR4NIei/wtmLPXA+cYPvJse6nldEgbyhPafqIpBWSHpW0YuxNjojoF2pxGWUv0tbAu4E5tvcAJgJHt9OiVmrWnwFeY/vmdgJERPSdzpY41gOmSloFTAMWtbOTVmrWS5KoI2Kd0nrNevbQ2UHLZe6zdmPfB3wWuBtYDDxi+/J2mtRKz3qBpPOB7wF/PVep7YvaCRgRUWtDk2Jas8z2nJGeLK+GdSSwA/Aw8B1Jx9r+5lib1Uqy3hBYCRzesM5AknVEjEsdnPByGHCH7QcAJF0EvBjofLK2fcKYmxcR0c86NxrkbmA/SdMohkAfCixoZ0etjAbZRtLF5cVvl0j6rqRt2gkWEdEP5NaW0di+GrgQuJZi2N4EYF47bWrlAOPZFOeh3grYGvhhuS4iYvxp9eBii6US26fZ3tX2HraPs9u7TmEryXpT22fbXl0u5wCbthMsIqL+WjzjXg3PurdM0rGSJpbLscCD3W5YRETP1HC6eSvJ+u8prhZzP8U4wTeW6yIixqfBFpcKrXE0iKSJwN/Zfm1F7YmI6K2xjbOuzBp71rYHKAZ0R0SsMzo1GqSTWpkU8ytJXwLOB/56Bj7b13atVRERvVRxIm5FK8n6xeXtxxrWGTik882JiIjhtDKD8eAqGhIRURdVlzhaMWqylvSR4dbb/thw67tCwIT6Ffy7YWBKK192Om/S4wOVx5z6YCuDkTrvqVW9+VsamNqbuE8vnVp5TE9/ovKYHWM6Od28Y1rJDI1XipkCvBrIKVMjYvzqx5617c81Ppb0WYrp5xER41JflkGGMQ3YsdMNiYiojX5M1pKu55mmT6Q4L0h19eqIiKr1Y7KmqFEPWU1xma/VXWpPRERP9WLCSytGPRxv+y5gW+CQ8npiMyXt0PWWRUT0yqBaWyrUShnkNGAOsAvFeawnU1yS5oDuNi0iojf6smcNvB54LeUQPtuLgBndbFRERE/V8BSprdSsn7ZtqfiskTS9y22KiOidfq1ZAxdIOpOiVn0S8FPgK91tVkRED/Vjz9r2ZyW9HFhBUbf+iO35XW9ZRESPqOILC7SipUkxZXJOgo6I6JFRyyCS3iDpVkmPSFoh6VFJK6poXERET/RjGQT4DPAa2zl5U0SMf318gHFJEnVErFM61LOWtIuk6xqWFZJObqdJrfSsF0g6H/ge8NTQStsXtRMwIqL2OtSztv0nYC/46wXI7wMubmdfrSTrDYGVwOGNbQCSrCNi3BFdGw1yKHBbeQqPMWtl6N4J7ew4IqIvja1mPVvSgobH82zPG2Hbo4Hz2m1Wb64hFRFRZ60n62W254y2kaTJFKft+EC7TUqyjoho1vnRIK8ErrW9pN0dJFlHRDTpwtC9Y1iLEgi0NinmQw3311+bYBERfaGDk2IkTQNezloOyhgxWUt6v6T9gTc2rP7N2gRr2Pedkq4vxx0uGP0VEREVcTEapJWlpd3ZK23Psv3I2jRrTWWQPwFHATtK+i/gZmCWpF3KsYNr62Dbyzqwn4iIzuqzGYzLgQ8CfwEOAv5Puf5USb/ucrsiInpm6DqMoy1VWlOyPgL4EbAT8HlgX+Bx2yfYfvFaxjVwuaRrJM0dbgNJcyUtkLTgaT813CYREd3RTydysv1BAEkLKa65uDewqaRfAsttv2Yt4h5ge5GkzYD5km6xfVVT/HnAPICNJs6q4ZeSiBiXepCIW9HKiZwus/37Mnnea/slwFrNaiyv44jtpRTz5Pddm/1FRHSK6L8yCAC239/w8K3lurYPDEqaLmnG0H2Kc47c0O7+IiI6rY7JekyTYmwv7EDMzYGLJQ3F/7btSzuw34iIzqhhGaTyGYy2bwf2rDpuRETLkqwjImqupleKSbKOiGiWZB0RUX9duvjAWkmyjohokjJIRETd1XRSTJJ1RESzJOuIiHobmsFYN0nWERFNNFi/bJ1kHRHRKDXriIj+kDJIREQ/SLKOiKi/9KwjIvpBknVERM05080jImqvruOsW7msV0TEusVubWmBpJmSLpR0i6SbJe3fTpPSs46IaNLhnvUXgUttv1HSZGBaOztJso6IaNTBSTGSNgQO5Jnr1z4NPN3OvlIGiYhoosHWFmC2pAUNy9ymXe0IPACcLekPkr5aXih8zNKzjohoMobRIMtsz1nD8+sB+wDvsn21pC8CpwIfHmub0rOOiGhkOnmA8V7gXttXl48vpEjeY5ZkHRHRRG5tGY3t+4F7JO1SrjoUuKmdNvVJGUQgVRpx9dIHKo03ZPCF2/Uk7uRlKyuPOWnWhpXHBHhqo2r/loZMWtGbuINbt3U8a608tbpPUstIOjsa5F3At8qRILcDJ7Szkz7/iUZEdFanJ8XYvg5YU127JUnWERGN7Fx8ICKiL9QvVydZR0Q0q+O5QZKsIyIaGUgZJCKiD9QvVydZR0Q0SxkkIqIPZDRIRETddfCse52UZB0R0aCYFFO/bJ1kHRHRLNdgjIiov/SsIyLqLjXriIh+kHODRET0h5RBIiJqzmO6rFdlkqwjIpqlZx0R0Qfql6uTrCMimmmwfnWQJOuIiEYmk2IiIupOOJNiIiL6QpI1SJoCXAWsX8a/0PZpVbcjImJESdYAPAUcYvsxSZOAX0r6ie3f9qAtERHP1uGataQ7gUeBAWC17Tnt7KfyZG3bwGPlw0nlUr+PsYhYZ3VhNMjBtpetzQ4mdKolYyFpoqTrgKXAfNtX96IdERHP5aIM0spSoZ4ka9sDtvcCtgH2lbRH8zaS5kpaIGnB036y+kZGxLrJdDpZG7hc0jWS5rbbrJ6OBrH9sKSfA0cANzQ9Nw+YB7DRxNkpk0REdVqvgsyWtKDh8bwydzU6wPYiSZsB8yXdYvuqsTapF6NBNgVWlYl6KnAYcEbV7YiIGMkYxlkvG+2Aoe1F5e1SSRcD+1KMiBuTXpRBtgSulPRH4PcUNetLetCOiIjhdagMImm6pBlD94HDaaoitKoXo0H+COxdddyIiJbYMNCx0SCbAxdLgiLfftv2pe3sKDMYIyKadWikh+3bgT07sa8k64iIZpnBGBFRcwZyDcaIiLozuH7nSE2yjohoZDp5gLFjkqwjIpqlZh0R0QeSrCMi6q76kzS1Isk6IqKRgVwwNyKiD6RnHRFRdx2dbt4xSdYREY0MzjjriIg+kBmMERF9IDXriIiaszMaJCKiL6RnHRFRd8YDA71uxHMkWUdENMopUiMi+kSG7kVE1JsBp2cdEVFzzsUHIiL6Qh0PMMo1HKLSTNIDwF1tvnw2sKyDzalrzMQd33HXpfe6tnG3s71pu4ElXVrGb8Uy20e0G2ss+iJZrw1JC2zPGe8xE3d8x12X3msv49bZhF43ICIiRpdkHRHRB9aFZD1vHYmZuOM77rr0XnsZt7bGfc06ImI8WBd61hERfS/JOiKiD4zLZC1pW0lXSrpZ0o2S3lNR3CmSfidpYRn3o1XEbYg/UdIfJF1SYcw7JV0v6TpJCyqKOVPShZJuKX/H+1cQc5fyPQ4tKySd3O24Zez3ln9PN0g6T9KUCmK+p4x3Y7ffp6SzJC2VdEPDuk0kzZd0a3m7cTfb0A/GZbIGVgOn2N4N2A94h6QXVBD3KeAQ23sCewFHSNqvgrhD3gPcXGG8IQfb3qvCcbFfBC61vSuwJxW8Z9t/Kt/jXsDfACuBi7sdV9LWwLuBObb3ACYCR3c55h7AScC+FD/fV0vauYshzwGaJ5acClxhe2fgivLxOm1cJmvbi21fW95/lOKfeesK4tr2Y+XDSeVSyRFcSdsArwK+WkW8XpG0IXAg8DUA20/bfrjiZhwK3Ga73Vm1Y7UeMFXSesA0YFGX4+0G/Nb2SturgV8Ar+9WMNtXAQ81rT4SOLe8fy7wum7F7xfjMlk3krQ9sDdwdUXxJkq6DlgKzLddSVzg34D3A1WfgcbA5ZKukTS3gng7Ag8AZ5cln69Kml5B3EZHA+dVEcj2fcBngbuBxcAjti/vctgbgAMlzZI0DfhbYNsux2y2ue3FUHS+gM0qjl874zpZS9oA+C5wsu0VVcS0PVB+Vd4G2Lf8StlVkl4NLLV9TbdjDeMA2/sAr6QoNx3Y5XjrAfsAX7a9N/A4FX5FljQZeC3wnYribUzRy9wB2AqYLunYbsa0fTNwBjAfuBRYSFFajB4at8la0iSKRP0t2xdVHb/8av5znluL64YDgNdKuhP4T+AQSd+sIC62F5W3SylquPt2OeS9wL0N31gupEjeVXklcK3tJRXFOwy4w/YDtlcBFwEv7nZQ21+zvY/tAylKFLd2O2aTJZK2BChvl1Ycv3bGZbKWJIqa5s22P19h3E0lzSzvT6X4R7ul23Ftf8D2Nra3p/iK/jPbXe19AUiaLmnG0H3gcIqv0F1j+37gHkm7lKsOBW7qZswmx1BRCaR0N7CfpGnl3/WhVHBAVdJm5e3zgDdQ7XsG+AFwfHn/eOD7FcevnfF6PusDgOOA68v6McAHbf+4y3G3BM6VNJHig/AC25UNo+uBzYGLixzCesC3bV9aQdx3Ad8qSxK3AydUEJOyfvty4B+qiAdg+2pJFwLXUpQi/kA1U7G/K2kWsAp4h+3l3Qok6TzgIGC2pHuB04BPAxdIOpHiA+uobsXvF5luHhHRB8ZlGSQiYrxJso6I6ANJ1hERfSDJOiKiDyRZR0T0gSTrqJykT0k6SNLrJI04+1DSWxrO/HaTpH+qsp0RdZJkHb3wIopztbwM+K/hNpD0SuBk4HDbu1PMUnykshZG1EzGWUdlJP0r8AqK81zcBuwE3AFcaPtjTdteBZxu+2fD7OckYC4wGfgLcJztlZLOAZ4EdqeYsPOPti8pT+b1DWDohE/vtP3rchrz+cCGFJN6/qftYT88InotyToqJWlfitml/wj83PYBI2z3ELCD7ef0piXNsv1gef/jwBLb/7dM1ltQnCVuJ+BK4PkU3yAHbT9Znpf5PNtzJJ0CTLH9iXLW6bTylLoRtTNep5tHfe0NXAfsSvvn9NijTNIzgQ2Ayxqeu8D2IHCrpNvLOHcAX5K0FzAA/Ldy298DZ5Un/fqe7euIqKkk66hEmSjPoTh17DKKk+irPHfL/rafaHrJjRRXZHlOGaTcz+tsL5T0VorzSgxp/qpo4L3AEoqrnkygKJVg+6rylK6vAr4h6V9tf73NtxjRVTnAGJWwfV15nu8/Ay+gSMKvKC+V1ZyoAT4FfEbSFgCS1pf07vK5GcDiskf85qbXHSVpgqSdKC5U8CdgI2Bx2eM+juLSWEjajuI84F+hOEtjladajRiT9KyjMpI2BZbbHpS0q+0RyyC2fyxpc+Cn5alBDZxVPv1hitEkdwHXUyTvIX+iuAzV5sDbyzr1f1CcRe4oijr24+W2BwHvk7QKeAx4S4feakTH5QBjjBvlAcZLbF/Y67ZEdFrKIBERfSA964iIPpCedUREH0iyjojoA0nWERF9IMk6IqIPJFlHRPSB/w+Yn61VPG/E1QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fce8b3b13d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r_testT1\n",
    "r_testT1\n",
    "plt.pcolor(r_testT1)\n",
    "plt.yticks(np.arange(0.5, len(r_testA3.index), 1), r_testA3.index)\n",
    "plt.xticks(np.arange(0.5, len(r_testA3.columns), 1), r_testA3.columns)\n",
    "plt.colorbar()\n",
    "plt.title('training time, activation = relu')\n",
    "plt.xlabel('# Capas')\n",
    "plt.ylabel('# neuronas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAEWCAYAAACg+rZnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAHy5JREFUeJzt3Xm8HFWd9/HPNwkhJATCLhBkUQZURsHJgyA+yqo4KjKOOPi4AOOAPu7boOKC476OOuPoY1QEBRFEcJfFhWFcUAOCooAosoQlIew7yb3f54+qi0Vzb27dTnd19c33/XrV63ZXV9fvVHffX58+dc4p2SYiItptxqALEBERk0uyjogYAknWERFDIMk6ImIIJFlHRAyBJOuIiCGQZN1Hkv6fpHf2ets1JelFks5uIla/SDpG0hf6tO8fSDqsH/vupUG9j5PFlXSupH9pskxrA6Wf9fgkXQX8i+0fDrosa0LSdsBfgHVsrxpsabojaW/gRNsL+7DvdwOPtv3iXu97bSXpXIr3qy9fpmur1Ky7JGnWoMsQEWuPJOtxSPoK8EjgO5LuknS0pO0kWdLLJF0D/Ljc9uuSbpR0u6TzJD2usp/jJb2vvL23pKWS3iRpuaQbJB3R5babSPqOpDsk/VrS+yT9dILDOa/8e1t5LHtKOry6fXlcr5R0haQ7Jb1X0qMk/aKMcaqk2ZXtny3pIkm3Sfq5pMfXfF03kvRdSTdJurW8vbDy+MaSviTp+vLxb0qaB/wA2Kos/12StpL0bkknls87U9KrO2JdLOl55e1PSbq2PJYLJP3vcv2BwDHAP5X7vbhc/+DPeEkzJL1D0tXle/FlSRuWj419Jg6TdI2kFZLeXue1mIry/bqyfG/+IulFlfXV9/Hpki4vP4ufkfTfleM4XNLPJH2ifN+ulPTkcv215bEdVtnXhuWx3lQe+zskzZgg7gGSLivjfhpQr1+DSLIel+2XANcAz7G9vu2PVB5+GvAY4Bnl/R8AOwKbAxcCJ61m148ANgS2Bl4G/JekjbrY9r+Au8ttDiuXiTy1/LugPJZfTLDdgcDfAXsARwOLgRcB2wC7AC8EkPRE4Djg5cAmwOeAb0tadzVlGDMD+BKwLcWX4b3ApyuPfwWYCzyO4vX8hO27gWcC15flX9/29R37/epY+coyPraM8b1y1a+BXYGNy22/LmmO7TOBDwCnlPt9wjhlPrxc9gF2ANbvKDPAU4CdgP2Ad0l6zHgHL+mtZaIcd5ngOfOA/wCeaXs+8GTgonG22xQ4DXgbxftyeblt1ZOA35aPfxX4GvC/gEcDLwY+LWn9ctv/pPj87UDxmX8pcETH/sbifgN4B7Ap8Gdgr/GOJdaQ7SzjLMBVwP6V+9sBBnZYzXMWlNtsWN4/HnhfeXtviuQ0q7L9cmCPqWwLzARWAjtVHnsf8NMJyjRW7uq+Dq9uXz6+V+X+BcBbKvc/DnyyvP1Z4L0dMS4HntbFa7wrcGt5e0tgFNhonO32BpZ2rHs3RbsowHyKL69ty/vvB45bTdxbgSd07qfy+LkU5ysAfgS8svLYTuXrP6vy2i6sPP4r4NAefg7nAbcB/wis1/HYg+8jRTL9ReUxAddWjuNw4IrK439bln2Lyrqby/dkJnA/8NjKYy8Hzp0g7vkdcZeOxc3SuyU166m7duyGpJmSPiTpz5LuoEjwUNQwxnOzH3qS7x6KmtpUtt2MIlFcW3msertbyyq37x3n/lg5twXe1FEj3AbYarIAkuZK+lz5s/oOiiaaBZJmlvu4xfatUy247TspatGHlqsOpfILR0Vz0qXlz/TbKGqME71HnbYCrq7cv5ri9d+isu7Gyu3VvadT5uKXxT8BrwBukPQ9STtPUM5rK88zRdKs6nxPsT3e+7wpMJuHH/fWNeP24vMYHZKsJzZRN5nq+v8DPBfYnyIBbFeu72eb3U3AKqDaM2Kb1Wzf6+4+1wLvt72gssy1fXKN576Jomb6JNsb8NcmmrFa4MaSFozzvDrHcDLwQkl7AusBPwEo26ffAryAota+ALidv75Hk+37eoovqDGPpHj9l42/+cRUdDe8a6JloufZPsv2ARS/Pi4DPj/OZjdQ+UxIEg/9jEzFCopfD53Hfd0EcR/8/JVxV/d5jC4lWU9sGUV73erMp/i5eDNFW+sH+l0o2yPA6cC7y5rqzhQ/RSdyE0XzwmTHUtfngVdIepIK8yQ9S9J8ePBE6fETPHc+Re3tNkkbA8eOPWD7Bor2/8+oOBG5jqSxZL4M2GTsxN4Evk+RXN5D0QY9Wom5iuJ1mCXpXcAGlectA7YbO3k2jpOBN0javmzPHWvjnnI3SNsf8F/b3R+2jPccSVtIOqhsu74fuAsYGWfT7wF/K+lgFT2VXkVxTmPKys/YqcD7Jc2XtC3wRuDECeI+TtLzyriv7TZurF6S9cQ+CLyj/Kn/5gm2+TLFz8PrgD8A5zdUtldT1ORvpDgpdzLFP/LD2L6Hog33Z+Wx7LEmgW0vAY6kOMl2K/AnijbMMdsAP5vg6Z+kqPWuoHitzux4/CUUNbrLKNroX1/GvIziGK8sj+FhTS6276f4Etuf4uTZmLMovgT+SPFe3cdDf6Z/vfx7s6QLxynzcRSv8XkU/dXvA14zwfH1wwyKXyTXA7dQnOx7ZedGtlcAhwAfoag8PBZYwgSfixpeQ3Ee4ErgpxSv6XGrifuhMu6OTPz+xxrIoJhpQNKHgUfYHuioOxXd+y4GHm975SDLsrYrfyksBV5k+yeDLk+sudSsh5CknSU9vmyG2J2ia98Zgy6X7QdsPyaJejAkPUPSgrIb5TEU7fJN/dqLPssovOE0n6JZYCuK5oKPA98aaImiDfakaK6YTdEsd7DtewdbpOiVNINERAyBNINERAyBoWgGmT1rrtebPV732/7xjLVseoMBHK8H9BKPzhpMYM8cSNje97SvE3JQxwrct2zpCtubdfv8Z+wzzzffMl7vyIe74Lf3n2X7wG5jTcVQJOv1Zi9gz79pdnrckbmzJ99oGhmZ1/xHYXTmYJLmvZuvM5C4D8wfzPFqABPjPrDB5Nv0y+8/9sarJ99qYjffMsKvznpkrW1nbnlF3ZGwa2woknVERFMMjDI66XZNS7KOiKgwZqXrNYM0Kck6IqJDatYRES1nzEgLuzSn615ERIdRXGuZjKTjyqvwXDLOY29WcaWhWicpk6wjIioMjOBaSw3HU1yF6SEkbQMcQHFFqlqSrCMiOvSqZm37PIrZEjt9guLyebXbW9JmHRFRYWBl/TbrTSUtqdxfbHvx6p4g6SDgOtsXF9dqqCfJOiKiwvWbOABW2F5Ud2NJc4G3A0+farmSrCMiqgwj/esM8ihge2CsVr0QuFDS7rZvXN0Tk6wjIiqKEYx92rf9O2DzsfuSrgIWlVfcWa2cYIyIeAgxUnOZdE/SycAvgJ0kLZX0sm5LlZp1RERFcYKxN5Nu2X7hJI9vV3dfSdYRERVFP+v2TZGcZB0R0WF0UJOtr0aSdURERWrWERFDwIiRFva9SLKOiOiQZpCIiJYz4oFBXkRyAknWEREVxaCYNINERLReTjBGRLScLUbcvpp130okaY6kX0m6WNLvJf1buX57Sb+UdIWkUyTN7lcZIiK6MYpqLU3q59fH/cC+tp8A7AocKGkP4MPAJ2zvCNwKdD1WPiKi14oTjLNqLU3qW7J24a7y7jrlYmBf4LRy/QnAwf0qQ0TEVI2dYKyzNKmv0STNlHQRsBw4B/gzcJvtVeUmS4Gt+1mGiIipGrFqLU3qaz3e9giwq6QFwBnAY8bbbLznSjoKOApgzjob9q2MERFVa/UIRtu3SToX2ANYIGlWWbteCFw/wXMWA4sBNpy7Vf+u2xAR0WF0LesNsllZo0bSesD+wKXAT4Dnl5sdBnyrX2WIiJiqYiKnGbWWJvWzZr0lcIKkmRRfCqfa/q6kPwBfk/Q+4DfAF/tYhoiIKTFi5do03Nz2b4Hdxll/JbB7v+JGRKwJm1YOiskIxoiIh2h+wEsdSdYRERUmNeuIiKGw1nbdi4gYFka5+EBERNsZWNnwvB91tK9EEREDpcxnHRHRdqadIxiTrCMiOqRmHRHRcrZSs46IaLviBONaNNw8ImI4tfMajEnWEREVxQnG9rVZt+/rIyJiwHo1Raqk4yQtl3RJZd1HJV0m6beSzhibSnoySdYRERVjIxjrLDUcDxzYse4cYBfbjwf+CLytzo6SrCMiOvTqgrm2zwNu6Vh3duU6tOdTXDFrUmmzjoiosGHlaO167KaSllTuLy4vSVjXPwOn1NkwyToioqJoBqmdrFfYXtRNHElvB1YBJ9XZPsk6IqJDv0cwSjoMeDawn+1aFwRPso6IqOh31z1JBwJvAZ5m+566z0uyjoh4iN4NN5d0MrA3Rdv2UuBYit4f6wLnSAI43/YrJttXknVERIdeXYPR9gvHWf3FbvaVZB0RUVH0BsncIF3xvfcx8ttLG405a+cdG433oJtumXybPpi1/tzmg2ow3fznXLvuQOKyzmASwAObNP/ejs4e3iEcuaxXRMSQ6FUzSC8lWUdEVLR1Iqck64iIDrn4QEREy9liVZJ1RET7pRkkIqLl0mYdETEkkqwjIlou/awjIoZE+llHRLScDavqX3ygMUnWEREd0gwSEdFyabOOiBgSTrKOiGi/nGCMiGg5O23WERFDQIykN0hERPulzToiouUyN0hExDBw0W7dNknWEREd0hskIqLlnBOMERHDIc0gERFDoI29QfpW15d0nKTlki6prNtY0jmSrij/btSv+BER3bCLZF1naVI/G2aOBw7sWPdW4Ee2dwR+VN6PiGiVUavW0qS+JWvb5wG3dKx+LnBCefsE4OB+xY+I6JZdb2nSpMla0iGS5pe33yHpdElP7DLeFrZvACj/br6auEdJWiJpyUru7zJcRMTUGDE6OqPW0qQ60d5p+05JTwGeQVEj/mx/iwW2F9teZHvROqzb73AREQ9yzaVJdZL1SPn3WcBnbX8LmN1lvGWStgQo/y7vcj8REf3RwxOMvexoUSdZXyfpc8ALgO9LWrfm88bzbeCw8vZhwLe63E9ERP/0rmp9PD3qaFEn6b4AOAs40PZtwMbAv072JEknA78AdpK0VNLLgA8BB0i6AjigvB8R0Sq9qln3sqPFpINibN8DnC5pc0mPLFdfVuN5L5zgof3qFCwiYhAMjI7W7pa3qaQllfuLbS+e5DkP6WghacKOFlWTJmtJBwEfB7aiaGN+JEWyflydABERQ8VA/T7UK2wv6mNpHlSnGeS9wB7AH21vD+wP/KyvpYqIGKA+97PuqqNFnWS90vbNwAxJM2z/BNi162JGRLRdf/vuddXRos5ETrdJWh84DzhJ0nJgVVdFjIhovd7N+1F2tNibom17KXAsRceKU8tOF9cAh9TZV51k/VzgPuANwIuADYH3TL3YERFDokcjXnrZ0aJOb5C7K3dPmHDDiIjpwOD6vUEaU2dukOeVI21ul3SHpDsl3dFE4SIiBkM1l+bUaQb5CPAc25f2uzAREa0wpFeKWZZEHRFrlSFN1ksknQJ8E/46V6nt0/tWqoiIQZnaoJjG1EnWGwD3AE+vrDOQZB0R09JQXjDX9hFNFCQiojWGtDfIQklnlHOyLpP0DUkLmyhcRMQgyPWWJtUZbv4liuGRWwFbA98p10VETD91h5q3MFlvZvtLtleVy/HAZn0uV0TEgKg4wVhnaVCdZL1C0oslzSyXFwM397tgEREDM6Q163+muFrMjcANwPPLdRER09NozaVBq+0NImkm8I+2D2qoPBERg9XSftarrVnbHqGYdS8iYq3Rxt4gdQbF/EzSp4FTgAdn4LN9Yd9KFRExSMM4KAZ4cvm3Ooe1gX17X5yIiBhPnRGM+zRRkIiItmi6iaOOOlc3f9d46203drUYzZjBzHnrNxUOAF93Y6PxxozceedA4s6aWadjUI/NWbf5mIBWjQwkrteZOZC4PzzpuMZjHnDo4Y3H7BnTyuHmdZpBqleKmQM8G8iUqRExfQ1jzdr2x6v3JX2MYvh5RMS0NJTNIOOYC+zQ64JERLTGMCZrSb/jr0WfSTEvSK5uHhHT1zAma4o26jGrKC7ztapP5YmIGKhBDHipY9IuALavBrYB9rV9HbBA0vZ9L1lExKCMqt7SoDrNIMcCi4CdKOaxng2cCOzV36JFRAzGUNasgX8ADqLswmf7emB+PwsVETFQLZwitU6b9QO2LRXfNZLm9blMERGDM6xt1sCpkj5H0VZ9JPBD4PP9LVZExAANY83a9sckHQDcQdFu/S7b5/S9ZBERA6KGLyxQR61BMWVyToKOiBiQSZtBJD1P0hWSbpd0h6Q7Jd3RROEiIgaihc0gddqsPwIcZHtD2xvYnm97g34XLCJiIGpeJabuSUhJb5D0e0mXSDpZ0pxuilUnWS+znVn2ImLt0aOataStgdcCi2zvQjFlx6HdFKlOm/USSacA3wTuH1tp+/RuAkZEtF5vmzhmAetJWkkxEd713e5kMhsA9wBPr6wzkGQdEdOOmFJvkE0lLancX2x78dgd29eV00pfA9wLnG377G7KVafr3hHd7DgiYihNbVDMCtuLJnpQ0kbAc4HtgduAr0t6se0Tp1qsAVzLKSKi5XrXG2R/4C+2b7K9kqJF4smTPGdcSdYREZ16l6yvAfaQNFeSgP3o8rKI3VwpJiJiWuvV3CC2fynpNOBCiusB/AZYvPpnja/OoJh3VG4P5nLUERFN6uGgGNvH2t7Z9i62X2L7/smf9XATJmtJR0vaE3h+ZfUvugkyzr6vkvQ7SRd1nEmNiBgsF71B6ixNWl0zyOXAIcAOkv6Hop1lE0k72b68B7H3sb2iB/uJiOitIZsi9VbgGOBPwN7Af5Tr3yrp530uV0TEwPRyuHmvrC5ZHwh8D3gU8O/A7sDdto+w3VXXkwoDZ0u6QNJR420g6ShJSyQtecD3rWG4iIgpaOFEThM2g9g+BkDSxRTXXNwN2EzST4FbbT9nDeLuZft6SZsD50i6zPZ5HfEXU5413XDmpi38URIR09IAEnEddfpZn2X712XyXGr7KcAajWosr+OI7eXAGRS19oiIgRPD1wwCgO2jK3cPL9d1fWJQ0jxJ88duU8w5ckm3+4uI6LU2JuspDYqxfXEPYm4BnFEM5mEW8FXbZ/ZgvxERvdHCZpDGRzDavhJ4QtNxIyJqS7KOiGi5ATRx1JFkHRHRKck6IqL9mh5KXkeSdUREhzSDRES0XUsHxSRZR0R0SrKOiGi3sRGMbZNkHRHRQaPty9ZJ1hERVWmzjogYDmkGiYgYBknWERHtl5p1RMQwSLKOiGg5Z7h5RETrpZ91RMSwcPuydZJ1RESH1KwjItoug2IiIoZDTjBGRAyBJOuIiLYzrTzBOGPQBYiIaBu53lJrX9ICSadJukzSpZL27KZMw1GznjEDrTen0ZBeuarReGNmrNvscT5o5szmYw7oNWb27MHEHZB9jjyy8Zia08J2hKnobcX6U8CZtp8vaTYwt5udDEeyjohoSC8HxUjaAHgqcDiA7QeAB7rZV5pBIiKqbDRabwE2lbSkshzVsbcdgJuAL0n6jaQvSJrXTbGSrCMiOrnmAitsL6osizv2NAt4IvBZ27sBdwNv7aZISdYRER16eIJxKbDU9i/L+6dRJO8pS7KOiKgyMOp6y2S7sm8ErpW0U7lqP+AP3RQrJxgjIjr1tjfIa4CTyp4gVwJHdLOTJOuIiA69nMjJ9kXAojXdT5J1REQH1WjiaFqSdUREVWbdi4hov2JQTPuydZJ1RESnFo6WT7KOiOiQmnVERNulzToiYhg4vUEiIoZCmkEiIlrOuaxXRMRwSM06ImIItC9XJ1lHRHTSaPvaQZKsIyKqTAbFRES0nXAGxUREDIUka5A0BzgPWLeMf5rtY5suR0TEhJKsAbgf2Nf2XZLWAX4q6Qe2zx9AWSIiHipt1gXbBu4q765TLu37GouItVYbe4MM5IK5kmZKughYDpxTufJvRMSAuWgGqbM0aCDJ2vaI7V2BhcDuknbp3EbSUZKWSFrywOi9zRcyItZOJsm6k+3bgHOBA8d5bLHtRbYXzZ6xXuNli4i12GjNpUGNJ2tJm0laUN5eD9gfuKzpckRETER2raVJg+gNsiVwgqSZFF8Wp9r+7gDKERExvnTdA9u/BXZrOm5ERC02jLSvN0hGMEZEdErNOiJiCCRZR0S0nIFcgzEiou0MTpt1RES7mZxgjIgYCi1ssx7oCMaIiFbq8XDzcj6k30jqekxJatYREQ/Rl3k/XgdcCmzQ7Q5Ss46IqDIwOlpvqUHSQuBZwBfWpFipWUdEdKpfs95U0pLK/cW2F3ds80ngaGD+mhQpyToi4iGmNNx8he1FEz0o6dnActsXSNp7TUqVZB0RUWVw7/pZ7wUcJOnvgTnABpJOtP3iqe4obdYREZ1GXW+ZhO232V5oezvgUODH3SRqSM06IuLhWtjPOsk6IqLKrt3TY2q79bkUV8bqSpJ1RESn1KwjItrOeGRk0IV4mCTriIiqTJEaETEkMkVqRES7GXBq1hERLedcfCAiYii08QSj3MIuKp0k3QRc3eXTNwVW9LA4bY2ZuNM77tp0rGsad1vbm3UbWNKZZfw6Vtg+sNtYUzEUyXpNSFqyuolWpkvMxJ3ecdemYx1k3DbL3CAREUMgyToiYgisDcm6cyLw6Rozcad33LXpWAcZt7WmfZt1RMR0sDbUrCMihl6SdUTEEJiWyVrSNpJ+IulSSb+X9LqG4s6R9CtJF5dx/62JuJX4MyX9RtJ3G4x5laTfSbqo48Kh/Yy5QNJpki4r3+M9G4i5U3mMY8sdkl7f77hl7DeUn6dLJJ0saU4DMV9Xxvt9v49T0nGSlku6pLJuY0nnSLqi/LtRP8swDKZlsgZWAW+y/RhgD+BVkh7bQNz7gX1tPwHYFThQ0h4NxB3zOuDSBuON2cf2rg32i/0UcKbtnYEn0MAx2768PMZdgb8D7gHO6HdcSVsDrwUW2d4FmElxeah+xtwFOBLYneL1fbakHfsY8nigc2DJW4Ef2d4R+FF5f602LZO17RtsX1jevpPin3nrBuLa9l3l3XXKpZEzuJIWAs8CvtBEvEGRtAHwVOCLALYfsH1bw8XYD/iz7W5H1U7VLGA9SbOAucD1fY73GOB82/fYXgX8N/AP/Qpm+zzglo7VzwVOKG+fABzcr/jDYlom6ypJ2wG7Ab9sKN5MSRcBy4FzbDcSF/gkcDTQ9Aw0Bs6WdIGkoxqItwNwE/ClssnnC5LmNRC36lDg5CYC2b4O+BhwDXADcLvts/sc9hLgqZI2kTQX+Htgmz7H7LSF7RugqHwBmzccv3WmdbKWtD7wDeD1tu9oIqbtkfKn8kJg9/InZV9Jejaw3PYF/Y41jr1sPxF4JkVz01P7HG8W8ETgs7Z3A+6mwZ/IkmYDBwFfbyjeRhS1zO2BrYB5krq6OnZdti8FPgycA5wJXEzRtBgDNG2TtaR1KBL1SbZPbzp++dP8XB7eFtcPewEHSboK+Bqwr6QTG4iL7evLv8sp2nB373PIpcDSyi+W0yiSd1OeCVxoe1lD8fYH/mL7JtsrgdOBJ/c7qO0v2n6i7adSNFFc0e+YHZZJ2hKg/Lu84fitMy2TtSRRtGleavvfG4y7maQF5e31KP7RLut3XNtvs73Q9nYUP9F/bLuvtS8ASfMkzR+7DTyd4id039i+EbhW0k7lqv2AP/QzZocX0lATSOkaYA9Jc8vP9X40cEJV0ubl30cCz6PZYwb4NnBYefsw4FsNx2+d6Tqf9V7AS4Dfle3HAMfY/n6f424JnCBpJsUX4am2G+tGNwBbAGcUOYRZwFdtn9lA3NcAJ5VNElcCRzQQk7L99gDg5U3EA7D9S0mnARdSNEX8hmaGYn9D0ibASuBVtm/tVyBJJwN7A5tKWgocC3wIOFXSyyi+sA7pV/xhkeHmERFDYFo2g0RETDdJ1hERQyDJOiJiCCRZR0QMgSTriIghkGQdjZP0QUl7SzpY0oSjDyW9tDLz2x8kvbnJcka0SZJ1DMKTKOZqeRrwP+NtIOmZwOuBp9t+HMUoxdsbK2FEy6SfdTRG0keBZ1DMc/Fn4FHAX4DTbL+nY9vzgHfb/vE4+zkSOAqYDfwJeInteyQdD9wHPI5iwM4bbX+3nMzrK8DYhE+vtv3zchjzKcAGFIN6/q/tcb88IgYtyToaJWl3itGlbwTOtb3XBNvdAmxv+2G1aUmb2L65vP0+YJnt/yyT9SMoZol7FPAT4NEUvyBHbd9Xzst8su1Fkt4EzLH9/nLU6dxySt2I1pmuw82jvXYDLgJ2pvs5PXYpk/QCYH3grMpjp9oeBa6QdGUZ5y/ApyXtCowAf1Nu+2vguHLSr2/avoiIlkqyjkaUifJ4iqljV1BMoq9y7pY9bd/b8ZTfU1yR5WHNIOV+DrZ9saTDKeaVGNP5U9HAG4BlFFc9mUHRVILt88opXZ8FfEXSR21/uctDjOirnGCMRti+qJzn+4/AYymS8DPKS2V1JmqADwIfkfQIAEnrSnpt+dh84IayRvyijucdImmGpEdRXKjgcmBD4Iayxv0SiktjIWlbinnAP08xS2OTU61GTElq1tEYSZsBt9oelbSz7QmbQWx/X9IWwA/LqUENHFc+/E6K3iRXA7+jSN5jLqe4DNUWwCvKdurPUMwidwhFO/bd5bZ7A/8qaSVwF/DSHh1qRM/lBGNMG+UJxu/aPm3QZYnotTSDREQMgdSsIyKGQGrWERFDIMk6ImIIJFlHRAyBJOuIiCGQZB0RMQT+P+vOxYBHspx2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcec8ac2250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r_testT2\n",
    "plt.pcolor(r_testT2)\n",
    "plt.yticks(np.arange(0.5, len(r_testA3.index), 1), r_testA3.index)\n",
    "plt.xticks(np.arange(0.5, len(r_testA3.columns), 1), r_testA3.columns)\n",
    "plt.colorbar()\n",
    "plt.title('training time, activation = sigmoid')\n",
    "plt.xlabel('# Capas')\n",
    "plt.ylabel('# neuronas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fce7b692990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r_testT3\n",
    "plt.pcolor(r_testT3)\n",
    "plt.yticks(np.arange(0.5, len(r_testA3.index), 1), r_testA3.index)\n",
    "plt.xticks(np.arange(0.5, len(r_testA3.columns), 1), r_testA3.columns)\n",
    "plt.colorbar()\n",
    "plt.title('training time, activation = Leakyrelu')\n",
    "plt.xlabel('# Capas')\n",
    "plt.ylabel('# neuronas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fce7bbf2710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig=plt.figure()\n",
    "#fig.title(\"Heatmap\")\n",
    "\n",
    "f1=fig.add_subplot(321)\n",
    "f1.pcolor(r_testA1)\n",
    "f1.title.set_text('Relu')\n",
    "#plt.colorbar(f1)\n",
    "\n",
    "f2=fig.add_subplot(323)\n",
    "f2.pcolor(r_testA2)\n",
    "f2.title.set_text('Sigmoid')\n",
    "#plt.colorbar(f2)\n",
    "\n",
    "f3=fig.add_subplot(325)\n",
    "f3.pcolor(r_testA3)\n",
    "f3.title.set_text('LeakyRelu')\n",
    "#plt.colorbar(f3)\n",
    "\n",
    "f4=fig.add_subplot(322)\n",
    "f4.pcolor(r_testT1)\n",
    "f4.title.set_text('Relu')\n",
    "\n",
    "f5=fig.add_subplot(324)\n",
    "f5.pcolor(r_testT2)\n",
    "f5.title.set_text('Relu')\n",
    "\n",
    "f6=fig.add_subplot(326)\n",
    "f6.pcolor(r_testT3)\n",
    "f6.title.set_text('Relu')\n",
    "plt.show()\n",
    "\n",
    "#plt.colorbar(f6,ax6)\n",
    "#plt.yticks(np.arange(0.5, len(r_test.index), 1), r_test.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[2,3,4,5,6,7,8,9,10]\n",
    "activations=[\"relu\", \"sigmoid\", \"leakyRelu\"]\n",
    "#cuando sea leakyrelu toca poner la fn de activacion directamente =tf.nn.leaky_relu\n",
    "neuro=[3,5,10,30]\n",
    "r=[0,0,0,0,0,0,0,0,0]\n",
    "s=[0,0,0,0,0,0,0,0,0]\n",
    "lr=[0,0,0,0,0,0,0,0,0]\n",
    "        \n",
    "for a in activations:\n",
    "    if a=='relu':\n",
    "        for n in [0,1,2,3,4,5,6,7,8]:\n",
    "            for l in [0,1,2,3]:\n",
    "                r[n]+=np.array(r_testA1.iloc[l,n])\n",
    "                \n",
    "    r[n]/=9\n",
    "\n",
    "    if a=='sigmoid':\n",
    "        for n in [0,1,2,3,4,5,6,7,8]:\n",
    "            for l in [0,1,2,3]:\n",
    "                s[n]+=np.array(r_testA2.iloc[l,n])\n",
    "                \n",
    "    s[n]/=9\n",
    "\n",
    "    if a=='leakyRelu':\n",
    "        for n in [0,1,2,3,4,5,6,7,8]:\n",
    "            for l in [0,1,2,3]:\n",
    "                lr[n]+=np.array(r_testA3.iloc[l,n])\n",
    "                \n",
    "    lr[n]/=9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4666666666666666,\n",
       " 1.8133333333333332,\n",
       " 0.9866666666666667,\n",
       " 1.28,\n",
       " 0.4533333333333333,\n",
       " 1.16,\n",
       " 0.4533333333333333,\n",
       " 0.4533333333333333,\n",
       " 0.4533333333333333,\n",
       " 0]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.72,\n",
       " 0.6666666666666666,\n",
       " 0.7866666666666666,\n",
       " 0.4533333333333333,\n",
       " 0.4533333333333333,\n",
       " 0.4533333333333333,\n",
       " 0.4533333333333333,\n",
       " 0.4533333333333333,\n",
       " 0.4533333333333333,\n",
       " 0]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.88,\n",
       " 1.64,\n",
       " 2.026666666666667,\n",
       " 1.8933333333333333,\n",
       " 1.1866666666666668,\n",
       " 1.8133333333333332,\n",
       " 1.0,\n",
       " 1.5333333333333334,\n",
       " 1.0266666666666666,\n",
       " 0]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[2,3,4,5,6,7,8,9,10]\n",
    "layers1=[2.2,3.2,4.2,5.2,6.2,7.2,8.2,9.2,10.2]\n",
    "layers2=[2.4,3.4,4.4,5.4,6.4,7.4,8.4,9.4,10.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAGz1JREFUeJzt3Xt0VPW99/H3B0lFxSpKtAiW4OMNBC80IkhbolVUvIBLqVIV0arn2Gptq62VtZ5K7elqT1cX5/Fyji4rSmstehaKUgk9ytHGy1MvAalWohZ9qKR4NCJyEa0Evs8fs0mHMGEmySQz2Xxea81i9uzf7P2dDXzyy2/v/RtFBGZmli69Sl2AmZkVn8PdzCyFHO5mZinkcDczSyGHu5lZCjnczcxSyOFuViYk3SHpf5e6DksH+Tp3s55P0h+A30TEXZ3cTk2ynUHFqMtKxz13szwk7VLqGszay+FuLSStkPQ9SS9L+kjSLEn7S1ooab2kRZL6ZbUfLen/SvpQ0p+SXt/WdZdIakje95akf8paVyOpUdK1kt6T9I6kS3ZQV5vbStZPlLRU0jpJb0o6NXl9H0n3SFolaY2kh5PXp0l6ptU2QtLByfPZkm6XVCvpI+AESadLeinZx0pJM1q9/4tZx2Jlso9jJb0rqXdWu3MkLW3jc86W9C/tPUaSfgJ8CbhN0gZJtyWvHy7pcUkfSHpd0lez3jNB0rLkmP5N0nWS9gAWAgck29kg6YC2/l6szEWEH34QEQArgOeA/YGBwHvAEuAYYFfgCeDGpO1AYDUwgUwn4eRkuTJZfzrwvwAB44CNwMhkXQ3QDNwEVCTb2Aj0a6OuHW1rFLA22X+vpK7Dk3ULgAeAfsl+xiWvTwOeabWPAA5Ons9Otjk22WafpOYRyfKRwLvApKT954H1wJRkP/sCRyfrlgGnZe1nHnBtG59zNvAvHTxGfwAuy1reA1gJXAL0BkYC7wNHJOvfAb6UPO/X6u+msdT/Fv3o/MM9d2vt1oh4NyL+BjwNPB8RL0XE38kE0zFJuwuB2oiojYgtEfE4UE8mhIiIBRHxZmTUAY+R6V1utQm4KSI2RUQtsAE4LFdBebb1deDuiHg8qeNvEfGapAHAacA/R8SaZD917TgOj0TEs8k2P4mIP0TEK8nyy8AcMj9oAC4AFkXEnGQ/qyNia+/8V8mxQtI+wCnAbwusoeBjlMMZwIqIuCcimiNiCfAgcG7WtodJ+mxyfJYUuF3rIRzu1tq7Wc8/zrHcN3k+GJicDEN8KOlD4IvAAABJp0l6LhkS+JBM6PfP2tbqiGjOWt6Yte1t5NnWgcCbOd52IPBBRKwp4DPnsrJVDcdJelJSk6S1wD8XUAPAb4AzJfUFvgo8HRHvFFhDwccoh8HAca3+fi4APpesP4fMcfyrpDpJYwrcrvUQDnfrqJXAvRGxd9Zjj4j4maRdyfQSfwHsHxF7A7VkhlXapYBtrSQzZJOrvn0k7Z1j3UfA7ln7+FyONq0vI/stMB84MCL2Au4ooAaS34D+CJwNXATcm6tdEbSudyVQ1+rvp29EXJnU9WJETAT2Ax4G/rON7VgP5XC3jtraIz1F0i6S+iQnAQcBnyEzRt8ENEs6DRjfwf3k29Ys4BJJX5HUS9JASYcnveOFwH9I6iepQtKXk/f8CThC0tGS+gAzCqhjTzK/CXwiaRTwtax19wEnSfqqpN6S9pV0dNb6XwPfJzNmP6/dR6Aw7wIHZS0/Chwq6aLks1ckJ3iHSvqMpAsk7RURm4B1wOas7ewraa8uqtO6icPdOiQiVgITgelkgncl8D2gV0SsB75Fpje4hkwQzu/gfna4rYh4gcxJw38jcxK0jsyQBGR6ypuA18icHP528p43yJyoXAT8Bdjmypk2fAO4SdJ64If8o6dLRLxNZojjWuADYClwVNZ75yU1zYuIjwr+8O1zM3BuclXQLclxGw+cD6wC/gf4VzI/KCFzbFZIWkdmiOnC5LO8RuZ8wlvJcI6vlumhfBOTWTeQ9CbwTxGxqNS12M7BPXezLibpHDJj2U+UuhbbefTO38TMOkqZaQGGARdFxJYSl2M7EQ/LmJmlkIdlzMxSKO+wTHKp2FNkzrL3BuZGxI2t2uxK5nKvL5C5Bf28iFixo+32798/qqqqOla1mdlOavHixe9HRGW+doWMuf8dODEiNkiqAJ6RtDAinstq83VgTUQcLOl8MpdcnbejjVZVVVFfX1/A7s3MbCtJfy2kXd5hmWQ+jw3JYkXyaD1QP5HMHBoAc4GvSGr33YhmZlYcBY25J3cgLiVzI8jjEfF8qyYDSebiSObCWEtmZjwzMyuBgsI9IjZHxNHAIGCUpOGtmuTqpW93GY6kKyTVS6pvampqf7VmZlaQdl3nHhEfJtftngr8OWtVI5mZ8RqTLybYi8xt2K3ffydwJ0B1dbWvwTRLkU2bNtHY2Mgnn3xS6lJSoU+fPgwaNIiKiooOvb+Qq2UqgU1JsO8GnETmhGm2+cDFZGa/Oxd4InwBvdlOpbGxkT333JOqqip8yq1zIoLVq1fT2NjIkCFDOrSNQoZlBgBPSnoZeJHMmPujkm6SdFbSZhaZmeSWA98FftChasysx/rkk0/Yd999HexFIIl99923U78F5e25J986c0yO13+Y9fwTYHKHqzCzVHCwF09nj6XvUDUzSyGHu5l1Cam4j2KqqalJ/U2UO9+skDv6V+JzwGapERFEBL167Zx92J3zU5tZKq1YsYKhQ4fyjW98g5EjR3LvvfcyZswYRo4cyeTJk9mwYcN27+nb9x/fOT537lymTZvWjRV3HYe7maXK66+/ztSpU3n88ceZNWsWixYtYsmSJVRXVzNz5sxSl9dtdr5hGTNLtcGDBzN69GgeffRRli1bxtixYwH49NNPGTNmTImr6z4OdzNLlT322APIjLmffPLJzJkzZ4ftsy85TNPdtR6WsYzuukzBrJuMHj2aZ599luXLlwOwceNG3njjje3a7b///jQ0NLBlyxbmzZvX3WV2GYe7mXWJiOI+2quyspLZs2czZcoUjjzySEaPHs1rr722Xbuf/exnnHHGGZx44okMGDCgCJ+8PJTsO1Srq6ujJNeZ+lLI3HxcrJMaGhoYOnRoqctIlVzHVNLiiKjO91733M3MUsjhbmaWQg53M7MU8qWQVp58DsCsUxzupeQAM7Mu4mEZM7MUcribWdcowZy/2ZOAddTs2bO56qqrOr2d7tpuWxzuZmZF0tzcXOoSWjjczfLx1Aw9WlNTE+eccw7HHnssxx57LM8++ywAL7zwAscffzzHHHMMxx9/PK+//vp2712wYAFjxoxh5cqVDBkyhE2bNgGwbt06qqqq2LRpEzU1NUyfPp1x48Zx8803t7m/bNOmTWPu3Lkty8X4jaM1n1A1s1S75ppr+M53vsMXv/hF3n77bU455RQaGho4/PDDeeqpp+jduzeLFi1i+vTpPPjggy3vmzdvHjNnzqS2tpZ+/fpRU1PDggULmDRpEvfffz/nnHMOFRUVAHz44YfU1dUB8LWvfS3n/rqbw93MUm3RokUsW7asZXndunWsX7+etWvXcvHFF/OXv/wFSS29coAnn3yS+vp6HnvsMT772c8CcNlll/Hzn/+cSZMmcc899/DLX/6ypf15552Xd3/dzeFuZqm2ZcsW/vjHP7Lbbrtt8/rVV1/NCSecwLx581ixYgU1NTUt6w466CDeeust3njjDaqrM9O4jB07lhUrVlBXV8fmzZsZPnx4S/ut0wzvaH/ZevfuzZYtW4DM1MSffvppMT7qNnrkmLuHQM2sUOPHj+e2225rWV66dCkAa9euZeDAgUDmSpZsgwcP5qGHHmLq1Km8+uqrLa9PnTqVKVOmcMkll7R7f9mqqqpYvHgxAI888sg2vzUUS48MdzPrAUow5+/GjRsZNGhQy2PmzJnccsst1NfXc+SRRzJs2DDuuOMOAL7//e9zww03MHbsWDZv3rzdtg477DDuu+8+Jk+ezJtvvgnABRdcwJo1a5gyZUqbNbS1v2yXX345dXV1jBo1iueff36bnn+x9Mgpfzt1Y2c53RXqWtpWTvWUUy1lbGeY8nfu3Lk88sgj3Hvvvd2yv85M+esxdzOzAlx99dUsXLiQ2traUpdSEIe7mXVc9m81CxfCRx/9Y7k6b+eyR7n11ltLXUK75B1zl3SgpCclNUh6VdI1OdrUSForaWny+GHXlGtmZoUopOfeDFwbEUsk7QkslvR4RCxr1e7piDij+CWamVl75e25R8Q7EbEkeb4eaAAGdnVh1jG+TNTMoJ2XQkqqAo4Bns+xeoykP0laKOmINt5/haR6SfVNTU3tLtYsLfxD2LpawSdUJfUFHgS+HRHrWq1eAgyOiA2SJgAPA4e03kZE3AncCZlLITtctZmVPf2ouD+p4saORcZll13Gd7/7XYYNG1bUerJNmDCB3/72t+y9997bvD5jxgz69u3Ldddd12X7bktB4S6pgkyw3xcRD7Venx32EVEr6T8k9Y+I94tXqplZ+911111dvo9yvDyykKtlBMwCGiJiZhttPpe0Q9KoZLuri1momVk+H330EaeffjpHHXUUw4cP54EHHqCmpoatN0zOmjWLQw89lJqaGi6//PKWL8+YNm0aV155JSeccAIHHXQQdXV1XHrppQwdOpRp06a1bH/OnDmMGDGC4cOHc/3117e8XlVVxfvvZ/qyP/nJTzjssMM46aSTck4j3F0K6bmPBS4CXpG0dZKE6cDnASLiDuBc4EpJzcDHwPlRqltfzWyn9fvf/54DDjiABQsWAJn5Y26//XYAVq1axY9//GOWLFnCnnvuyYknnshRRx3V8t41a9bwxBNPMH/+fM4880yeffZZ7rrrLo499liWLl3Kfvvtx/XXX8/ixYvp168f48eP5+GHH2bSpEkt21i8eDH3338/L730Es3NzYwcOZIvfOEL3XsQEnnDPSKeAXY4eBYRtwG37ahNmvnudLPyMGLECK677jquv/56zjjjDL70pS+1rHvhhRcYN24c++yzDwCTJ0/mjTfeaFl/5plnIokRI0aw//77M2LECACOOOIIVqxYwV//+ldqamqorKwEMvPMPPXUU9uE+9NPP83ZZ5/N7rvvDsBZZ53V5Z+5Lb5D1cxS49BDD2Xx4sXU1tZyww03MH78+JZ1+QYTdt11VwB69erV8nzrcnNzM717FxaXKpNLnjwrpJmlxqpVq9h999258MILue6661iyZEnLulGjRlFXV8eaNWtobm7e5luXCnHcccdRV1fH+++/z+bNm5kzZw7jxo3bps2Xv/xl5s2bx8cff8z69ev53e9+V5TP1RHuuZtZl+jopYud8corr/C9732PXr16UVFRwe23395yGeLAgQOZPn06xx13HAcccADDhg1jr732KnjbAwYM4Kc//SknnHACEcGECROYOHHiNm1GjhzJeeedx9FHH83gwYO3GRbqbp7yt11vLvImU15Lp5RTPeX091Rusj5Iw8KFDO3f/x/rynDisA0bNtC3b1+am5s5++yzufTSSzn77LNLXVabOjPlr4dlzGynMWPGDI4++miGDx/OkCFDtjkZmjYeljHraVLT7e9+v/jFL0pdQrdxz93MimPLFvyjpXg6O2TucDezouizfDmrm5sd8EUQEaxevZo+ffp0eBseljGzohg0YwaNM2bQdPDB0KsXNDSUuqQerU+fPgwaNKjD73e4m1lRVKxZw5Brsr6ozeP/JeVwN7N08InmbXjM3cwshRzuZmYp5HA3M0shj7lbl/IwqFlpuOduZpZC7rmbWVnxb3vF4Z67mVkKOdzNzFLI4W5mlkIOdzOzFHK4m5mlkMPdzCyFHO5mZinkcDczSyGHu5lZCjnczcxSKG+4SzpQ0pOSGiS9KumaHG0k6RZJyyW9LGlk15RrZmaFKGRumWbg2ohYImlPYLGkxyNiWVab04BDksdxwO3Jn2ZmVgJ5e+4R8U5ELEmerwcagIGtmk0Efh0ZzwF7SxpQ9GrNzKwg7Rpzl1QFHAM832rVQGBl1nIj2/8AQNIVkuol1Tc1NbWvUrNOktp+7Ox8bNKn4HCX1Bd4EPh2RKxrvTrHW7abnDMi7oyI6oiorqysbF+lZmZWsILCXVIFmWC/LyIeytGkETgwa3kQsKrz5ZmZWUcUcrWMgFlAQ0TMbKPZfGBqctXMaGBtRLxTxDrNzKwdCrlaZixwEfCKpKXJa9OBzwNExB1ALTABWA5sBC4pfqlmZlaovOEeEc+Qe0w9u00A3yxWUWZm1jm+Q9XMLIUc7mZmKeRwNzNLIYe7mVkKOdzNzFLI4W5mlkIOdzOzFHK4m5mlkMPdzCyFHO5mZinkcDczS6FCJg7rUfSjHX+7wHaTzJuZpZB77mZmKeRwNzNLIYe7mVkKOdzNzFLI4W5mlkIOdzOzFHK4m5mlkMPdzCyFHO5mZinkcDczSyGHu5lZCjnczcxSyOFuZpZCDnczsxRyuJuZpVDecJd0t6T3JP25jfU1ktZKWpo8flj8Ms3MrD0K+bKO2cBtwK930ObpiDijKBWZmVmn5e25R8RTwAfdUIuZmRVJscbcx0j6k6SFko5oq5GkKyTVS6pvamoq0q7NzKy1YoT7EmBwRBwF3Ao83FbDiLgzIqojorqysrIIuzYzs1w6He4RsS4iNiTPa4EKSf07XZmZmXVYp8Nd0uckKXk+Ktnm6s5u18zMOi7v1TKS5gA1QH9JjcCNQAVARNwBnAtcKakZ+Bg4PyKiyyo2M7O88oZ7REzJs/42MpdKmplZmSjkOnfrBP1Iba7zrzdm1lU8/YCZWQo53M3MUsjhbmaWQg53M7MUcribmaWQw93MLIUc7mZmKeTr3HcivubebOfhnruZWQo53M3MUsjhbmaWQg53M7MUcribmaWQw93MLIUc7mZmKeRwNzNLIYe7mVkKOdzNzFLI4W5mlkIOdzOzFPLEYVYynsjMrOu4525mlkIOdzOzFHK4m5mlkMPdzCyF8oa7pLslvSfpz22sl6RbJC2X9LKkkcUv08zM2qOQnvts4NQdrD8NOCR5XAHc3vmyzMysM/KGe0Q8BXywgyYTgV9HxnPA3pIGFKtAMzNrv2KMuQ8EVmYtNyavbUfSFZLqJdU3NTUVYddmZpZLMcI9150oOe9BiYg7I6I6IqorKyuLsGszM8ulGOHeCByYtTwIWFWE7ZqZWQcVI9znA1OTq2ZGA2sj4p0ibNfMzDoo79wykuYANUB/SY3AjUAFQETcAdQCE4DlwEbgkq4q1szMCpM33CNiSp71AXyzaBWZmVmn+Q5VM7MUcribmaWQw93MLIUc7mZmKeRwNzNLIYe7mVkKOdzNzFLI4W5mlkIOdzOzFHK4m5mlkMPdzCyF8s4tY7Yz0I9yfS1BRs4vJzArc+65m5mlkMPdzCyFHO5mZinkcDczSyGfUDUrMzs6uQs+wWuFcc/dzCyFHO5mZinkcDczSyGHu5lZCvmEqpntkO/e7Zkc7mbWY/gHTeE8LGNmlkIOdzOzFHK4m5mlkMPdzCyFCgp3SadKel3Sckk/yLF+mqQmSUuTx2XFL9XMzAqV92oZSbsA/w6cDDQCL0qaHxHLWjV9ICKu6oIazcysnQrpuY8ClkfEWxHxKXA/MLFryzIzs84oJNwHAiuzlhuT11o7R9LLkuZKOjDXhiRdIaleUn1TU1MHyjUzs0IUEu657hpofb/A74CqiDgSWAT8KteGIuLOiKiOiOrKysr2VWpmZgUrJNwbgeye+CBgVXaDiFgdEX9PFn8JfKE45ZmZWUcUEu4vAodIGiLpM8D5wPzsBpIGZC2eBTQUr0QzM2uvvFfLRESzpKuA/wJ2Ae6OiFcl3QTUR8R84FuSzgKagQ+AaV1Ys5mZ5VHQxGERUQvUtnrth1nPbwBuKG5pZmbWUb5D1cwshRzuZmYp5HA3M0shh7uZWQo53M3MUsjhbmaWQg53M7MUcribmaWQw93MLIUc7mZmKeRwNzNLIYe7mVkKFTRxmJmZtZNyfc9RIlp/31HxueduZpZCDnczsxRyuJuZpZDD3cwshRzuZmYp5HA3M0shh7uZWQo53M3MUsjhbmaWQg53M7MUcribmaWQw93MLIUc7mZmKeRwNzNLoYLCXdKpkl6XtFzSD3Ks31XSA8n65yVVFbtQMzMrXN5wl7QL8O/AacAwYIqkYa2afR1YExEHA/8G/GuxCzUzs8IV8mUdo4DlEfEWgKT7gYnAsqw2E4EZyfO5wG2SFNENM9KbmZWIftT2F3KUOvyUL38lnQucGhGXJcsXAcdFxFVZbf6ctGlMlt9M2rzfaltXAFcki4cBrxfpc/QH3s/baufkY9M2H5vcfFzaVg7HZnBEVOZrVEjPPdePptY/EQppQ0TcCdxZwD7bRVJ9RFQXe7tp4GPTNh+b3Hxc2taTjk0hJ1QbgQOzlgcBq9pqI6k3sBfwQTEKNDOz9isk3F8EDpE0RNJngPOB+a3azAcuTp6fCzzh8XYzs9LJOywTEc2SrgL+C9gFuDsiXpV0E1AfEfOBWcC9kpaT6bGf35VF51D0oZ4U8bFpm49Nbj4ubesxxybvCVUzM+t5fIeqmVkKOdzNzFKoR4e7pAMlPSmpQdKrkq4pdU3lRNIukl6S9GipayknkvaWNFfSa8m/nTGlrqlcSPpO8n/pz5LmSOpT6ppKRdLdkt5L7uPZ+to+kh6X9Jfkz36lrHFHenS4A83AtRExFBgNfDPH1Ag7s2uAhlIXUYZuBn4fEYcDR+FjBICkgcC3gOqIGE7mAoruvjiinMwGTm312g+A/46IQ4D/TpbLUo8O94h4JyKWJM/Xk/lPOrC0VZUHSYOA04G7Sl1LOZH0WeDLZK7wIiI+jYgPS1tVWekN7Jbcr7I729/TstOIiKfY/n6dicCvkue/AiZ1a1Ht0KPDPVsyE+UxwPOlraRs/B/g+8CWUhdSZg4CmoB7kiGruyTtUeqiykFE/A34BfA28A6wNiIeK21VZWf/iHgHMp1LYL8S19OmVIS7pL7Ag8C3I2JdqespNUlnAO9FxOJS11KGegMjgdsj4hjgI8r4V+vulIwfTwSGAAcAe0i6sLRVWUf1+HCXVEEm2O+LiIdKXU+ZGAucJWkFcD9woqTflLakstEINEbE1t/w5pIJe4OTgP8XEU0RsQl4CDi+xDWVm3clDQBI/nyvxPW0qUeHuySRGTttiIiZpa6nXETEDRExKCKqyJwQeyIi3AMDIuJ/gJWSDkte+grbTl+9M3sbGC1p9+T/1lfwyebWsqdauRh4pIS17FAhs0KWs7HARcArkpYmr02PiNoS1mTl72rgvmSupLeAS0pcT1mIiOclzQWWkLkS7SV60O32xSZpDlAD9JfUCNwI/Az4T0lfJ/PDcHLpKtwxTz9gZpZCPXpYxszMcnO4m5mlkMPdzCyFHO5mZinkcDczSyGHu5lZCjnczcxS6P8D+/wSZwKhm1oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcec18fc950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.subplot(111)\n",
    "ax.bar(layers, r, width=0.2,color='b', label='relu')\n",
    "ax.bar(layers2, lr, width=0.2,color='r', label='Leakyrelu')\n",
    "\n",
    "ax.bar(layers1, s, width=0.2,color='g', label='sigmoid')\n",
    "\n",
    "plt.title(\"mean accuracy in test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.471160</td>\n",
       "      <td>4.490754</td>\n",
       "      <td>4.975682</td>\n",
       "      <td>5.905574</td>\n",
       "      <td>6.287317</td>\n",
       "      <td>6.875630</td>\n",
       "      <td>7.335145</td>\n",
       "      <td>7.780571</td>\n",
       "      <td>8.949683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.489604</td>\n",
       "      <td>4.806472</td>\n",
       "      <td>5.375358</td>\n",
       "      <td>5.852316</td>\n",
       "      <td>6.205827</td>\n",
       "      <td>7.168689</td>\n",
       "      <td>7.495427</td>\n",
       "      <td>8.298495</td>\n",
       "      <td>8.499672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.938597</td>\n",
       "      <td>4.851333</td>\n",
       "      <td>5.471946</td>\n",
       "      <td>6.610244</td>\n",
       "      <td>6.300147</td>\n",
       "      <td>7.460280</td>\n",
       "      <td>7.771602</td>\n",
       "      <td>8.541917</td>\n",
       "      <td>8.948889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.668166</td>\n",
       "      <td>6.365580</td>\n",
       "      <td>7.104553</td>\n",
       "      <td>8.022554</td>\n",
       "      <td>7.984202</td>\n",
       "      <td>9.591400</td>\n",
       "      <td>9.764355</td>\n",
       "      <td>11.124410</td>\n",
       "      <td>11.945088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           2         3         4         5         6         7         8  \\\n",
       "3   4.471160  4.490754  4.975682  5.905574  6.287317  6.875630  7.335145   \n",
       "5   4.489604  4.806472  5.375358  5.852316  6.205827  7.168689  7.495427   \n",
       "10  4.938597  4.851333  5.471946  6.610244  6.300147  7.460280  7.771602   \n",
       "30  5.668166  6.365580  7.104553  8.022554  7.984202  9.591400  9.764355   \n",
       "\n",
       "            9         10  \n",
       "3    7.780571   8.949683  \n",
       "5    8.298495   8.499672  \n",
       "10   8.541917   8.948889  \n",
       "30  11.124410  11.945088  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_testT1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.48960376, 4.80647206, 5.37535834, 5.8523159 , 6.205827  ,\n",
       "       7.16868949, 7.49542713, 8.29849529, 8.49967217])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(r_testT1.iloc[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/lib/python2.7/site-packages/ipykernel_launcher.py:3: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,u'Accuracy in test')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fce82fff9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(np.arange(len(np.array(r_testT1.iloc[1,:]))),np.array(r_testA1.iloc[1,:]),c='b')\n",
    "\n",
    "plt.hold(True)\n",
    "\n",
    "plt.scatter(np.arange(len(np.array(r_testT1.iloc[2,:]))),np.array(r_testA1.iloc[2,:]),c='b', label='relu')\n",
    "plt.scatter(np.arange(len(np.array(r_testT1.iloc[3,:]))),np.array(r_testA1.iloc[3,:]),c='b')\n",
    "plt.scatter(np.arange(len(np.array(r_testT1.iloc[0,:]))),np.array(r_testA1.iloc[0,:]),c='b')\n",
    "\n",
    "plt.scatter(np.arange(len(np.array(r_testT2.iloc[0,:]))),np.array(r_testA2.iloc[0,:]),c='g',label='sigmoid')\n",
    "plt.scatter(np.arange(len(np.array(r_testT2.iloc[1,:]))),np.array(r_testA2.iloc[1,:]),c='g')\n",
    "plt.scatter(np.arange(len(np.array(r_testT2.iloc[2,:]))),np.array(r_testA2.iloc[2,:]),c='g')\n",
    "plt.scatter(np.arange(len(np.array(r_testT2.iloc[3,:]))),np.array(r_testA2.iloc[3,:]),c='g')\n",
    "\n",
    "plt.scatter(np.arange(len(np.array(r_testT3.iloc[0,:]))),np.array(r_testA3.iloc[0,:]),c='r',label='Leakyrelu')\n",
    "plt.scatter(np.arange(len(np.array(r_testT3.iloc[1,:]))),np.array(r_testA3.iloc[1,:]),c='r')\n",
    "plt.scatter(np.arange(len(np.array(r_testT3.iloc[2,:]))),np.array(r_testA3.iloc[2,:]),c='r')\n",
    "plt.scatter(np.arange(len(np.array(r_testT3.iloc[3,:]))),np.array(r_testA3.iloc[3,:]),c='r')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Training time')\n",
    "plt.ylabel('Accuracy in test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=[0,0,0,0,0,0,0,0,0,0]\n",
    "s=[0,0,0,0,0,0,0,0,0,0]\n",
    "lr=[0,0,0,0,0,0,0,0,0,0]\n",
    "r[0]=np.array(r_testA1.iloc[:,0])+np.array(r_testA1.iloc[:,1])+np.array(r_testA1.iloc[:,2])+np.array(r_testA1.iloc[:,3])\n",
    "r[0]np.mean(r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.38666667, 1.98333333, 3.18666667, 3.45333333]),\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32      , 0.32      , 0.74333333, 0.86333333])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(r_testA1.iloc[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5025"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     0.113333\n",
       "5     0.490000\n",
       "10    0.730000\n",
       "30    0.830000\n",
       "Name: 8, dtype: float64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_testA1.iloc[:,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
